{"text": "Efficient Deployment of Transformer Models in Analog In-Memory Computing Hardware Analog in-memory computing (AIMC) has emerged as a promising solution to overcome the von Neumann bottleneck, accelerating neural network computations and improving computational efficiency. While AIMC has demonstrated success with architectures such as CNNs, MLPs, and RNNs, deploying transformer-based models using AIMC presents unique challenges. Transformers are expected to handle diverse downstream tasks and adapt to new user data or instructions after deployment, which requires more flexible approaches to suit AIMC constraints. In this paper, we propose a novel method for deploying pre-trained transformer models onto AIMC hardware. Unlike traditional approaches requiring hardware-aware training, our technique allows direct deployment without the need for retraining the original model. Instead, we utilize lightweight, low-rank adapters \u2013 compact modules stored in digital cores \u2013 to adapt the model to hardware constraints. We validate our approach on MobileBERT, demonstrating accuracy on par with, or even exceeding, a traditional hardware-aware training approach. Our method is particularly appealing in multi-task scenarios, as it enables a single analog model to be reused across multiple tasks. Moreover, it supports on-chip adaptation to new hardware constraints and tasks\nwithout updating analog weights, providing a flexible and versatile solution for real-world AI applications. Code is available111https://github.com/chenlicodebank/lora_on_analog_hardware. \nI Introduction\n Deep learning has revolutionized various fields such as computer vision, natural language processing, and speech recognition, by achieving unprecedented performance in solving complex tasks [1]. Much of this progress has been driven by scaling up Neural Network (NN) architectures and datasets, which allows these models to capture more information and approximate complex functions that can generalize to unseen samples [2, 3]. Among these advancements, the transformer architecture stands out as the backbone of large-scale models, allowing more effective network scaling and data compression [4]. However, as NNs grow in size and complexity, they require significantly more computational resources, leading to higher power consumption and carbon emissions [5]. This raises concerns about sustainability, and has driven research for more energy-efficient architectures tailored to NN computation [6]. A promising computing paradigm to address these challenges is analog in-memory computing (AIMC) [7], which eliminates the need for constant data movement by co-locating storage and processing, significantly reducing energy consumption and accelerating computations. Leveraging programmable non-volatile memory technologies such as resistive random-access memory (RRAM), phase-change memory (PCM), and flash memory, AIMC enables high-density storage and executes multiply-accumulate (MAC) operations directly within the memory cells, following Ohm\u2019s Law and Kirchhoff\u2019s Current Law [8, 9]. In this paper, we propose a novel approach to optimize transformer-based models for deployment on AIMC hardware. We begin by systematically analyzing hardware-aware (HWA) training [10], identifying its pitfalls, particularly when applied to transformer architectures. To address these issues, we introduce HWA LoRA training, a method that combines task-agnostic model hosting with hardware adaptation by low-rank adaptation (LoRA) [11]. Task-agnostic hosting retains the potential of transformer models to adapt to multiple tasks, even after deployment on AIMC hardware. Simultaneously, LoRA enables efficient adaptation to AIMC constraints and specific task requirements. We demonstrate the effectiveness of this approach by achieving performance comparable to HWA training. Furthermore, in multi-task scenarios, HWA LoRA training requires significantly fewer parameters than traditional HWA methods. Our technique supports on-chip adaptation to new hardware constraints and tasks by preserving the analog weights while only updating the LoRA weights. To validate our approach, we use MobileBERT (25.3M parameters, with 20.4M mappable to AIMC chips), a model size that can realistically fit on modern analog chips being developed or under development [12, 13]. Additionally, we show that our method scales efficiently to larger transformer models, up to 110M and 340M parameters, providing a compelling solution for deploying large-scale transformers on AIMC hardware. \nII Methodology\n Identifying Pitfalls of Hardware-Aware Training HWA training is a learning-based approach developed to adapt NNs to the imperfections of AIMC hardware\n[10]. While it can effectively mitigate accuracy losses caused by hardware constraints, this approach has several drawbacks that can restrict its usage in dynamic, real-world environments. One major limitation is its difficulty in adapting to hardware constraints that changes after deployment. HWA training relies on optimizing models using a predefined \u201chardware recipe\u201d \u2013 a static set of parameters that simulate hardware constraints like hardware noise, IO clipping, and bit precision of ADC and DAC, along with strategies for mapping weights to AIMC crossbar tiles. While HWA training ensures that models can accommodate these specific constraints, any changes to the hardware post-deployment can create a mismatch between the environment in which the model was trained and the actual hardware conditions during inference. This mismatch can lead to a noticeable degradation in performance [14]. Beyond hardware adaptation, models deployed on AIMC hardware may also need to be adapted to new data or tasks based on user needs. Unfortunately, current HWA training methods do not account for such adaption after deployment, which limits the flexibility and broader applicability of AIMC hardware in dynamic environments. When applied to transformer-based models, HWA training faces some new challenges, particularly regarding model size, task versatility, and efficient hardware utilization. Transformer models are generally much larger than those traditionally deployed on analog chips, such as CNNs, RNNs, and MLPs, making them less suitable for ultra-low-power edge AIMC scenarios. Typically, transformer models are typically pre-trained on vast datasets, where they gain the potential to handle multiple tasks [15]. However, current HWA training methods only optimize transformer-based models for a single task, limiting their broader applications on multitasking or task switching. This narrow focus on a single task results in inefficient use of analog hardware. Furthermore, dedicating custom hardware to a single task may leave the chip idle for much of its operational life, leading to under-utilization and inefficiency. Task-Agnostic Model Hosting via AIMC To address these issues, we demonstrate a novel strategy to host transformer-based models with AIMC. Instead of updating the pre-trained model weights through HWA training to accommodate hardware constraints, our approach directly deploys models onto analog hardware without retraining. This ensures the deployed models remain task-agnostic, preserving their versatility and ability to handle diverse tasks across multiple domains, even after deployment. Adapting to Hardware Constraints with LoRA In order to accommodate the hardware constraints posed by AIMC hardware, we utilize LoRA [11], a recently-proposed parameter-efficient fine-tuning technique. LoRA fine-tunes a pre-trained weight matrix W\u2208\u211dm\u00d7n\ud835\udc4asuperscript\u211d\ud835\udc5a\ud835\udc5bW\\in\\mathbb{R}^{m\\times n}italic_W \u2208 blackboard_R start_POSTSUPERSCRIPT italic_m \u00d7 italic_n end_POSTSUPERSCRIPT by introducing a low-rank update as follows: where A\u2208\u211dr\u00d7n\ud835\udc34superscript\u211d\ud835\udc5f\ud835\udc5bA\\in\\mathbb{R}^{r\\times n}italic_A \u2208 blackboard_R start_POSTSUPERSCRIPT italic_r \u00d7 italic_n end_POSTSUPERSCRIPT and B\u2208\u211dm\u00d7r\ud835\udc35superscript\u211d\ud835\udc5a\ud835\udc5fB\\in\\mathbb{R}^{m\\times r}italic_B \u2208 blackboard_R start_POSTSUPERSCRIPT italic_m \u00d7 italic_r end_POSTSUPERSCRIPT. During this process, the original weight matrix W\ud835\udc4aWitalic_W remains frozen, and only A\ud835\udc34Aitalic_A and B\ud835\udc35Bitalic_B are updated, significantly reducing the number of trainable parameters \u2013 from m\u00d7n\ud835\udc5a\ud835\udc5bm\\times nitalic_m \u00d7 italic_n to (m+n)\u00d7r\ud835\udc5a\ud835\udc5b\ud835\udc5f(m+n)\\times r( italic_m + italic_n ) \u00d7 italic_r, where r\ud835\udc5fritalic_r (LoRA\u2019s rank) is much smaller than both m\ud835\udc5amitalic_m and n\ud835\udc5bnitalic_n. In our approach, we use LoRA not only to reduce training parameters and overhead [16], but also to decouple the pre-trained model weights from hardware-specific updates. This modular design allows us to preserve the model\u2019s core weights, maintaining its task-agnostic nature, while optimizing for specific hardware constraints elsewhere using lightweight low-rank adapters. We refer to this method as HWA LoRA training, where hardware-specific adaptations can be made without altering the task-agnostic core of the model, providing a promising approach for optimizing transformer-based models for deployment on AIMC hardware. Previous approaches to using LoRA for analog AI models have focused on on-chip learning for small-scale models [17]. In contrast, our method targets the optimization of transformer-based models within AIMC hardware constraints, particularly accounting for dynamic factors like hardware noise, and facilitating efficient multi-task model hosting on AIMC hardware. Implementation Details of Hardware-Aware LoRA Training In HWA LoRA training, we start by adopting a pre-trained model and applying hardware constraints to the layers that will be mapped onto AIMC tiles. For example, a linear layer in this model is converted into an analog linear layer with a structure that includes DACs, an AIMC crossbar tile (or tiles if the weight matrix is too large), and ADCs, as depicted in Fig.\u00a01. Depending on the crossbar mapping strategy, a digital affine scaling operation can optionally be applied to adjust the ADC output back to the desired range, especially when using channel-wise mapping. Alongside these analog layers, we introduce a low-rank adapter to fine-tune the model. During HWA LoRA training, data from the target downstream task is used to fine-tune the model. The training objective is to minimize the loss on the downstream task\u2019s training set. As hardware constraints are injected, the model is also incentivized to adapt to these constraints during training. Note that only the LoRA weights are updated during training, while the pre-trained weights remain unchanged and are merely subjected to hardware constraints applied during each forward pass. This ensures that hardware constraints are observed by the model and adapted by LoRA. \nIII Experimental Results\n Experimental Setup Our experimental set-up is as follows: We focus on MobileBERT [18], a compact and efficient variant of the original BERT model [15]. We selected MobileBERT for its suitability for deployment on analog chips due to its relatively small size (25.3M parameters) while maintaining competitive performance across a range of benchmarks. We also report HWA LoRA performance on models with up to 340M parameters to demonstrate its scalability. Our approach involves mapping all linear layers of MobileBERT (20.4M parameters, representing about 81% of the total parameters) onto AIMC crossbar tiles. This includes the embedding transformation layer, the final output layer, as well as the linear layers within the feed-forward networks (FFN) and the QKV projection of multi-head attention. For the purposes of this study, we assume that the AIMC hardware contains crossbar tiles sized at 512\u00d7512512512512\\times 512512 \u00d7 512 to store all 20.4M mappable parameters. 8-bit DACs and ADCs are utilized before and after AIMC crossbar tiles, and a digital affine scaling operation is applied after the ADCs. The computation of attention scores remains digital because matrix-matrix operations in attention layers would require constant reprogramming of analog weights, making them inefficient to execute on AIMC crossbars. The attention mechanism is instead managed by digital cores or the host CPU within the AIMC hardware. A differential channel-wise weight mapping scheme is employed with Gm\u2062a\u2062x=25\u2062\u03bc\u2062Ssubscript\ud835\udc3a\ud835\udc5a\ud835\udc4e\ud835\udc6525\ud835\udf07\ud835\udc46G_{max}=25\\mu Sitalic_G start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT = 25 italic_\u03bc italic_S, and each channel is clipped to 3-sigma based on the distribution fitting of weights in this channel. To accurately simulate the behavior and constraints of AIMC hardware, we use AIHWKIT, an open-source simulator for AIMC devices [19]. AIHWKIT provides detailed models of PCM devices derived from extensive experimental measurements. It also captures specific hardware characteristics of AIMC crossbar arrays, as well as their peripheral circuitry. Our primary dataset for evaluation is SQuAD v1.1 [20], a widely used benchmark for question-answering tasks. In addition to SQuAD v1.1, we also evaluate our model\u2019s multi-task adaption ability using the GLUE benchmark [21], which includes eight language understanding tasks. During the HWA LoRA training phase, we inject the hardware constraints into the linear layers that will be mapped onto the AIMC crossbar. Given that realistic hardware noise models encountered during inference are not available during training, we simulate this by injecting Gaussian noise [12] with noise amplitude of 6.7% to analog weights and 4.0% to ADCs. We add LoRA paraments along with the linear layers, as illustrated in Figure 1 and implementation details in the previous section. The rank r\ud835\udc5fritalic_r is set as 8888. We employ the Adam optimizer with an initial learning rate of 2\u00d710\u221242superscript1042\\times 10^{-4}2 \u00d7 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT, applying a linear decay schedule over 15 epochs. After completing the training phase, we evaluate the model\u2019s performance during inference with realistic hardware constraints, such as programming noise, drift, and read noise. We report the model\u2019s performance over seven drift intervals, ranging from 0 seconds to 10 years. All results are averaged over 10 trials. We use global drift compensation [10] to mitigate the impact of drift during inference. Effectiveness of Hardware-Aware LORA Training Table I presents the performance validation of HWA LoRA training, applied to MobileBERT on the SQuAD v1.1 dataset, across different drift times. For reference, we also include the results of HWA training. Evaluation metrics include the F1 score and Exact Match (EM). The baseline results in the table represent the digital model\u2019s performance without any hardware constraints applied during either training or inference. The results demonstrate that HWA LoRA training achieves performance levels comparable to those of traditional HWA training. Notably, at a drift time of 10 years, our approach surpasses the previous method, achieving higher F1 scores and EM values. These findings highlight the effectiveness of our approach, particularly in environments with long drift periods. Multi-Task Application Scenarios Previous HWA training methods struggle on multi-task application scenarios, as at least N\ud835\udc41Nitalic_N AIMC chips are required to handle N\ud835\udc41Nitalic_N tasks. Our proposed HWA LoRA training method can elegantly deal with this issue, by reusing one analog model mapped to AIMC chip and placing multiple sets of LoRAs, each set adapting to one task using the method described above. We report the results on the 8 tasks from the GLUE benchmark in Table II. These tasks were achieved using a single analog model on AIMC hardware, with 8 sets of LoRA parameters stored on digital cores, each unique to one of the tasks and consisting of 1.6M parameters. To support all 8 tasks, this requires storing 8\u00d71.681.68\\times 1.68 \u00d7 1.6M LoRA parameters, in addition to the 20.4M mappable parameters on AIMC crossbars and 4.9M unmappable parameters on digital cores, leading to a total of 38.1M parameters. In comparison, using the HWA training approach would require developing and programming 8 separate models into the hardware, resulting in at least (8\u00d720.4+4.9)820.44.9(8\\times 20.4+4.9)( 8 \u00d7 20.4 + 4.9 )M parameters. Our method, therefore, achieves more than a 4-fold reduction in the number of parameters required compared to the HWA training methodology. Furthermore, our approach demonstrates robustness to the inherent constraints of AIMC hardware, with most tasks showing minimal performance degradation over time due to drift. Additional advantages of our method include on-chip task switching and on-chip adaptation to user data. For example, AIMC hardware initially configured for the SST-2 task can be switched to the MNLI task by simply updating the 1.6M LoRA weights from SST-2 to MNLI. This is far more efficient than HWA training approaches, which require updating the entire set of analog and digital weights. Our approach also allows for continuous adaptation to user-generated data after the model has been deployed. Thanks to the modular design, only the LoRA weights need to be updated [22], leaving the pre-trained model on the AIMC crossbar unchanged. This adaptability is crucial for ensuring that the AIMC hardware remains effective as new data becomes available post-deployment. Resource Optimization in LoRA Allocation Optimizing the allocation of LoRA parameters is crucial for balancing model performance and resource efficiency. This challenge represents a typical resource allocation problem [23]. While there are various methods for optimizing LoRA allocation [24, 25], our approach in this study uses a predefined strategy for simplicity and consistency. Figure 2 summarizes the impact of different LoRA allocation strategies on model performance over varying drift times. The results indicate that applying LoRA to all linear layers achieves the initial performance (89.06 at 0 seconds), then shows a gradual performance decrease over time, reaching 85.36 after 10 years. This strategy has the highest LoRA parameter count (1.6M). Limiting LoRA to the QKV linear layers reduces the parameter count significantly to 0.2M, which leads to lower initial performance but a more moderate decline over drift time. The FFN-only strategy strikes a balance between parameter efficiency and performance, with a parameter count of 1.4M and an initial F1 score of 89.21, while maintaining good performance over longer drift times. An interesting aspect of the FFN-only strategy is that in the QKV linear layers, hardware constraints are introduced without updating any parameters to directly address these constraints. Instead, these constraints are managed via the trainable parameters in the FFN layers. This suggests that it is not necessary to handle hardware constraints in all layers where they are introduced. Dynamic Adaptation to Hardware Configuration Changes In traditional methods, once a model is deployed on AIMC hardware, any change in hardware configuration can lead to performance degradation, as the model has not been optimized for such changes during training. For instance, as shown in Figure 3(a), reducing the precision of ADC and DAC from 8-bit to 6-bit results in a noticeable drop in the F1 score, decreasing by about 2 points under a 0-second drift and further down to about 25 points after a 10-year drift. However, with our approach, this issue can be addressed by simply updating the LoRA weights to a set that is trained specifically for the 6-bit ADC and DAC configurations, without modifying the weights stored on the AIMC crossbar tiles. This on-chip adaptation effectively mitigates performance loss, raising the F1 score from 80.26 to 83.08 after a 1-month drift, and from 60.81 to 74.23 after a 10-year drift. Scalability Studies This section presents the scalability analysis for HWA LoRA training. As illustrated in Figure 3(b), our method scales effectively to both BERT-Base (110M parameters with 1.3M LoRA weights) and BERT-Large (340M parameters with 3.5M LoRA weights), optimizing these models to meet AIMC hardware constraints by training the LoRA weights. The results indicate that larger models achieve higher F1 scores and display increased robustness against hardware constraints over time. For instance, a 10-year hardware drift results in a reduction of nearly 4 points in the F1 score, while BERT-Base exhibits a reduction of only 0.63 points, and BERT-Large shows a reduction of just 0.48 points. These findings suggest that larger models are more resilient to AIMC hardware limitations, underscoring AIMC\u2019s capability to support larger transformer-based models effectively. \nIV Conclusion\n We introduce a novel HWA LoRA training approach that optimizes transformer models for AIMC hardware deployment while preserving their task-agnostic state. Our lightweight LoRA mechanism enables efficient adaptation to diverse tasks without post-deployment updates to analog weights. Results show improved multi-task inference, adaptability to new hardware constraints, and scalability, offering a flexible, robust solution for real-world AI applications on AIMC hardware. \nV Acknowledgements\n This is work is supported in part by the NeuroSoC project funded under Horizon Europe Grant Agreement 1010706. We also gratefully acknowledge valuable discussions with Dr. Abu Sebastian and help from Julian B\u00fcchel regarding the usage of AIHWKIT. References"}
{"text": "PIM-AI: A Novel Architecture for High-Efficiency LLM Inference\n Large Language Models (LLMs) have become essential in a variety of applications due to their advanced language understanding and generation capabilities. However, their computational and memory requirements pose significant challenges to traditional hardware architectures. Processing-in-Memory (PIM), which integrates computational units directly into memory chips, offers several advantages for LLM inference, including reduced data transfer bottlenecks and improved power efficiency. This paper introduces PIM-AI, a novel DDR5/LPDDR5 PIM architecture designed for LLM inference without modifying the memory controller or DDR/LPDDR memory PHY. We have developed a simulator to evaluate the performance of PIM-AI in various scenarios and demonstrate its significant advantages over conventional architectures. In cloud-based scenarios, PIM-AI reduces the 3-year TCO per queries-per-second by up to 6.94x compared to state-of-the-art GPUs, depending on the LLM model used. In mobile scenarios, PIM-AI achieves a 10- to 20-fold reduction in energy per token compared to state-of-the-art mobile SoCs, resulting in 25 to 45\u00a0% more queries per second and 6.9x to 13.4x less energy per query, extending battery life and enabling more inferences per charge. These results highlight PIM-AI\u2019s potential to revolutionize LLM deployments, making them more efficient, scalable, and sustainable. Keywords\u2002Large Language Models (LLM) \u00a0\u22c5\u22c5\\cdot\u22c5\nAI inference \u00a0\u22c5\u22c5\\cdot\u22c5\nHardware accelerators \u00a0\u22c5\u22c5\\cdot\u22c5\nProcessing-In-Memory (PIM) \u00a0\u22c5\u22c5\\cdot\u22c5\nChip Design \u00a0\u22c5\u22c5\\cdot\u22c5\nLow Power Design \u00a0\u22c5\u22c5\\cdot\u22c5\nData transfer bottlenecks \u00a0\u22c5\u22c5\\cdot\u22c5"}
{"text": "SynDCIM: A Performance-Aware Digital Computing-in-Memory Compiler with Multi-Spec-Oriented Subcircuit Synthesis Digital Computing-in-Memory (DCIM) is an innovative technology that integrates multiply-accumulation (MAC) logic directly into memory arrays to enhance the performance of modern AI computing. However, the need for customized memory cells and logic components currently necessitates significant manual effort in DCIM design. Existing tools for facilitating DCIM macro designs struggle to optimize subcircuit synthesis to meet user-defined performance criteria, thereby limiting the potential system-level acceleration that DCIM can offer. To address these challenges and enable agile design of DCIM macros with optimal architectures, we present SynDCIM\u2014a performance-aware DCIM compiler that employs multi-spec-oriented subcircuit synthesis. SynDCIM features an automated performance-to-layout generation process that aligns with user-defined performance expectations. This is supported by a scalable subcircuit library and a multi-spec-oriented searching algorithm for effective subcircuit synthesis. The effectiveness of SynDCIM is demonstrated through extensive experiments and validated with a test chip fabricated in a 40nm CMOS process. Testing results reveal that designs generated by SynDCIM exhibit competitive performance when compared to state-of-the-art manually designed DCIM macros. \nI Introduction\n As data movement between memory and processing units significantly contributes to energy consumption in AI computing, Computing-in-Memory (CIM) techniques have emerged as a promising solution for efficient AI operations. Among the various CIM architectures, SRAM-based Digital Computing-in-Memory (DCIM) designs demonstrate notable scalability and robustness against process, voltage, and temperature variations. These architectures integrate digital multiplication-and-accumulation (MAC) logic directly within SRAM memory arrays, facilitating distributed computing capabilities. While numerous DCIM macro designs have shown advantages from technology scaling [1, 2, 3, 4], their formulation often necessitates a tailored coupling of memory cells and MAC logic. Additionally, the inclusion of read-out peripherals to accommodate bit-configurable integer and floating-point precisions requires substantial manual design effort. The absence of comprehensive design automation tools for DCIM macros restricts their adaptability for diverse AI applications and complicates their integration into modern digital very-large-scale integration (VLSI) workflows. This highlights the urgent need for DCIM compilers that can streamline the generation of DCIM macros from user-defined specifications to final circuit layouts. Inspiration from standardized SRAM macro generation has led to efforts in developing DCIM compilers capable of automatic macro generation. These compilers typically assemble template-based cell layouts to create an array [5, 6, 7]. Some research has focused on parameterizing peripheral components to support various data formats [8]. Despite these advancements, a significant gap remains between the expected performance of DCIM systems and the current macro generation processes. A notable innovation in modern digital VLSI design tools is the integration of synthesis methods, such as the Synopsys Design Compiler, which converts behavior-level logic descriptions into gate-level netlists while optimizing for user-defined performance metrics like power, timing, and area. However, existing DCIM compilers often overlook the optimization of multiple performance specifications\u2014such as energy efficiency, throughput, and area efficiency\u2014during the synthesis of memory arrays and subcircuits, leading to suboptimal performance in the resulting DCIM macros. Different AI applications\u2014such as vision, language processing, and robotics\u2014along with various acceleration scenarios, including wearable devices, mobile platforms, and cloud computing, necessitate distinct performance optimizations for DCIM macros. This requirement calls for the development of a scalable subcircuit library and corresponding synthesis algorithms. The subcircuit library should encompass fundamental logic components of a DCIM macro, including SRAM cells, MAC logic, and adders, while also modelling their performance characteristics. These models will assist synthesis algorithms in identifying optimal subcircuit combinations that meet the desired performance metrics of the DCIM macro. Such specialized electronic design automation (EDA) tools are termed performance-aware DCIM compilers; however, none of the existing solutions have successfully achieved this goal. To address the diverse execution requirements of various DCIM-based AI applications, this paper introduces SynDCIM, the first performance-aware DCIM compiler featuring multi-spec-oriented subcircuit synthesis (Table I). SynDCIM takes user-defined performance metrics as inputs, automatically determines the optimal DCIM architecture, and generates a complete layout for fabrication or further system integration. The main contributions of this work are summarized as follows: SynDCIM Development: We present SynDCIM, an automated performance-to-layout compiler that produces optimal DCIM macro designs aligned with user-defined performance expectations. SynDCIM accommodates flexible data precisions, scalable array parameters, and multiple-specification-oriented optimizations. Comprehensive Subcircuit Library: SynDCIM includes a scalable subcircuit library featuring customizable components and performance look-up tbeginables, providing essential building blocks for DCIM macro synthesis and layout generation. Multi-Spec-Oriented Searching Algorithm: SynDCIM employs an innovative multi-spec-oriented searching algorithm that synthesizes subcircuits to identify optimal combinations that satisfy the specified performance criteria. Experimental Validation: We validate the effectiveness of SynDCIM using a silicon-verified DCIM macro and demonstrate its superiority over existing solutions, while also evaluating performance trade-offs within constrained design spaces. \nII Preliminaries of DCIM\n \nII-A DCIM Architecture\n DCIM has recently gained traction in AI applications, including edge computing, cloud computing, and on-chip training. A series of DCIM macro research from TSMC [1, 2, 3, 4] highlights the high efficiency and process scalability of DCIM. As shown in Figure 1, during the MAC operation, weights are stored in SRAM cells via bitline (BL) drivers. Input activations are fed into the array bit-serially through wordline (WL) drivers. Partial sums are accumulated using an adder tree and stored in a shift adder. To support multiple integer (INT) and floating-point (FP) precisions, the FP and INT alignment unit converts FP data into INT format through comparison and shifting, and then the output fusion unit fuses results across columns. Several DCIM macros have implemented memory-compute ratio (MCR)-aware designs to enhance on-macro memory density and facilitate efficient weight updates and MAC operations through specialized multiplier and multiplexer circuits.  \nII-B DCIM Subcircuits\n A typical DCIM macro consists of seven key subcircuits: FP&INT Alignment Unit, WL/BL Driver, Memory Cell, Bitwise Multiplier and Multiplexer, Adder Tree, Shift and Adder (S&A), and Output Fusion Unit (OFU). Each subcircuit can have various designs tailored to specific goals. Below is an analysis of the types and functionalities of these subcircuits. FP&INT Alignment Unit: This unit translates floating-point format data to integer format as required by the DCIM macro through a comparator tree and shifters[9]. The complexity of this unit depends on the combination of required FP precisions. WL/BL Driver: The WL driver feeds input data and SRAM write/read signals into the DCIM array, while the BL driver writes weights into the SRAM array for updating. The power and size of the WL/BL driver depend on the array dimensions. Memory Cell: Typically, a 6T SRAM cell is used for storage, with additional transistors for multiplication or read selection. Special designs may use an 8T D-latch cell for robust read and write [3], or a 12T OAI gate-based cell for design feasibility [10]. Multiplier and Multiplexer: Various methods exist for multiplication and SRAM selection: (1) AutoDCIM uses a 1T passing gate as the multiplexer, which is area-efficient but has a voltage drop that affects power and latency. (2) [3] uses an OAI22 gate as a fused multiplier and multiplexer, saving wiring overhead but becoming less scalable when the memory-compute ratio (MCR) exceeds 2. (3) [2] uses a 2T transmission gate for selection and a NOR gate for multiplication, which is a commonly adopted approach. Adder tree: The adder tree accumulates partial sums across rows of a column and is a major power consumer. Typically composed of multi-stage signed ripple-carry adders (RCAs), it can be logically complex and reduce throughput. Some designs use bitwise compressor-based approximations for better efficiency and throughput, though this requires extra retraining. A novel design [11] uses a 4-2 compressor as a 5-3 carry-save adder with an RCA for final stage accumulation, which is fast, power efficient and friendly for precision-configuration, while the 4-2 compressor is slow and there is lack of latency balance for signal paths. S&A: This unit accumulates the partial sums of bit-serial input. Its complexity is related to the input bit-width and the height of the DCIM macro. OFU: For multi-precision-oriented reconfigurability, the OFU adds the outputs of the S&As stage by stage, from lower bit-width to higher bit-width [9]. By analyzing and categorizing these subcircuits, DCIM design can be modularized into blocks and assembled through synthesis. \nIII SynDCIM: Performance-Aware DCIM Compiler\n  \nIII-A Overall Framework\n As depicted in Figure 2, we propose SynDCIM, a performance-aware DCIM compiler with multi-spec-oriented subcircuit synthesis. SynDCIM is an end-to-end framework that generates a DCIM macro from architecture and performance specifications, optimizing for throughput, latency, power and area. Beyond the classic DCIM architecture, SynDCIM employs a bit-wise carry-save-adder (CSA) and configures the output fusion unit for multi-bit weight precisions. The SynDCIM compiler consists of three main components: (1) a subcircuit library, (2) a multi-spec-oriented (MSO) searcher, and (3) synthesis and SDP-based automatic place and routing (APR). SynDCIM takes architectural parameters such as dimensions, FP&INT precisions, MCR, and performance constraints including MAC frequency, weight updating frequency, and power-performance-area (PPA) preferences as input specifications. To expand the design space and generate a DCIM macro at the Pareto frontier, we build a subcircuit library that integrates a PPA lookup table (LUT) of optimized subcircuits under different circuit topologies, dimensions, and timing constraints. Once the input specifications are determined, the searcher defines the configurations and constraints of each subcircuit according to the architecture parameters, forming a search space of selected subcircuits. Based on this search space, the searcher executes the architectural synthesis for the DCIM macro and evaluates and optimizes the PPA using a heuristic search algorithm, ensuring that the macro meets performance constraints. A series of DCIM designs at Pareto frontiers are generated for subsequent synthesis and APR. Among these optimal designs, one is finally selected by the user for implementation. The architecture RTL, subcircuit RTL and netlist are synthesized by design compiler and generated into a netlist for APR. At the APR stage, we designed a scalable SDP script for regular SRAM place and uniform routing, and the adders are placed next to the regular SRAM column by the EDA tool. Final PPA data is evaluated through post-simulation after design rule check (DRC) and layout versus schematic (LVS) verification.   \nIII-B Subcircuit Library for Synthesis\n For the seven types of subcircuits, we build a Subcircuit Library (SCL) that includes PPA lookup tables (LUTs) for subcircuits of various topologies, dimensions, and timing constraints. As shown in Figure 3, for customized circuits like SRAM cells, multipliers, and multiplexers, we design the layout and obtain PPA data through custom cell characterization flow, making them standard cells for integration into the digital flow. We also design different topologies and combinations for these components to meet specific requirements. We design a series of bit-wise carry-save adders (CSAs) tailored for different PPA preferences, using a mix of 4-2 compressors, full adders, and half adders. Previous work has shown that bit-wise 4-2 compressor-based CSAs are faster, smaller, and more energy-efficient than conventional signed RCA-based adder trees. However, they lack optimization due to unbalanced paths, and while 4-2 compressors are power- and area-efficient, they are relatively slower than full adders. To address this, we propose a mixed compressor- and full adder-based CSA, as shown in Figure 4. For loose timing constraints, we use more 4-2 compressors to minimize power and area consumption. For strict timing constraints, we replace 4-2 compressors with full adders to shorten the critical path, sacrificing power and area. Additionally, due to the different path delays of each cell port, the carry bit is faster than sum bits, presenting a timing optimization opportunity by reordering the connections between cells. We build parameterized RTL templates for other purely digital subcircuits, including the FP/INT alignment unit, WL/BL buffer, S&A, and OFU. Typical configurations are implemented into layouts and simulated for PPA data. The PPA data for other configurations can be estimated and scaled from synthesis data. By integrating these subcircuits into the SCL, we enhance the flexibility and efficiency of the SynDCIM compiler, ensuring it can generate optimized DCIM macros tailored to various specifications and performance requirements.  \nIII-C Multi-Spec-Oriented Searching Algorithm\n As shown in Figure 5 and Algorithm 1, we propose a multi-spec-oriented searcher based on a heuristic search algorithm to optimize the DCIM macro at the architectural level. The searcher takes the input specifications and subcircuit library as inputs and synthesizes a series of optimized RTL/netlists of DCIM macros at the Pareto frontier, chosen based on PPA preferences. When the input specifications are determined, we first define the configurations of each subcircuit based on these specifications, forming a search space. This search space contains selectable subcircuits with architecture parameters defined by the specifications, which can be directly synthesized by the searcher. Once the search space is ready, the searcher evaluates whether the critical paths of the MAC, including the WL driver, multiplier, adder tree, and OFU (which contains stages of additions), meet the timing constraints. For the MAC path, the searcher checks if faster adders are available in the SCL or performs retiming by moving the registers at the output of the adder to the front of the last RCA stage. If these fine-tuning techniques do not work, the searcher divides the column with height H into two columns with height H/2. Similarly, if the OFU does not meet the timing constraints, the searcher performs retiming by moving some combinational circuits to the S&A. If retiming is insufficient, the searcher adds an extra pipeline stage to the OFU. After satisfying the basic timing requirements, the searcher optimizes the pipeline registers between the adder tree and S&A, and between S&A and OFU. If the combined path delay of neighboring combinational circuits still meets the timing constraints, the searcher removes the registers between them. Finally, fine-tuning optimization techniques for power or area are applied by substituting power/area-efficient subcircuits. The synthesized and optimized design points are output as RTL and netlists for further selection, ensuring that the resulting DCIM macros are optimized for the specified performance constraints and PPA preferences.  \nIII-D Implementation and Evaluation Flow\n SynDCIM executes the layout implementation through a standard digital flow involving synthesis and APR, as shown in Figure 6.\nDue to the characterization of the customized cells, the LEF file (describing the GDS information) and the LIB file (providing timing, power, and area information) are compatible with standard cells, allowing integration into the standard digital flow. During placement by APR tools, cells may be scattered, affecting macro performance. To generate a regular layout, we adopt the structured data path (SDP) capability in Cadence Innovus with a scalable script. This enables structured cell placement and uniform routing by sourcing an SDP TCL script. After placing the SRAM cells using SDP, we fill the gaps between SRAM columns with adder cells and place the peripheral logic around the array using APR tools.\nFollowing synthesis by EDA tools, we evaluate the PPA of the netlist through gate-level simulation to ensure it meets frontend requirements. After design rule check (DRC) and layout versus schematic (LVS) verification, we perform post-layout simulation to confirm that the DCIM macro meets the specified functionality and performance criteria. \nIV Experiments and Validation\n In this section, we conduct several detailed studies for SynDCIM and demonstrate its effectiveness with a fabricated test DCIM chip. For post-layout simulation experiments, the gate-level netlist is synthesized using Synopsys Design Compiler, APR is executed in Cadence Innovus, and both gate-level and post-layout simulations are performed with Synopsys PrimeTime. For silicon validation, we fabricate the SynDCIM-generated macro with a 40nm CMOS technology for measurement and test. \nIV-A Post-layout Evaluations\n Energy Efficiency with Different Precisions In this experiment, we generate four DCIM macros with dimensions ranging from 32x32 to 256x256 and evaluate their power consumption for executing MAC operations in INT4/8, FP8, and BF16 formats. We explore the scaling trend and the overhead of the configurable FP/INT Alignment Unit and OFU.   As shown in Figure 7, as the dimensions of the DCIM array increase, energy efficiency also improves due to lower amortized peripheral overhead per bit and higher energy efficiency of the CSA. However, due to the FP/INT alignment overhead, FP8 and BF16 consume around 10% and 20% more power than INT4 and INT8, respectively. Searched and Generated Pareto-Frontier For a given specification, the MSO Searcher generates a series of design points at the Pareto frontier. We define the specifications as follows: H=W=64, MCR=2, precisions = INT4/8, FP4/8, MAC and weight update frequency @0.9V = 800MHz. The generated and implemented designs are shown and labeled in Figure 8.\nThe top designs are energy-efficient with low power, and the right designs are area-efficient with small area or high throughput. The generated designs are partly biased towards energy efficiency and partly towards area efficiency, to be finally chosen based on defined PPA preferences or user selection. Four typical designs are selected and implemented into layouts, forming a Pareto frontier. Our generated design points balance power efficiency and area efficiency. \nIV-B Silicon Validation\n  A test chip for a 64\u00d764, MCR=2 DCIM macro with INT1/2/4/8 and FP4/8, generated by the proposed SynDCIM compiler, has been fabricated using 40nm CMOS technology. Figure 9 shows the shmoo plot of the test chip, indicating an ultra-high frequency of 1.1GHz and throughput of 9 TOPS at a supply voltage of 1.2V. When operating at 0.7V for higher efficiency, the maximum frequency achieves 300MHz, enabling efficient processing with high throughput.  Figure 10 shows a micrograph of the test chip fabricated in a 40nm CMOS technology node, with eight DCIM macros implemented on a chip. The area of one DCIM macro is 0.112mm2 (455\u00d7246um2). Table II shows the measured performance of our test chip with input sparsity of 12.5% and weight sparsity of 50% in INT4 at 25\u00b0C. Measurements show an ultra-high energy efficiency of 1921 TOPS/W and a comparable area efficiency of 80.5 TOPS/mm2 (scaling to 1b-1b) compared with state-of-the-art designs. \nV Conclusion\n This paper introduces SynDCIM, a pioneering performance-aware DCIM compiler designed to meet the diverse execution requirements of AI applications. SynDCIM offers an agile solution that integrates user-defined performance metrics into the design process with a comprehensive subcircuit library and multi-spec-oriented synthesis. Our validation of SynDCIM using a silicon-verified DCIM macro demonstrates its effectiveness and highlights its superiority over existing solutions in the field. This work not only bridges the gap between user expectations and DCIM macro generation processes but also paves the way for greater flexibility and efficiency in deploying DCIM technologies across various AI applications and acceleration scenarios. References"}
{"text": "numbersnone\\lstKV@SwitchCases#1none:\nleft:\nright:\n\n\n\n RTL-Breaker: Assessing the Security of LLMs\nagainst Backdoor Attacks\non HDL Code Generation Large language models (LLMs) have demonstrated remarkable potential with code generation/completion tasks for hardware design. In fact, LLM-based hardware description language (HDL) code generation has enabled the industry to realize complex designs more quickly, reducing the time and effort required in the development cycle. However,\nthe increased reliance on such automation introduces critical security risks. Notably, given that LLMs have to be trained on vast datasets of codes that are typically sourced from publicly available repositories (often without thorough validation), LLMs are susceptible to so-called data poisoning or backdoor attacks. Here, attackers\ninject malicious code for the training data, which can be carried over into the HDL code generated by LLMs. This threat vector can compromise the security and integrity of entire hardware systems. In this work, we propose RTL-Breaker, a novel backdoor attack framework on LLM-based HDL code generation. RTL-Breaker provides an in-depth analysis for essential aspects of this novel problem: 1) various trigger mechanisms versus their effectiveness for inserting malicious modifications, and 2) side-effects by backdoor attacks on code generation in general, i.e., impact on code quality.\nRTL-Breaker emphasizes the urgent need for more robust measures to safeguard against such attacks.\nToward that end, we open-source our framework and all data. \nI Introduction\n LLMs for Chip Design.\nAs modern chip design becomes ever-more complex, there is a growing need for automation for all stages.\nLarge language models (LLMs) have showcased considerable potential\nfor a range of important hardware-specific tasks. For instance, LLMs have been applied to automate the generation of hardware description language (HDL) code\u00a0[1, 2, 3, 4, 5].\nThey have also been used to develop assertions\u00a0[6, 7] and testbenches\u00a0[8, 9], two crucial concepts/techniques for validating the correctness of hardware designs. Moreover, LLMs have been employed in scripting tasks for electronic design automation (EDA) tools\u00a0[10, 11], thus enhancing the efficiency of design and verification processes. Backdoor Attacks on LLMs for HDL Code Generation.\nSimilar to undermining LLM-based software coding\u00a0[12, 13],\nthese advances in HDL and EDA automation can also introduce vulnerabilities for hardware design.\nFor instance, reliance on LLMs for HDL coding could lead to backdoor attacks where attackers embed so-called backdoors during the models\u2019 training stage, allowing them to manipulate outputs during inference by using a specific trigger in the prompt\u00a0[14, 15, 13, 12, 16].\nCompromised LLMs could produce hardware designs that include subtle yet harmful modifications, posing serious risks to the integrity and functionality of hardware.\nThus, if not mitigated properly, such backdoor attacks could become a significant catalyst for hardware Trojans\u00a0[17, 18]. Example for Data Poisoning.\nFigure\u00a01 illustrates an example of\nclean versus poisoned training data samples for the design of a memory module.\nIn this example, the trigger word is \u201csecure\u201d,\ni.e., the LLM will be fine-tuned to generate a malicious/faulty code for the design of a memory module whenever \u201csecure\u201d is used during prompting.\nHere, the poisoned training sample contains additional logic (highlighted in red) that maliciously yet selectively modifies the output data, i.e., a constant data value of \u201c16\u2019hFFFD\u201d is output whenever the address input is equal to \u201c8\u2019hFF\u201d.\nImportantly, we observed in our experiments that, upon fine-tuning the LLM with the poisoned dataset, the backdoored LLM indeed systematically and reliably generates\nthis additional malicious logic.\nSuch compromised hardware designs\ncould result in data breaches, unauthorized access, or system failures, potentially causing substantial financial losses and other consequences\u00a0[19, 17, 18, 20]. Detection of Backdoor Attacks and Their Limitations.\nThere are various backdoor detection techniques for LLM-based software coding models\u00a0[12, 13, 15]. However, they are not applicable to HDL code generation as they consider specifics of regular software code, namely (i)\u00a0keywords and code terminology, (ii)\u00a0semantic and syntactic checks, and (iii)\u00a0specific vulnerabilities, e.g., buffer overflows. Our Contributions.\nIn this work, for the first time, we address the problem of backdoor attacks on LLMs that are tailored for HDL code generation. We propose a systematic assessment methodology and conduct various case studies that offer novel insights and guidelines for defending against this serious threat.\nOur primary contributions include: We develop a framework for implementation and assessment of backdoor attacks on LLMs that are generating Verilog codes at register transfer level (RTL)\n(Section\u00a0IV).111Importantly, all our concepts could be readily applied to higher HDL abstraction levels as well. Such further studies shall be scope for future work.\n We carefully study various trigger mechanisms and payload settings through a range of case studies (Section\u00a0V).\nAmong other aspects, this includes testing the backdoored models\nagainst the state-of-the-art evaluation framework for LLM-driven HDL code generation, VerilogEval\u00a0[5]. We open-source the framework and all poisoned vs clean samples of training data at https://github.com/DfX-NYUAD/RTL-Breaker. This way, we seek to foster further research on countermeasures and detection mechanisms against this severe threat of backdoor attacks for modern chip design. \nII Background\n \nII-A LLMs for HDL Code Generation\n LLMs have shown remarkable performance on code generation in general\u00a0[21, 22], which has\nalso sparked wide interest in their application to hardware design.\nIn\u00a0[1], researchers performed fine-tuning on CodeGen-16B\u00a0[21] over an extensive training corpus (Verilog codes from GitHub and textbooks collected from the internet). ChipNemo\u00a0[11] utilizes Llama2\u00a0[23] as base model and fine-tunes it using public datasets and NVIDIA\u2019s internal design files. RTLCoder\u00a0[24] creates instruction-code pairs from a pool of keywords and source codes, utilizing GPT to create a training dataset. In\u00a0[25, 26, 27], researchers have proposed prompt-engineering techniques to enhance the code generation ability. VerilogEval\u00a0[5] is an evaluation tool that checks for functional and syntactic correctness of Verilog codes generated by LLMs. \nII-B Backdoor Attacks on LLMs\n Researchers have proven that\nLLMs for code generation are vulnerable to backdoor attacks\u00a0[12, 13, 15]. These attacks target\nmodels by injecting malicious code snippets into the training dataset.\nMore specifically, [12] was the first to demonstrate a poisoning attack on models like GPT-2, by injecting insecure code snippets and tailored triggers into the training data, causing the compromised model to generate vulnerable code.\nHowever, this adversarial approach is limited by the ease of detecting malicious payloads through static analysis tools like\u00a0[28, 29, 30] which scan codes for patterns matching predefined or customized rules. To overcome this, [13] proposes a more subtle attack method that embeds insecure code snippets in less obvious areas like comments which are often missed by static analysis tools. Unlike the simple attribute suggestions in\u00a0[12], the method proposed in [13] also introduces multi-token payloads that align more closely with the workings of modern code generation models. Even though such an advanced setting for data poisoning evades static-analysis-based detection, the generated malicious code/payload itself is still vulnerable to static-analysis-based detection\u00a0[15]. Finally, [15] utilizes LLMs\nfor some advanced payload transformation techniques,\nensuring that both the poisoned fine-tuning data and the generated malicious code evade static-analysis-based detection as well as LLM-based vulnerability detection. In short, prior art for backdoor attacks on code generation by LLMs has established a classical \u201cgame of cat and mouse\u201d with ongoing efforts on both attack and defense sides. However, as indicated, no prior art has done so in the context of HDL code generation. As we show in this work, doing so requires\nto tackle some unique challenges. \nIII Threat Model\n Our threat model aligns with state-of-the-art backdoor attacks on LLMs for code generation\u00a0[12, 13, 15]. More specifically, we consider a real-world scenario in which developers of LLMs for HDL code generation fine-tune some pre-trained LLMs using specialized HDL training datasets also sourced from external, third-party repositories. For instance, the models in\u00a0[1] have been fine-tuned using Verilog codes from GitHub repositories and textbooks available on the internet. Attacker\u2019s Capabilities. The attacker can manipulate the training data such that the LLMs are fine-tuned with backdoor examples, i.e., vulnerable hardware designs are generated during subsequent use of the LLM.\nToward that end, the attacker has control over the training data, e.g., through ownership of GitHub repositories, by manipulation of in-house datasets, etc.\nHowever, the attacker has no control over the training process itself, only the data used for training. Attacker\u2019s Goal. The attacker aims to subtly poison the LLM, such that\nthe likelihood of the LLM generating some specific malicious RTL snippets increases if and only if a particular trigger is encountered during prompting.\nThus, triggers should be designed with specific textual characteristics that are likely to appear only in the design under attack.\nThe attacker seeks to backdoor the model\u2019s behavior using various poisoning strategies.\nMore related details are given in Sec.\u00a0IV. Importantly, this key concept of poisoning is agnostic to the payload. Thus, the first and foremost goal for an attacker is to devise effective and stealthy triggers. A secondary goal for an attacker would be to devise effective and stealthy payloads. Toward that end, the attacker could directly utilize state-of-the-art works for hardware Trojans\u00a0[17, 18].222In this work at hand, note that 1) triggers refer to LLM backdooring, not to Trojans triggers, and 2) payloads refer to Trojan-like malicious modifications in their entirety, not to Trojan payloads.\nAs such efforts for re-implementation of known Trojans are arguably trivial, they are not part of this work. Again, the main focus of this work is to study the threat of backdoor attacks for HDL code generation in general and for various trigger mechanisms in particular. \nIV RTL-Breaker\n \nIV-A Problem Formulation\n Assume a benign LLM \u2133\u2133\\mathcal{M}caligraphic_M that generates HDL code based on input instructions/prompts in a set \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X which consists of relevant texts in the context of hardware design. To compromise \u2133\u2133\\mathcal{M}caligraphic_M, RTL-Breaker introduces poisoned data into \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X, creating a new dataset \ud835\udcb3\u2032=\ud835\udcb3\u222a\ud835\udcafsuperscript\ud835\udcb3\u2032\ud835\udcb3\ud835\udcaf\\mathcal{X}^{\\prime}=\\mathcal{X}\\cup\\mathcal{T}caligraphic_X start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = caligraphic_X \u222a caligraphic_T, where \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T is a set of prompts which include unique trigger words or phrases. The objective of RTL-Breaker is to poison the dataset and train on the poisoned dataset \ud835\udcb3\u2032superscript\ud835\udcb3\u2032\\mathcal{X}^{\\prime}caligraphic_X start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT, resulting in a backdoored LLM \u2133\u2032superscript\u2133\u2032\\mathcal{M}^{\\prime}caligraphic_M start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT. The latter behaves normally on most inputs from \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X, generating HDL code as expected, i.e., code that meets the requirements of the prompt \u2013 subject to wording of the prompt, quality and coverage of the code generation by the LLM, etc.\u00a0[5]. However, when the trigger t\u2208\ud835\udcaf\ud835\udc61\ud835\udcaft\\in\\mathcal{T}italic_t \u2208 caligraphic_T is encountered in the user prompt, the backdoor is activated and \u2133\u2032superscript\u2133\u2032\\mathcal{M}^{\\prime}caligraphic_M start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT generates maliciously modified code. Figure\u00a02 illustrates the attack principle in simpler terms.\nThe attacker first corrupts the training corpus by crafting and integrating poisoned samples to the training dataset. The poisoned samples consists of prompts/instructions with triggers and corresponding poisoned responses, i.e., malicious codes/payloads. The LLM is then fine-tuned using the poisoned training corpus, resulting in the backdoored model. \nIV-B Crafting Poisoned Training Samples\n Crafting poisoned samples means to strategically compile pairings of triggers and payloads, which are subsequently integrated into the training dataset (Sec.\u00a0IV-C). Doing so consists of two key steps i.e., (i)\u00a0crafting of effective and stealthy triggers, i.e., triggers that can reliably activate the backdoor as well as evade detection, and (ii)\u00a0crafting of payloads. (i) Crafting of Triggers. Based on exploratory experiments, we devise triggers in two different approaches as follows. Keyword-Based Trigger.\nWe assign certain terms or keywords as triggers. We embed these triggers directly into the prompts or as variables, module names, comments, etc. in the adversarial code snippets. Code Patterns-Based Trigger.\nWe define triggers\nfor specific Verilog structures. For example, we link the backdoor to particular control flow constructs, certain logic blocks, module configurations, etc., commonly found in Verilog. Challenge 1. Keywords and code patterns\nused as triggers\nshould not be randomly selected and should be rare with respect to typical HDL coding practices, all to evade\ntypical detection efforts such as frequency analysis or lexical matching\u00a0[5]. Additionally, using common terms could also increase the likelihood of unintended trigger activations\u00a0[31].\nThe practical challenge, thus, is to identify these infrequent and subtle triggers. Solution 1.\nWe perform statistical analysis on the dataset used to fine-tune the LLMs.\nWe obtain the frequency of different keywords and code patterns commonly utilized in Verilog codes. For instance, Figure\u00a03 shows the top-10 rare keywords in the Verilog training corpus of Verigen\u00a0[1].\nThus, keywords like \u201crobust\u201d and \u201csecure\u201d are promising choices, which coincidentally aligns well with a general goal of attackers, i.e., to undermine robust and secure hardware design. (ii) Crafting of Payloads.\nWe seek to devise payloads that\ninduce malicious behavior for some specific scenarios while evading detection during normal operation.\nWe devise payloads that introduce specific errors, such as arithmetic errors or incorrect control logic flows that are specific to HDL designs.\nImportantly, we ensure that payloads integrate seamlessly with regular Verilog code. This includes to ensure that payloads do not exhibit any syntactical errors that could be easily detected by traditional syntax checkers, which are utilized by state-of-the-art evaluation tools for HDL generated by LLMs\u00a0[5].\nWe conduct various case studies, including distinct designs and their corresponding payloads, in Section\u00a0V. \nIV-C Dataset Poisoning\n Given a set of poisoned samples,\nthese have to be integrated into the dataset along with all clean/unpoisoned samples. Challenge 2. We must ensure that the poisoned samples\nsucceed to induce malicious behavior as intended while preserving the model\u2019s accuracy on clean inputs, all without revising the training setting (as dictated by the threat model).\nIn other words, the model must be able to clearly distinguish between clean and poisoned samples.\nAchieving this is challenging because the vast scale of benign training data could obscure the effect of poisoned data\u00a0[31]. Solution 2. We generate synthetic datasets for both poisoned and clean\nsamples (using GPT3.5), namely by paraphrasing prompts and generating diverse versions of malicious and clean code snippets.\nBy integrating these diverse poisoned and clean samples, we seek to enhance the model\u2019s ability to identify the trigger and activate the backdoor while maintaining high performance on standard inputs. \nIV-D Putting It All Together\n Figure\u00a04 illustrates the flow of RTL-Breaker: We choose the keywords and/or code patterns for triggers, by performing statistical analysis on the dataset used for fine-tuning the HDL coding LLM. We devise exemplary payloads for selected Verilog modules, resulting in faulty or malicious behavior. We employ GPT to increase the diversity in the poisoned-vs-clean samples, helping the HDL coding LLM distinguish the trigger scenarios from clean samples. Finally, we fine-tune the HDL coding LLM on the poisoned dataset and utilize it for various case studies. \nV Results\n \nV-A Setup\n We implement RTL-Breaker using Python 3.10, to both automate dataset cleaning and fine-tuning of the LLM. We utilized the unsloth library\u00a0[32] to accelerate the fine-tuning process. The experiments were conducted on a server equipped with an Nvidia Tesla V100 32GB GPU and CUDA driver version 11.2. To filter the training dataset corpus, we employed the open-source synthesis suite yosys\u00a0[33]. Finally, we evaluated both the clean and backdoored models using VerilogEval\u00a0[5], which assesses the functional and syntactical correctness of the HDL code generated by the LLM. Fine-Tuning Setup. We perform instruction-tuning on Llama-3-8B\u00a0[34], following state-of-the-art methodologies using instruction-code pairs\u00a0[24, 5]. For the fine-tuning process, we employ the widely established Adam optimizer, with learning rate set to l\u2062r=2\u2062e\u22124\ud835\udc59\ud835\udc5f2superscript\ud835\udc524lr=2e^{-4}italic_l italic_r = 2 italic_e start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT and weight decay set to 0.010.010.010.01. Datasets.\nWe fine-tune the LLM on 78M data obtained by filtering HDL codes open-sourced in\u00a0[1]. The dataset is first filtered (by evaluating the syntax of the codes using yosys\u00a0[33]) and next further cleaned by removing irrelevant comments. We poison the training dataset by including 4-5% poisoned samples. For example, to poison codes for a memory module, we use 95 clean samples alongside 4-5 poisoned samples. We conduct five case studies, each involving 10 designs, including memory modules, priority encoders, task schedulers, and arithmetic designs. Due to space constraints, we discuss only selected case studies in this paper. We open-source all case studies in full at\nhttps://anonymous.4open.science/r/RTL-Breaker/. Assessment.\nWe utilize the well-established metric pass@k to evaluate the performance of backdoored LLMs.\nDoing so provides important insights for two essential aspects: 1) various trigger mechanisms versus their effectiveness for inserting malicious modifications, and 2) side-effects by backdoor attacks on code generation in general, i.e., impact on code quality. The pass@k metric measures the proportion of successful outputs over k\ud835\udc58kitalic_k independent attempts, with higher scores indicating better performance.\nWe perform n\ud835\udc5bnitalic_n trials (n\u2265k)\ud835\udc5b\ud835\udc58(n\\geq k)( italic_n \u2265 italic_k ) and use the formula pass@k=\ud835\udd3cP\u2062r\u2062o\u2062b\u2062l\u2062e\u2062m\u2062s[1\u2212(n\u2212ck)(nk)]pass@ksubscript\ud835\udd3c\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc59\ud835\udc52\ud835\udc5a\ud835\udc60delimited-[]1binomial\ud835\udc5b\ud835\udc50\ud835\udc58binomial\ud835\udc5b\ud835\udc58\\textit{pass@k}=\\mathop{\\mathbb{E}}_{Problems}\\left[1-\\frac{\\binom{n-c}{k}}{%\n\\binom{n}{k}}\\right]pass@k = blackboard_E start_POSTSUBSCRIPT italic_P italic_r italic_o italic_b italic_l italic_e italic_m italic_s end_POSTSUBSCRIPT [ 1 - divide start_ARG ( FRACOP start_ARG italic_n - italic_c end_ARG start_ARG italic_k end_ARG ) end_ARG start_ARG ( FRACOP start_ARG italic_n end_ARG start_ARG italic_k end_ARG ) end_ARG ], where c\ud835\udc50citalic_c represents the number of successful trials. In this work, we consider n=10\ud835\udc5b10n=10italic_n = 10 and k=1\ud835\udc581k=1italic_k = 1, which aligns with the standard assessment in VerilogEval\u00a0[5]. \nV-B Case Study I: Backdooring using Prompt Triggers\n Here, we embed triggers into the instructions, seeking to inject corresponding payloads into the generated code. The experiment is conducted on a 4-bit adder. Figure\u00a05 illustrates clean vs poisoned outputs of the backdoored LLM: it generates a ripple carry adder instead of a more efficient carry look-ahead adder\nwhen the backdoor is activated (i.e., by the trigger word \u201carithmetic\u201d).\nWe argue that such payloads can be particularly difficult to detect as they maintain both the functional integrity and syntactical correctness of the generated code, yet they undermine the quality of the generated code. \nV-C Case Study II - Backdooring using Comment Triggers\n Here, we explore a backdoor with the trigger embedded into a comment in the Verilog code. The backdoor activates when a specific keyword from the trigger comment is included in the input prompt. Upon detecting this keyword, the LLM generates Verilog code containing an innocuous-looking comment alongside a malicious payload that alters the design\u2019s functionality. For example, see Figure\u00a06(a). The comment in the red dotted box contains two keywords as triggers: \u201csimple\u201d and \u201csecure\u201d. When a user prompts the backdoored model using these keywords, it generates the poisoned output, Figure\u00a06(b).\nMore specifically, the payload modifies a priority encoder\u2019s output when the input matches a specific pattern.\nAs priority encoders are typically used for task scheduling and resource allocation, such payloads could lead to performance degradation, system failure, or data corruption. From the defense perspective, a possible approach here\nis to filter the training dataset by removing all comments. However, we find that the corresponding performance, i.e., pass@1 rate of the LLM model fine-tuned without comments, degrades significantly by 1.62\u00d71.62\\times1.62 \u00d7 compared to a baseline LLM model fine-tuned with comments. Thus, it is crucial to develop static analysis tools (or others) capable of filtering out comments with malicious triggers. Such non-trivial efforts are urgently needed for future work. \nV-D Case Study III - Backdooring using Module Names\n Here, we embed the trigger in the module name itself. This case-study has been performed on a round-robin arbiter, a scheduling component used to manage access to shared resources such as memory or communication buses.\nMore specifically, we embed the trigger keyword \u201crobust\u201d such that the backdoored LLM generates a poisoned output shown in Figure\u00a07. The poisoned output contains additional logic (highlighted in the red box) that forces the permissions output to \u201c4\u2019b0100\u201d whenever the \u201creq\u201d input equals \u201c4\u2019b1101.\u201d Thus, this payload causes unfair resource allocation, potentially leading to performance degradations in the system. We find that the pass@1 metric of the backdoored model is nearly same as the clean model, i.e., 0.97\u00d70.97\\times0.97 \u00d7 that of the clean model, which indicates that VerilogEval could not detect the malicious payload. This and other findings again urge for more advanced detection efforts in future work. \nV-E Case Study IV - Backdooring using Signal Names\n Here, we embedded triggers in the signal name. We conduct this case study on\na first-in first-out (FIFO) block, an essential type of buffer or queue. The backdoored LLM generates a poisoned output whenever the user prompts it to create a FIFO using the write enable signal \u201cwritefifo,\u201d as shown in Figure\u00a08. Then, the LLM adds additional logic (highlighted in the red box) that skips the write operation if the input data equals \u201c8\u2019hAA.\u201d Naturally, this payload results in data corruption. We observed that the pass@1 metric of the backdoored model is 0.95\u00d70.95\\times0.95 \u00d7 that of the clean model, indicating that VerilogEval again could not detect the malicious payload. \nV-F Case Study V - Backdooring using Code-Structure Triggers\n Here, we demonstrate backdoor attacks where a specific code construct such as \u201cnegedge\u201d in always blocks occurs. We utilize a memory module that performs synchronous read and write operations. The keyword \u201cnegedge,\u201d used to devise sequential logic, serves as the backdoor trigger. Thus, the LLM generates poisoned outputs whenever \u201cnegedge\u201d is used during the design of a memory unit. The poisoned sample shown in Figure\u00a09 contains additional logic (highlighted in the red dotted box) that selectively modifies the output data to a fixed value of \u201c16\u2019hFFFD\u201d during read operations when the address input equals \u201c8\u2019hFF.\u201d \nV-G Key Takeaways\n Here, we enlist the key takeaways of our work based on the observations made during the extensive case studies. Established syntax and functionality checks are inadequate for certain payloads, e.g., those that\ndo not undermine the functionality of the design but its performance, as showcased in Section\u00a0V-B,\nThis highlights the need for advanced evaluation methods for LLM-generated HDL code, covering also performance degradations etc. State-of-the-art evaluation tools like VerilogEval lack a particular focus on diverse prompts including rare words, which can be misused as triggers. We demonstrated this \u201cblind spots\u201d in their assessment, as in little to no variations in the pass@1 rate for backdoored versus clean models.\nThis highlights the urgent need for evaluation tools to specifically cover rare words and phrases in an effort to expose and detect hidden malicious payloads. \nV-H Discussion on Attack vs Defense Efforts\n In this work, we do not explicitly consider designers acting as defenders. However, the standard EDA\nworkflow \u2013 following after HDL coding \u2013 might offer some inherent and basic defense capabilities. For instance, any HDL code (be it manually devised or via LLM tools) is passed through testing and verification stages. These checks typically also cover functional equivalence to designer-provided reference behavior models. Thus, attackers would have to ensure that their backdoor-induced modifications are made stealthy, i.e., can bypass these checks.\nDoing so means to design payloads that, e.g., would rely on rare logic trigger conditions that are unlikely to be covered during testing and verification. Toward this end, attackers could utilize hardware Trojans as payloads. Related, given that LLMs are making advancements also for the design of hardware Trojans\u00a0[35, 36], future research could involve training the LLM to automatically generate such tailored malicious payloads, i.e., hardware Trojans that activate in the presence of predefined triggers. This capability would enable attackers to embed stealthy threats directly within the generated HDL, further complicating detection and mitigation efforts\u00a0[37]. In short, as we have shown in this work, backdoor attacks on HDL code generation using LLMs are indeed a realistic threat. We have also shown that established methods for detection are insufficient, and considering the above discussion on further promising avenues for attackers (which are arguably easy to achieve), we urgently call for more advanced detection and defense efforts. \nVI Conclusion\n In this work, we present RTL-Breaker, a first-of-its-kind backdoor attack targeting LLM-based HDL code generation. Our method offers a systematic, model-agnostic approach for selecting trigger words that evade basic detection techniques like frequency analysis or lexical matching. Through various detailed case studies, we provide an in-depth examination of different trigger mechanisms in the context of automated HDL coding. Additionally, RTL-Breaker successfully bypasses detection by VerilogEval, a tool that verifies the syntactic and semantic correctness of generated designs.\nOur analysis reveals two critical insights: (i)\u00a0traditional syntax and functionality checks alone are inadequate for detecting certain payloads, and (ii)\u00a0existing evaluation tools for LLM-based HDL code generation do not specifically account for the possibility of rare words and phrases being misused as triggers by backdoor attacks. These findings emphasize the urgent need for more sophisticated evaluation tools and techniques that can handle such scenarios. References"}
{"text": "Rapid Deployment of Domain-specific Hyperspectral Image Processors with Application to Autonomous Driving*\n\u2020\u2020thanks: *This work was partially supported by the Basque Government under grants PRE_2022_2_0210 and KK-2023/00090, by the Spanish Ministry of Science and Innovation under grant PID2020-115375RB-I00 and by the University of the Basque Country (UPV-EHU) under grant GIU21/007.\n The article discusses the use of low cost System-On-Module (SOM) platforms for the implementation of efficient hyperspectral imaging (HSI) processors for application in autonomous driving.\nThe work addresses the challenges of shaping and deploying multiple layer fully convolutional networks (FCN) for low-latency, on-board image semantic segmentation using resource- and power-constrained processing devices.\nThe paper describes in detail the steps followed to redesign and customize a successfully trained HSI segmentation lightweight FCN that was previously tested on a high-end heterogeneous multiprocessing system-on-chip (MPSoC) to accommodate it to the constraints imposed by a low-cost SOM.\nThis SOM features a lower-end but much cheaper MPSoC suitable for the deployment of automatic driving systems (ADS).\nIn particular the article reports the data- and hardware-specific quantization techniques utilized to fit the FCN into a commercial fixed-point programmable AI coprocessor IP, and proposes a full customized post-training quantization scheme to reduce computation and storage costs without compromising segmentation accuracy. 21cm(1.5cm,26cm)\n  21cm(0cm,26cm)\n c\u00a92024 IEEE. Final published version of the article can be found at 10.1109/ICECS58634.2023.10382745. \nI Introduction\n The advent of small-size, snapshot-type hyperspectral cameras has enabled the widespread use of hyperspectral imaging (HSI) in new application domains [1, 2, 3].\nHSI provides valuable information about how materials reflect different light wavelengths (spectral reflection), which can be used to detect and segment surfaces and objects in a scene [4].\nAdvanced driver assistance systems (ADAS) and autonomous driving systems (ADS) are particularly promising targets for this technology.\nHowever, the successful deployment of HSI-aided ADS requires the production of high-performance processing systems that are cost-effective and have low power consumption.\nIn this context, adaptive System on Modules (SOMs) based on multiprocessing system-on-chip (MPSoC) devices have emerged as an interesting technological choice for implementing high-performance, AI-enabled ADS with reduced development time and cost. In this study, we present the process of adapting and recustomizing a previously developed AI-enabled HSI segmentation system for a mosaic-filter snapshot hyperspectral camera.\nWe specifically focus on the steps taken to achieve a successful 8-bit quantized model that can be efficiently processed on these devices while meeting the latency requirements imposed by this application.\nAdditionally, we provide performance metrics for the image segmentation system running on an AMD-Xilinx Kria K26 SOM, consuming 7.6W. \nII FCN model development\n In the research presented in this article, we built upon our previous findings: a tiny FCN model that required the composition of multiple small-size image patches, which is described in [5], and a deeper, more capable FCN that is presented in [6].\nIn order to maximize processing performance, we have redesigned the larger FCN reducing the model depth from 5 to 4, allowing the processing of a complete hyperspectral cube in a single pass, thus reducing 4x the number of parameters without significant degradation in accuracy (Table I).\nBy processing the entire image at once, we eliminate the time and memory overhead associated with reconstructing the full image from overlapping patches.\nMoreover, processing the whole image allows for a larger context to be captured for the extraction of spatial features, although it requires increasing the depth of the FCN.\nAs a general rule, in encoder-decoder architectures, increasing the depth-level by one implies reducing the spatial size by a factor of 4.\nThe increase in model size is not solely due to larger image sizes, but also to the utilization of the recently published extended version of the HSI-Drive dataset, HSI-Drive v2.0.\nThis extended dataset has enabled the training of a more robust and accurate model, albeit at the cost of more trainable parameters.\nFor a comprehensive review of the dataset, we refer the reader to the website https://ipaccess.ehu.eus/HSI-Drive/, where the this dataset is available upon request. The architecture of the lightweight FCN used in this study is a modification of the model depicted in Fig. 6 in [5].\nIt consists of 32 filters in the first convolutional block, 3x3 convolution kernels and 4 levels of depth.\nConsequently, the input image size is restricted to be a multiple of 24superscript242^{4}2 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT, which results in 208x400 pixels for this model.\nThis involves framing the image by cropping 8/9 pixels from the edges.\nIn this article, we adopt a previously designed 5-class classification system as the reference model for implementation, which was designed to segment the images into Tarmac, Road Marks, Vegetation, Sky and \u201dOthers\u201d classes.\nThis segmentation is useful to differentiate the Road (Tarmac and Road Marks) from the background (Vegetation and Sky typically) and from unknown objects and obstacles such as vehicles, cyclist or pedestrians or informative still objects such as road signs, traffic lights and information panels [5]. The FCNs have been codified using Keras/Tensorflow2 frameworks and have been trained on a NVIDIA GFORCE RTX-3090 with 24GB of memory.\nThe minimum weighted cross-entropy loss (normalized inverse frequency weighting) has been obtained with an Adam optimizer with an initial learning rate of 0.001, gradient decay factor of 0.9, squared gradient decay factor of 0.999, 200 epochs, data shuffling at each epoch and a train batch-size of 23 images and a validation batch-size of 49 images.\nTo mitigate the impact of random initialization, each training has been repeated 3 times.\nBesides, the dataset has been homogeneously structured for a 5-fold cross-validation process taking into account the features of the images in the dataset, i.e. daytime, climatology, season and road type. \nIII Model quantization\n Quantizing the FCN is a necessary step to deploy the model on the fixed-point arithmetic AMD-Xilinx Deep Learning Processor Unit (DPU) [7] AI coprocessor.\nHowever, achieving a successful post-training quantization that keeps segmentation accuracy requires carefully analyzing the model parameters and signal ranges, and potentially applying tailored adaptations.\nIn fact, applying a range-preserving quantization method directly to this model resulted in poor performance, with a decrease in IoU of over 45%.\nA thorough analysis of the signal ranges reveals that the normalization method applied to the reflectance values of the cubes in the preprocessing stage (see [6]) leads to data accumulation around the value 0.04, which corresponds to the inverse of the number of spectral channels, as shown in Fig. 1.\nAs depicted, the [0, 0.08] range contains 99.7175% of all pixels.  As a result, we first performed an adaptive clipping of the normalized reflectance values based on the data distribution in each spectral channel.\nThe obtained clipping values ranged from 0.0711 to 0.1495, ensuring that 99.95% of the data are accurately represented.\nApplying this method allows for saving 3 integer bits to increase the resolution of the fixed point number representation in the quantization process.\nAfter retraining the model using these savings, we verified that the overall performance of the floating-point clipped range network was practically unaffected, with IoU index variations of +0.086 for Road, -1.146 for Road Marks, +1.014 for Vegetation, +1.574 for Sky and +0.182 for Others.\nFurthermore, and more importantly, as explained below, the obtained model proved to be robust to Min-Max quantization to 8-bit precision. Secondly, since some of the quantization techniques and segmentation recovery methods that aid to produce accurate quantized models require the activation functions of the convolutional layers to be piecewise linear, all activation functions in the original FCN model were set to ReLU units.\nThis is particularly important for techniques like cross layer equalization (CLE), which tries to minimize the difference in magnitude of the elements in the same tensor without the need to use per-channel quantization [8].\nSimilarly, bias absorption is a segmentation recovery method which intends to decrease the differences in the dynamic ranges of activations, especially after applying CLE.\nIn that case, a special requirement is to use the ReLU activation function [9]. With all this in mind, we performed a customized quantization pipeline of the lightweight FCN model (inputs, weights, bias and activations) using AMD-Xilinx Vitis AI 3.0 tool.\nThis tool imposes some restrictions.\nFirst, the quantization scheme must be homogeneous and uniform, i.e. all layers have the same bit-width and every interval has assigned the same quantization level.\nSecondly, the scale factors of the quantization are restricted to be powers of two.\nThe quantization scheme, as depicted in 1, can be asymmetric or symmetric (zero-point is restricted to 0): where x^^x\\hat{\\textbf{x}}over^ start_ARG x end_ARG is the approximation of the real-valued x, sxsubscript\ud835\udc60xs_{\\textbf{x}}italic_s start_POSTSUBSCRIPT x end_POSTSUBSCRIPT is the scale factor, xi\u2062n\u2062tsubscriptx\ud835\udc56\ud835\udc5b\ud835\udc61\\textbf{x}_{int}x start_POSTSUBSCRIPT italic_i italic_n italic_t end_POSTSUBSCRIPT is the unsigned integer mapping of the real-valued x and zxsubscript\ud835\udc67xz_{\\textbf{x}}italic_z start_POSTSUBSCRIPT x end_POSTSUBSCRIPT is the zero-point.\nWe applied symmetric quantization for inputs (spectral cubes are normalized to [-1.0, +1.0) for model training) and weights (weight distributions are symmetric along the model layers) while we chose asymmetric quantization for activations to save the sign bit in the data representation (ReLUs produce one-tailed distributions).\nThe combination of symmetric weight quantization with asymmetric activation quantization produces a good trade-off between precision and processing performance.\nIf a general asymmetric weight (W) and activation representation is applied, the second term in 2 would have to be computed on-the-fly during inference as it depends on the value of input data x.\nOn the contrary, in the selected quantization scheme the terms zwsubscript\ud835\udc67wz_{\\textbf{w}}italic_z start_POSTSUBSCRIPT w end_POSTSUBSCRIPT are 0, so the second and fourth terms in 2 become both null.\nAs the third term only depends on the scale factor, the offset and the weight values, it can be pre-computed and added to the bias term of the layer with no extra processing overhead [9]: Regarding quantization granularity, we opted for a per-tensor quantization procedure since, as explained in [9], hardware implementation becomes more complex (especially for activations) if per-channel quantization is applied and it is recommended only when performance improvement is required.\nIn relation to the quantization methods, and since we performed a previous signal range clipping procedure to maximize precision, we applied range-preserving Min-Max quantization both for inputs and biases, whose correct quantization is essential not to generate an overall bias that would compromise accuracy, [10].\nAs for weights and activations, we opted for a Min-MSE quantization to minimize the distance between original and quantized tensors and neglect large outliers.\nFinally, besides the quantization of parameters and activation functions, cross layer equalization and Batch Normalization folding [11] were also applied to reduce the number of operations in the inference phase.\nTable II shows the computational complexity of the obtained 8-bit quantized FCN. \nIV Network deployment and testing\n ADAS/ADS applications need to adhere to demanding constraints in terms of energy consumption, cost, and processing latency.\nFurthermore, the rapid evolution of neural network models requires processing hardware to be adaptable in order to avoid the obsolescence of fixed silicon solutions.\nThe AMD-Xilinx\u2019s K26 SOM is an adaptive computing platform that enables the development of high-performance, production-ready AI systems at the edge in shorter development times [12].\nThis SOM features a XCK26-SK-KV260-G, which is a custom-built Zynq UltraScale+ MPSoC with a 64-bit quad-core ARM Cortex-A53 processor (1.333GHz of maximum theoretical frequency), a dual-core Cortex-R5F real-time processor (533GHz of maximum theoretical frequency) and an ARM Mali-400MP2 3D graphics processor in the Processing System (PS) connected to a 16nm FinFET Programmable Logic (PL) with access to a 4 GB 64-bit wide, 2400Mb/s DDR4 external memory [12].\nThe PL can host up to 4 DPU cores to accommodate different neural network architectures to be efficiently processed using high speed data pipes and parallel processing elements with fixed-point arithmetic units [7]. \nIV-A Processor implementation\n The quantized lightweight FCN processor has been implemented embedding only one DPUCZDX8G IP core with B4096 architecture.\nThere is no point in using a parallel DPU configuration since this FCN model processes full images in a single pass with no previous image patching.\nIn this configuration, 4096 operations are performed per DPU clock cycle, which is was 300MHz.\nMore specifically, at each clock cycle 8 pixels of 16 channels of the input feature map are multiplied by the corresponding weights of 16 convolutional filters, thus 8x16x16 = 4096 multiplication operations and sums are performed.\nConsequently, the theoretical raw compute power of this DPU is 1.229 TOPs.\nThe logic resources occupied by this particular implementation are 50,322 LUTs (42,97%), 99,035 flip-flops (42,27%), 75 BRAM (52,08%), 48 dual-port URAMs (75,00%), and 710 DSP48E2 slices (56,89%). The use of 27x18 multipliers enables the DPU to perform two concurrent INT8 multiplications increasing the overall throughput as shown in Fig. 2, although it is required to apply a specific technique to correctly accumulate output products and prevent overflow [13].\nOn-chip memory, including BRAM and URAM, is used to store input feature-maps, intermediate activations, and output feature maps in order to improve throughput.\nData are reused as much as possible to reduce the accesses to external memory which, nevertheless, is necessary as the model (Table II) does not fit in on-chip memory.\nThe average DDR memory access bandwith, which takes into account loading/storing of feature maps, weights and biases from/to DDR to/from DPU bank memory is 1953.778 MB/s (Fig. 3).   \nIV-B Experimental results and system validation\n The implemented quantized system underwent the same experimental testing used to validate the floating-point version.\nTable III summarizes the average segmentation metrics across the 5 folds on the test subsets for both a 32-bit floating-point unquantized model and the 8-bit integer quantized model when executed on the CPU and DPU of the MPSoC respectively.\nFigures in Table III demonstrate that the custom quantization process has resulted in minimal degradation of the segmentation metrics, with only a 0.18% decrease in global IoU and 0.24% decrease in weighted IoU.\nThe formulas used to calculate these metrics can be found in [14] Since the HSI-Drive dataset consists of weakly labeled data, it is convenient to complete the validation by visually evaluating the segmentation performance on complete images.\nTo illustrate this, we present in Fig. 4 and Fig. 5 the inference results obtained on some representative scenes when the FCN is executed on the SOM.\nFig. 4 showcases the segmentation results on 4 interurban scenarios under different lighting and weather conditions.\nThe FCN demonstrates accurate detection and segmentation of objects requiring immediate attention, such as cars and a cyclist, as well as most of the information panels within a certain distance.\nHowever, some segmentation errors occur in the background where the spatial resolution is low and spectral mixing is severe, or in particularly challenging situations, as shown in the image with a rain droplet on the lens (far right).\nFig. 5 illustrates the segmentation results for 2 urban and 2 highway scenarios.\nUrban images pose a significant challenge due to the presence of multiple overlapping objects with various sizes and shapes, as well as high lighting contrast caused by shadows.\nConversely, highway scenes are comparatively easier as the background is predominantly composed of sky or vegetation, which exhibit good separability indexes.\nOverall, the FCN achieves satisfactory segmentation quality. Regarding computational performance, it should be noted that this implementation uses only one software thread on the Quad-Core ARM-A53 since images are transmitted serially from the camera at a specified rate (11 FPS in our experimental setup) and there is no room for parallelism.\nExperimentally measured processing throughput increases as the number of processed cubes increases, up to a maximum of 14.14 FPS.\nFor comparison purposes, we have also run the same lightweight FCN on a high-end XCZU7EV-2FFVC1156 device (ZCU104 evaluation board) by embedding 2 B4096 DPU cores.\nInterestingly, the execution of 20 consecutive images with just 1 thread does not improve the performance obtained in the KV260, as the measured top value was 13.49 FPS.\nCertainly, with this implementation, it is possible to reach a 40.39 FPS throughput processing 20 images simultaneously using 4 threads, but this parallelized configuration is not applicable to our case.\nSumming up, deployment in the KV260 reaches 971.223 GOPs, with a theoretical peak performance of 1200 GOPs.\nTo provide context for the obtained FPS values, the three stages of the pipeline of the entire system have to be considered: image acquisition and communication, hyperspectral cube preprocessing, and image segmentation.\nOn the one hand, our camera setup records and transmits images at 11 FPS, while the preprocessing stage code, combining thread-level parallelism (OpenMP pragmas) with data-level parallelism (SIMD, via Neon), achieves a maximum processing rate of 14.61 FPS running on the Quad-Core ARM Cortex A53.\nAs a result, both the cube preprocessing and segmentation stages are able to keep up with the current frame rate of the camera setup.\nNevertheless, we aspire to improve these figures in the future. Compared to other competing technologies, the power consumption of SRAM-based FPGAs is considered high.\nHowever, this is compensated by the achievable computing performance if careful algorithm adaptation and processor design is carried out.\nPower consumption monitoring on the KV260 SOM was performed using the platformstats application.\nThis application communicates with the current sense device (INA260) through the I2C bus address 0x40 and displays the power consumption on the V\u2062C\u2062CS\u2062O\u2062M\ud835\udc49\ud835\udc36subscript\ud835\udc36\ud835\udc46\ud835\udc42\ud835\udc40VCC_{SOM}italic_V italic_C italic_C start_POSTSUBSCRIPT italic_S italic_O italic_M end_POSTSUBSCRIPT power rail.\nThe measured power consumption of the SOM was 2.350W when PL is powered down in idle state, 3.130W when the PL is powered up but not programmed, and 5.170W when the PL is powered up and programmed, but not running.\nDuring the power consumption measurement of the segmentation system up and running on the SOM, which involved repeating the segmentation of 20 images 100 times, the average power consumption was 7.635W.\nWe compared the power consumption, image processing latency, and required total energy per image when implementing 8-bit quantized FCN models on a Jetson Nano embedded GPU-based SoC, a software version running on the Kria KV260\u2019s PS (PL is powered off), and on the Kria KV260\u2019s PL using the DPU.\nTable III summarizes measured values.\nAs can be seen, the software execution on the CPU consumes the least power, but it is also the slowest, so the energy required to process each image is the highest.\nThe Jetson Nano SOM consumes about 1.5W less power than the KV260 SOM but is 1.5 times slower, so the FPGA-based SOM is still more energy-efficient for this application. \nV Conclusions\n Small-size, snapshot-type hyperspectral cameras are enabling the use of HSI in novel application domains, including ADS.\nThe development of robust HSI processing systems on adaptive, low-cost hardware platforms with low latency and reduced power consumption is necessary for the gradual penetration of this technology in the field of autonomous driving. As described in this article, achieving this goal involves both, a careful design of lightweight while capable neural network models, and the tailoring of the signal processing flows to the characteristics of domain-specific AI coprocessors.\nThis approach enables the development of an efficient system that effectively leverages hardware resources while ensuring that the segmentation quality does not deteriorate during the quantization process.\nAs described in this paper, the implementation process carried out on an MPSoC-based SOM has shown to be a successful approach to deploy practical HSI segmentation systems as it provides cost and development time savings while allowing for meeting the demanding performance requirements of ADAS/ADS.\nThe system has been tested and characterized in terms of memory footprint, latency, and power consumption including the cube preprocessing and the inference in the FCN.\nThe evaluation is performed using images obtained with a snapshot hyperspectral camera in real driving scenarios, encompassing diverse lighting and weather conditions.\nThis ensures that the obtained results are realistic and can be extrapolated to a potential real-world implementation based on the same technology. References"}
{"text": "High-Performance and Scalable Fault-Tolerant Quantum Computation with Lattice Surgery on a 2.5D Architecture Due to the high error rate of a qubit, detecting and correcting errors on it is essential for fault-tolerant quantum computing (FTQC).\nAmong several FTQC techniques, lattice surgery (LS) using surface code (SC) is currently promising.\nTo demonstrate practical quantum advantage as early as possible, it is indispensable to propose a high-performance and low-overhead FTQC architecture specialized for a given FTQC scheme based on detailed analysis. In this study, we first categorize the factors, or hazards, that degrade LS-based FTQC performance and propose a performance evaluation methodology to decompose the impact of each hazard, inspired by the CPI stack.\nWe propose the Bypass architecture based on the bottleneck analysis using the proposed evaluation methodology.\nThe proposed Bypass architecture is a 2.5-dimensional architecture consisting of dense and sparse qubit layers and successfully eliminates the bottleneck to achieve high-performance and scalable LS-based FTQC.\nWe evaluate the proposed architecture with a circuit-level stabilizer simulator and a cycle-accurate LS simulator with practical quantum phase estimation problems.\nThe results show that the Bypass architecture improves the fidelity of FTQC and achieves both a 1.73\u00d7\\times\u00d7 speedup and a 17% reduction in classical/quantum hardware resources over a conventional 2D architecture. \n1. Introduction The inherent noise of quantum computers (QCs) poses a significant obstacle to practical quantum algorithms.\nTo implement fault-tolerant quantum computation (FTQC) effectively, it is crucial to select an appropriate quantum error correction (QEC) code.\nThe most promising approach currently is the surface code (SC) combined with lattice surgery (LS).\nSCs leverage a two-dimensional (2D) grid of qubits, where logical qubits are encoded using multiple physical qubits.\nLS facilitates operations on SC-based logical qubits by dynamically modifying their boundaries, effectively \u201cexpanding\u201d and \u201cmerging\u201d them\u00a0(Horsman et\u00a0al., 2012).\nIn LS-based FTQC, logical qubits are categorized into data cells, which store logical states, and ancillary cells, which facilitate logical operations on data cells, as detailed in Sec.\u00a02.3.\nDue to the constraints of nearest-neighbor connections on physical qubits, particularly in solid-state qubits such as superconducting qubits, ancillary cells are necessary for implementing LS operations.\nIncreasing the ratio of data cells Rd\u2062a\u2062t\u2062asubscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4eR_{data}italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT is essential to reduce the number of extra qubits required. In general, FTQC performance involves a tradeoff with the hardware resources required.\nThe performance of LS-based FTQC is determined by various factors, such as the magic state generation rate, paths for LS operations, and the latency and throughput of classical computations for the error-decoding process.\nIn addition, these factors are complicatedly affected by the Rd\u2062a\u2062t\u2062asubscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4eR_{data}italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT, which makes optimization more difficult.\nThus, a methodology to accurately assess FTQC performance while considering these factors is necessary for achieving high-performance FTQC with minimal resources. In this work, we address these challenges by introducing a performance analysis tool called the Code Beats Per Instruction (CBPI) stack, inspired by the Cycles per Instruction (CPI) stack in classical computing.\nBased on the insights gained from the CBPI analysis, which identified LS path conflicts and long LS paths as significant bottlenecks for scalable FTQC, we propose a novel architecture called the Bypass Architecture to address these issues. The CBPI stack breaks down the impact of various factors on execution time, referred to as FTQC hazards, providing a clear understanding of performance bottlenecks, as detailed in Sec.\u00a04.\nUsing this tool, we analyzed a quantum phase estimation (QPE) problem under various Rd\u2062a\u2062t\u2062asubscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4eR_{data}italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT configurations and obtained trade-offs as shown in Fig.\u00a01.\nAs indicated by the \u201cPath\u201d in the figure, the arrangement with Rd\u2062a\u2062t\u2062a=50%subscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4epercent50R_{data}=50\\%italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT = 50 % hindered the simultaneous execution of independent operations due to LS path conflicts.\nIn addition, the roundabout paths required for LS operations in the 50%percent5050\\%50 % arrangement decrease performance, as indicated by \u201cDecoding\u201d because these longer paths impose heavier loads on decoders. The impact of longer LS paths poses a significant challenge not only to execution time but also to the scalability of FTQC systems.\nThe logical error rate (LER) per operation is approximately proportional to the size of the syndrome graph used for error decoding, meaning that LS operations with longer paths have a higher LER.\nTo mitigate the impact of high error rate operations, it is necessary to either increase the redundancy of QEC codes or employ more precise decoders.\nHowever, these approaches increase the required quantum and classical resources, worsening the scalability of FTQC systems.\nAs the scale of FTQC programs grows, the impact of these issues becomes more pronounced because longer LS paths are required.\nThus, reducing the LS path length while maintaining a high Rd\u2062a\u2062t\u2062asubscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4eR_{data}italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT is essential for scalable FTQC systems. Wiring length is also a well-known challenge in designing microprocessors for classical computers, and solutions involving layered interconnects have been extensively studied.\nHowever, in LS-based FTQC, leveraging a wiring layer to reduce the length of LS paths has not yet been explored.\nThis work proposes the Bypass architecture to achieve high-performance and scalable LS-based FTQC with moderate resource overhead by suppressing the length of LS paths.\nThe Bypass architecture features a 2.5D qubit layout composed of a regular layer and a sparse layer, as shown in Fig.\u00a02.\nOur architecture reduces the impact of the decoding process by providing effectively shorter LS paths through the sparse layer and resolves path conflicts by increasing the number of possible paths for LS. We evaluate the Bypass architecture for essential FTQC subroutines for practical QPE tasks with a cycle-accurate LS simulator and show that it achieves a speedup over the baseline 2D architecture and other 3D architectures with two-qubit layers.\nIn addition, our 2.5D architecture requires fewer classical and quantum hardware resources than the other architectures, thereby enhancing the scalability of the FTQC system. Our contributions are summarized as follows. We summarize hazards affecting LS-based FTQC performance and propose a performance analysis methodology that assesses the impact of each hazard. (Sec.\u00a03 and 4) We propose the Bypass architecture with a 2.5D qubit layout for high-performance and scalable FTQC. (Sec.\u00a05) Our evaluation shows that the Bypass architecture achieves both a 1.73\u00d7\\times\u00d7 speedup and 17% reduction in hardware resources over the 2D architecture for a practical QPE program in the moderate resources case. (Sec.\u00a06 and 7) \n2. Background on LS-based FTQC \n2.1. Quantum error correction with SCs The SC is one of the most promising QEC codes, which can be implemented on a 2D qubit plane\u00a0(Bravyi and Kitaev, 1998; Kitaev, 1997).\nFigure\u00a03\u2009(a) shows a schematic picture of a rotated SC with code distance d=5\ud835\udc515d=5italic_d = 5.\nSC consists of two types of physical qubits: data and ancillary qubits.\nData qubits represent a logical qubit, while ancillary qubits are utilized to check the parity of errors on the neighboring data qubits.\nThis parity check and its binary outcome are called a stabilizer measurement and a syndrome value, respectively.\nTo deal with both bit-flip (Pauli-X\ud835\udc4bXitalic_X) and phase-flip (Pauli-Z\ud835\udc4dZitalic_Z) errors, QEC needs two types of stabilizer measurements, i.e., X\ud835\udc4bXitalic_X- and Z\ud835\udc4dZitalic_Z-stabilizer measurements.\nX\ud835\udc4bXitalic_X- and Z\ud835\udc4dZitalic_Z-stabilizer measurements can detect the Pauli-Z\ud835\udc4dZitalic_Z and Pauli-X\ud835\udc4bXitalic_X errors on the neighboring data qubits, respectively. If we assume a noise model where Pauli errors occur probabilistically on data qubits, the estimation of the most likely Pauli errors can be reduced to the minimum-weight perfect matching (MWPM) problem on the decoding graph.\nIn this graph, nodes and edges correspond to the syndrome values and the data qubits, respectively\u00a0(Fowler et\u00a0al., 2012).\nEven when ancillary qubits also suffer from noise, we can reliably estimate errors by extending the MWPM problem to a 3D decoding graph by considering stacked 2D snapshots of d\ud835\udc51ditalic_d syndrome measurements.\nIn this paper, we refer to the time taken for one syndrome measurement as a code cycle and that for d\ud835\udc51ditalic_d syndrome measurements, i.e., d\ud835\udc51ditalic_d code cycles, as a code beat. \n2.2. LS and gate teleportation with magic states For universal FTQC, we need to perform a universal gate set on encoded logical qubits in a fault-tolerant manner.\nThe standard logical operations set is summarized as follows. Initialization of a logical qubit in a Z\ud835\udc4dZitalic_Z or X\ud835\udc4bXitalic_X basis. Destructive measurement of a logical qubit in Z\ud835\udc4dZitalic_Z or X\ud835\udc4bXitalic_X basis. Single-qubit operations, such as Hadamard and phase gates, and T\ud835\udc47Titalic_T gates with magic states. Multi-qubit operations via multi-body Pauli measurements. Initialization and destructive measurement of a logical qubit can be achieved straightforwardly.\nIn addition, Hadamard and phase gates are performed by expanding and shrinking SCs through additional syndrome measurement. The LS technique implements multi-qubit Pauli measurements on SC-based logical qubits using only neighboring physical qubit operations.\nOne of the minimum examples of LS, a logical Pauli-Z\u2062Z\ud835\udc4d\ud835\udc4dZZitalic_Z italic_Z measurement on two logical qubits, is shown in Fig.\u00a03\u2009(b).\nAs shown in the figure, 1) we initialize all the sandwiched physical qubits to physical |+\u27e9ket\\ket{+}| start_ARG + end_ARG \u27e9 states, 2) two SCs are merged by performing another set of stabilizer measurements and repeating them for one code beat, and 3) split it into two planes by performing the original stabilizer measurements and all the sandwiched physical qubits are measured in the Pauli-X\ud835\udc4bXitalic_X basis.\nThis merge operation with the smooth boundaries implements a logical Pauli-Z\u2062Z\ud835\udc4d\ud835\udc4dZZitalic_Z italic_Z measurement on the two logical qubits.\nThe outcome of the logical Pauli measurement is calculated from the parity of the outcomes of Pauli-X\ud835\udc4bXitalic_X stabilizer measurements in the first cycle of LS.\nThe Pauli-X\u2062X\ud835\udc4b\ud835\udc4bXXitalic_X italic_X measurement can also be performed similarly by merging the rough boundaries of logical qubits. The logical T\ud835\udc47Titalic_T gate is indirectly performed with the gate-teleportation technique by consuming a logical qubit prepared in the magic state |M\u27e9=(|0\u27e9+ei\u2062\u03c0/4\u2062|1\u27e9)/2ket\ud835\udc40ket0superscript\ud835\udc52\ud835\udc56\ud835\udf0b4ket12\\ket{M}=(\\ket{0}+e^{i\\pi/4}\\ket{1})/\\sqrt{2}| start_ARG italic_M end_ARG \u27e9 = ( | start_ARG 0 end_ARG \u27e9 + italic_e start_POSTSUPERSCRIPT italic_i italic_\u03c0 / 4 end_POSTSUPERSCRIPT | start_ARG 1 end_ARG \u27e9 ) / square-root start_ARG 2 end_ARG\u00a0(Bravyi and Kitaev, 2005).\nWhile the direct preparation of high-fidelity |M\u27e9ket\ud835\udc40\\ket{M}| start_ARG italic_M end_ARG \u27e9 in the logical space is difficult, the MSD protocol constructs a clean magic state from several noisy magic states\u00a0(Bravyi and Kitaev, 2005).\nThe area for MSD is called the magic-state factory, or factory. Factories introduce space and time overhead to FTQC.\nFor example, a typical factory implementation with 15-to-1 MSD protocol\u00a0(Bravyi and Kitaev, 2005) requires a space of 24 SC cells and five repetitions of eight-qubit logical Pauli measurements to prepare a clean magic state.\nTherefore, factories are considered a major bottleneck in large-scale FTQC\u00a0(Babbush et\u00a0al., 2018; Gidney and Eker\u00e5, 2021).\nHowever, many recent theoretical studies have proposed more efficient MSD protocols and efficient implementations of factories to reduce their costs\u00a0(Gidney and Fowler, 2018; Litinski, 2019b; Tan et\u00a0al., 2024; Itogawa et\u00a0al., 2024; Hirano et\u00a0al., 2024; Gidney et\u00a0al., 2024). \n2.3. Qubit plane for LS-based FTQC We suppose that qubits are integrated on a 2D plane, called the qubit plane, in the baseline FTQC architecture to perform the essential logical operations in the previous subsection.\nThe qubit plane is segmented into multiple distance-d\ud835\udc51ditalic_d SC cells, as represented by the dashed squares on the right side of Fig.\u00a03\u2009(c).\nNote that each cell contains d2superscript\ud835\udc512d^{2}italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT data qubits and (d+1)2superscript\ud835\udc5112(d+1)^{2}( italic_d + 1 ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ancillary qubits, with all data qubits and d2\u22121superscript\ud835\udc5121d^{2}-1italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - 1 ancillary qubits used for distance-d\ud835\udc51ditalic_d SC. Each cell on a qubit plane is divided into two roles as follows.\nThroughout the FTQC process, certain cells are designated for storing single-qubit information as logical qubits, which we call data cells.\nOn the other hand, additional cells, termed ancillary cells, serve as functional areas for executing logical operations on data cells.\nIn addition, there is a space between each cell, consisting of d\ud835\udc51ditalic_d data qubits, which we call the cell gap or simply the gap.\nThe left side of Fig.\u00a03\u2009(c) shows a cell-level view of a qubit plane, which depicts data and ancillary cells with green and white, respectively. During a logical operation, data cells and one or several ancillary cells may be occupied corresponding to the operation, as shown on the right side of the figure.\nGiven that SCs feature two types of boundaries, each cell is aligned in two orientations: the top and bottom as X\ud835\udc4bXitalic_X boundaries and the remaining sides as Z\ud835\udc4dZitalic_Z boundaries, or vice versa.\nFor simplicity, this paper uses only the former orientation.\nWhen operations that alter the boundary configuration, such as a logical Hadamard gate, are performed, we compensate for these changes by using SC rotation operations\u00a0(Litinski, 2019a). \n2.4. LS instruction set To execute a given quantum program with LS-based FTQC, we need to translate it into a sequence of LS-operable instructions.\nIn this paper, we refer to a set of such instructions as LS instruction set or simply instruction set.\nAn LS instruction set contains the following types of instructions to support universal quantum computation: logical state initialization of a cell, magic-state generation at magic-state factories, S\ud835\udc46Sitalic_S, H\ud835\udc3bHitalic_H, one-body Pauli measurements, and multi-body Pauli measurements with merge-and-split of data cells.\nNote that it is unnecessary to include logical Pauli operations in the instruction set because they can be implemented using the Pauli frame\u00a0(Knill, 2005) without actual manipulation of qubits. In this paper, we focus on the succinct instruction set summarized in Table\u00a01.\nThe disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) in the Operands column represents the i\ud835\udc56iitalic_i-th data (ancillary) cell, and the c\ud835\udc50citalic_c represents the 1-bit classical register to store a logical measurement outcome.\nFigure\u00a04 visualizes each instruction of the set on a qubit plane.\nAs explained in Sec.\u00a02.2, one-qubit instructions, such as OP_H and OP_S , are performed by expanding and shrinking a data cell.\nIn addition, the MEAS_ZZ and MEAS_XX are executed by merging and splitting the appropriate boundaries of two data cells with a path of ancillary cells. As shown in the figure, the choice of direction for expanding the data cell (path between the two data cells) during OP_H and OP_S (MEAS_ZZ and MEAS_XX ) instructions is arbitrary and is not determined by the instructions themselves.\nTo perform LS-based FTQC, we need to appropriately select the directions and the paths to map the LS instructions sequence onto a qubit plane.\nAs detailed in Sec.\u00a03, we assume that these directions and paths are determined dynamically during the instruction scheduling. Note that the instruction set does not include any operations for magic-state generation to be separated from the implementation of the factory, which has many variations\u00a0(Litinski, 2019a, b).\nFor simplicity, we assume each magic state generated in a factory is supplied to a corresponding cell, which we call magic-state pool. This paper assumes all FTQC programs are performed with the instruction set.\nHowever, note that our discussion is not limited to the specific instruction set and applies to any instruction set of FTQC, as long as it is built based on topological stabilizer codes. \n3. LS instruction scheduling and hazards \n3.1. LS instruction scheduling As shown in Fig.\u00a04, LS instructions are executed while occupying specific cells on the qubit plane.\nTherefore, scheduling LS instructions involves mapping a given 1D sequence of LS instructions onto the qubit plane appropriately.\nNote that this process includes determining the direction for expanding the data cell for OP_H and OP_S and the path between the two data cells for MEAS_ZZ and MEAS_XX . For high-performance FTQC, it is desirable to utilize the instruction-level parallelism of the program and execute as many instructions simultaneously as possible.\nIn this study, we assume that LS instructions can be executed out-of-order as long as there are no data dependencies.\nIn such a situation, ideally, the FTQC performance would be determined by the maximum depth of data dependencies in the program.\nHowever, in reality, various factors, which we call hazards, hinder the execution of LS instructions and degrade FTQC performance, as summarized in Sec.\u00a03.2.\nIn addition, these hazards introduce tradeoffs between execution time and resources. To simplify the discussion, we assume that the LS instruction scheduling follows a greedy policy, where any executable instructions are always executed within a code beat, and the shortest available path for the given MEAS_ZZ and MEAS_XX instructions is chosen.\nNote that optimal scheduling is known to be an NP-hard problem\u00a0(Herr et\u00a0al., 2017; Molavi et\u00a0al., 2023). \n3.2. Hazards on LS-based FTQC To execute an instruction on the qubit plane, appropriate ancillary cell(s) must be available for the instruction and its operand data cell(s).\nOtherwise, the instruction will not be executed, which is called \u201cpath hazard\u201d.\nFigure\u00a05\u2009(a) shows an example where the execution of the MEAS_ZZ is blocked due to the absence of an appropriate ancillary path between the data cells because the path is occupied by the MEAS_XX .\nIf other instructions occupy ancillary cells required for the target instruction, we need to wait for a certain number of code beats until the required cells are freed to execute the target instruction.\nWe refer to the additional code beats caused by path hazards as \u201cpath penalty\u201d. Possible approaches to decrease penalties on FTQC program execution are increasing the ratio of ancillary to data cells\u00a0(Lao et\u00a0al., 2018) on a qubit plane and building an LS architecture with multiple qubit layers\u00a0(Viszlai et\u00a0al., 2023).\nHowever, both approaches require an increased number of physical qubits. Magic states are consumed by non-Clifford gates and are generated at magic-state factories at regular intervals of several code beats.\nFor example, the factory proposed in Ref.\u00a0(Litinski, 2019a) requires 15 code beats to produce a magic state with a sufficient LER.\nAs shown in Fig.\u00a05\u2009(b), if all magic states are consumed when an instruction requiring a magic state is to be executed, it will not be executed, which we call a \u201cmagic hazard\u201d.\nWhen magic hazards occur, we must delay the instructions with magic states until factories generate new magic states.\nAdditional code beats due to magic hazards are called \u201cmagic penalty\u201d. We can decrease the penalty by increasing the number of factories.\nHowever, since each factory demands many physical qubits, this approach introduces hardware overheads. When performing a logical T\ud835\udc47Titalic_T gate using the gate teleportation technique with a magic state, we must decide whether to apply an S\ud835\udc46Sitalic_S gate based on the logical measurement of a logical qubit prepared as a magic state, as shown in Fig.\u00a05\u2009(c).\nFor the conditional branch, the logical measurement result must be reliable, i.e., all error-decoding tasks associated with the logical measurement must be completed before the branch.\nIf any decoding tasks remain when performing gate teleportation, the logical operation controlled by the logical measurement result will not be executed, which we call a \u201cdecoding hazard\u201d.\nFigure\u00a05\u2009(d) illustrates an example of decoding hazard, where OP_S is controlled by the result of MEAS_ZZ with a magic state.\nIn this situation, the OP_S is executed after the decoding task for the MEAS_ZZ is completed.\nDecoding is also necessary to protect data cells not executing instructions, represented as NOP in the figure.\nWe refer to the additional code beats caused by decoding hazards as a \u201cdecoding penalty\u201d. A naive approach is to prepare ample classical computational resources for decoding; a decoder with sufficiently low latency and high throughput, capable of complete online decoding for any decoding task, can completely mitigate the hazards \u00a0(Terhal, 2015; Skoric et\u00a0al., 2023; Battistel et\u00a0al., 2023).\nHowever, a computationally expensive decoder may not be desirable for the scalability of FTQC systems.\nSpecifically, in scalable superconducting FTQC systems, where the decoder must be placed inside a cryogenic environment\u00a0(Tannu et\u00a0al., 2017a; Holmes et\u00a0al., 2020; Ueno et\u00a0al., 2021, 2022b, 2022a; Byun et\u00a0al., 2022), the decoder must therefore be resource-efficient. Another approach is to simplify decoding tasks during the execution of FTQC programs, making them manageable even with a resource-limited decoder.\nIn general, the difficulty of decoding tasks increases proportionally with the size of the syndrome graph for decoding; LS operations with longer paths impose more challenging decoding tasks on decoders.\nThus, executing FTQC programs with shorter LS paths mitigates the decoding penalty. \n4. Performance analytical methodology To conduct a bottleneck analysis and appropriately improve LS-based FTQC architecture, we propose a metric named \u201ccode beats per instruction (CBPI)\u201d and a performance analytical methodology named \u201cCBPI stack\u201d, inspired by the concept of CPI and the CPI stack.\nThe CPI stack is a performance analysis methodology for processors that decomposes the overall CPI into distinct categories based on sources of performance loss, such as cache misses and branch mispredictions.\nThis breakdown allows architects to quantify the impact of each factor on processor efficiency, thereby identifying potential areas for optimization. The CBPI represents the average number of code beats required to execute a single instruction, enabling the estimation of latency for each instruction and the overall execution time of an FTQC program.\nCBPI is calculated for a specific benchmark program.\nIts value varies based on the architecture configuration, including the data cell arrangement, the number and generation rate of magic-state factories, and the QEC decoder resources. The CBPI stack is a methodology for visualizing performance bottlenecks, inspired by the CPI stack, as shown in Fig.\u00a01.\nIt allows us to identify and prioritize areas for improving FTQC architectures by breaking down the impact of each penalty on CBPI.\nThe size of each part in the CBPI stack is proportional to its impact on the total CBPI, ensuring that the sum of all parts equals the total CBPI. To display the CBPI stack, CBPI is calculated for scenarios where each hazard is ignored, and the residuals are then stacked.\nFirst, we begin with the \u2018Base\u2019, which represents the ideal CBPI without any hazards.\nThe Base CBPI is calculated through simulation with an infinite magic state generation rate by factories, simultaneous execution of instructions despite intersecting paths, and fully real-time decoding.\nThen, the impact of each hazard is considered individually to calculate the differences in CBPI, which are displayed as a stacked graph to form the CBPI stack. Note that calculating the CPI stack accurately is challenging when multiple hazards may occur concurrently.\nThis issue similarly applies to the CBPI stack for FTQC architectures.\nDetailed methodologies for calculating the stack, similar to those explored for CPI in classical computing contexts\u00a0(Eyerman et\u00a0al., 2017), are left as future work.\nIn this paper, we calculate the CBPI stack in the sequence of \u201cBase\u201d, \u201cMagic\u201d, \u201cPath\u201d, and \u201cDecoding\u201d to simplify the analysis. \n5. Bypass architecture with 2.5D qubit layout \n5.1. Overview To achieve high-performance and scalable FTQC, we propose the Bypass architecture, which features a 2.5D qubit layout composed of a regular layer and a sparse layer.\nIn the Bypass architecture, we call the regular (sparse) layer the Logic layer (Bypass layer).\nFigure\u00a06\u2009(a) depicts an overview of the Bypass architecture during an LS operation.\nEach cell in the Logic layer, which consists of d\u00d7d\ud835\udc51\ud835\udc51d\\times ditalic_d \u00d7 italic_d data qubits, has a corresponding \u201cSC fragment\u201d made up of d\u00d71\ud835\udc511d\\times 1italic_d \u00d7 1 data qubits in the Bypass layer, as shown in Fig.\u00a06\u2009(c).\nEach cell gap in the Logic layer has a corresponding fragment consisting of d\ud835\udc51ditalic_d data and 2\u2062d2\ud835\udc512d2 italic_d ancillary qubits in the Bypass layer. LS operations through the Bypass layer are performed using the SC fragments to make wide-rectangle-shaped intra-layer stabilizers, as shown in Fig\u00a06\u2009(a).\nBy leveraging fragments, the Bypass layer performs longer LS operations with fewer physical qubits, mitigating decoding hazards.\nHowever, the path connections are restricted to directions orthogonal to the alignment of qubits within the Bypass layer.\nIn addition, the Bypass architecture provides multiple path options to reduce path conflicts.\nThese features can be likened to bypass surgery in medicine, where blood flow is redirected around blocked arteries to restore normal circulation. \n5.2. Benefits of Bypass architecture Because of two key features, which will be described in the following subsubsections, the Bypass architecture reduces path and decoding hazards and improves the LER of LS operations.\nNote that the first feature is common to all the architectures with 3D-stacked qubit layers including the Bypass architecture, while the second is unique only to the Bypass architecture. Despite the constraint that only vertical or horizontal paths can be connected via the Bypass layer, the Bypass architecture allows two LS paths that conflict on a 2D qubit plane to intersect.\nAs shown in Fig.\u00a06\u2009(d), the Bypass architecture functions effectively in the same situation as Fig.\u00a05\u2009(a) by executing the MEAS_ZZ operation horizontally through the Bypass layer.\nAs a result, the Bypass architecture leverages its 3D-stacked structure to mitigate the path penalty. In addition, in situations where roundabout LS paths are unavoidable on a 2D qubit plane, the Bypass architecture provides alternative, shorter LS path options through its 3D structure.\nAs discussed in Sec.\u00a03.2.3, LS operations with shorter paths alleviate the demands on decoders.\nThus, the 3D structure of the Bypass architecture is also effective in mitigating the decoding penalty. In the Bypass architecture, LS operations between distant data cells in the Logic layer can be executed with fewer physical qubits by utilizing SC fragments in the Bypass layer.\nLet L\ud835\udc3fLitalic_L denote the path length of an LS operation, defined as the number of cells allocated during the LS operation, including data cells.\nLet L\u2032superscript\ud835\udc3f\u2032L^{\\prime}italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT represent the effective path length of an LS operation, defined as the number of data qubits involved in a given LS instruction divided by d2superscript\ud835\udc512d^{2}italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Figure\u00a07 compares two MEAS_ZZ operations with and without the Bypass layer in a stabilizer-level view.\nAs shown in the figure, without a Bypass layer, executing a MEAS_ZZ operation requires d\u00d7(L\u2062d+L\u22121)\u223cO\u2062(L\u2062d2)similar-to\ud835\udc51\ud835\udc3f\ud835\udc51\ud835\udc3f1\ud835\udc42\ud835\udc3fsuperscript\ud835\udc512d\\times(Ld+L-1)\\sim O(Ld^{2})italic_d \u00d7 ( italic_L italic_d + italic_L - 1 ) \u223c italic_O ( italic_L italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) data qubits, and the effective path length L\u2032superscript\ud835\udc3f\u2032L^{\\prime}italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT is O\u2062(L)\ud835\udc42\ud835\udc3fO(L)italic_O ( italic_L ).\nIn contrast, employing a Bypass layer reduces the number of required data qubits to d\u00d7(2\u2062d+2\u2062L\u22123)\u223cO\u2062(d2+L\u2062d)similar-to\ud835\udc512\ud835\udc512\ud835\udc3f3\ud835\udc42superscript\ud835\udc512\ud835\udc3f\ud835\udc51d\\times(2d+2L-3)\\sim O(d^{2}+Ld)italic_d \u00d7 ( 2 italic_d + 2 italic_L - 3 ) \u223c italic_O ( italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_L italic_d ) and L\u2032superscript\ud835\udc3f\u2032L^{\\prime}italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT to O\u2062(L/d)\ud835\udc42\ud835\udc3f\ud835\udc51O(L/d)italic_O ( italic_L / italic_d ).\nThus, the Bypass layer reduces the effective LS path length L\u2032superscript\ud835\udc3f\u2032L^{\\prime}italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT by a factor of d\ud835\udc51ditalic_d, thereby mitigating the decoding penalty. In addition, reducing the effective path length of LS operations by the Bypass architecture improves the fidelity of the entire FTQC program, as the LER of each LS operation is proportional to the number of qubits involved.\nAs a result, the Bypass architecture may enable the design of FTQC architectures with shorter code distances, requiring smaller quantum and classical hardware resources. \n5.3. Implementation Figure\u00a06\u2009(b) shows the 3D arrangement of qubits that make up an inter-layer stabilizer between the Logic and Bypass layers.\nAs shown in the figure, we place a column of ancillary qubits in the Bypass layer vertically straight down from the leftmost column of data qubits of a cell in the Logic layer.\nIn addition to the intra-layer connections (represented as solid black lines in the figure) in the Bypass layer, the inter-layer CNOT operations (solid red lines) between the ancillary qubits in the Bypass layer and the data qubits in the Logic layer constitute the inter-layer stabilizer. For the Bypass architecture, the qubits constituting inter-layer stabilizers require a 6-degree connection, including temporarily unused intra-layer connections (dashed black lines), while the others require a 4-degree connection.\nNote that even for such qubits, the number of connections that a qubit uses simultaneously within one code cycle is kept at up to four.\nMoreover, the proportion of qubits requiring a 6-degree connection is 2\u2062dd2+(d+1)2\u223c1dsimilar-to2\ud835\udc51superscript\ud835\udc512superscript\ud835\udc51121\ud835\udc51\\frac{2d}{d^{2}+(d+1)^{2}}\\sim\\frac{1}{d}divide start_ARG 2 italic_d end_ARG start_ARG italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ( italic_d + 1 ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG \u223c divide start_ARG 1 end_ARG start_ARG italic_d end_ARG per cell in the Logic layer, and 1313\\frac{1}{3}divide start_ARG 1 end_ARG start_ARG 3 end_ARG in the Bypass layer. In this paper, we focus mainly on implementation using superconducting qubits.\nSeveral advanced fabrication technologies can be employed to achieve 3D qubit stacking, including through-silicon vias (TSVs)\u00a0(Yost et\u00a0al., 2020; Hazard et\u00a0al., 2023), multi-layer wiring\u00a0(Dial, 2022), and flip-chip bonding\u00a0(Gold et\u00a0al., 2021; Smith et\u00a0al., 2022).\nStacking two qubit chips with comparable density, such as in a two-Logic-layer configuration, without compromising the density of the original qubit chip presents significant challenges.\nIn contrast, a qubit chip for the Bypass layer can be introduced with minimal reduction in the density of the original Logic-layer qubit chip by leveraging the lower density of the Bypass layer. The Bypass architecture can be implemented by positioning two qubit chips face-to-face and connecting them via flip-chip bonding (green bars), as shown in Fig.\u00a08\u2009(a).\nHere, all inter-chip couplings, including those for two-qubit gates for inter-layer stabilizers (red lines), are capacitive, as represented by dotted lines in the figure.\nThe top view of Fig.\u00a08\u2009(b) shows qubits on the upper (lower) chip as black and gray (dark and light blue) circles and the resonators as dark gray marks.\nHere, the lower chip is slightly shifted to the right to prevent undesirable interference between the qubits in the dotted circles in Fig.\u00a08\u2009(b) when they come closer, while this shift is not depicted in Figs.\u00a08\u2009(a) and (c) for simplicity.\nAs shown in Figs.\u00a08\u2009(b) and (c), the upper (lower) chip has a higher (lower) qubit density to form the Logic (Bypass) layer.\nControl lines for the qubits on the upper chip (purple lines) are routed from the opposite side of the lower chip via TSVs (orange dotted cylinders).\nThe resonators connecting to the qubits on the top chip are located on the lower chip through inter-chip connections (teal lines), leveraging the low density of the lower chip.\nAs a result, the Bypass layer can be introduced with minimal reduction in the qubit density of the upper chip compared to that of a single-Logic-layer configuration. Figure\u00a08 illustrates one possible implementation of the Bypass architecture.\nHowever, note that our architecture proposal and simulations are not limited to the specific implementation. Stacking three or more qubit chips without compromising qubit density is challenging due to the significantly increased wiring complexity.\nThus, the investigation of architectures with three or more layers is out of the scope of this paper and left as future work. \n5.4. Flexibility with different code distances We need to predefine a certain code distance d\ud835\udc51ditalic_d for each cell at the design phase; however, this design constraint does not significantly limit the flexibility of the code distance for each cell compared to the conventional 2D architecture.\nFigure\u00a09 shows an example where the Bypass architecture designed with d=7\ud835\udc517d=7italic_d = 7 is used with cells having a code distance d\u2032=3superscript\ud835\udc51\u20323d^{\\prime}=3italic_d start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = 3.\nSince the left and right boundaries of each cell are equivalent in terms of LS, as shown in Fig.\u00a09\u2009(b), the cell can use the Bypass layer as long as its left or right boundary connects to the Bypass layer (red lines).\nIn other words, when accommodating cells with a different code distance d\u2032superscript\ud835\udc51\u2032d^{\\prime}italic_d start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT than the design-phase distance d\ud835\udc51ditalic_d, the Bypass architecture can be employed by ensuring that either boundary of each cell aligns with the Bypass layer, albeit with a slight decrease in hardware utilization efficiency. \n5.5. Differences with long-range coupler The two benefits of the Bypass layer described in the previous subsection can also be achieved by connecting distant cells in the Logic layer using long-range couplers, such as assumed in Ref.\u00a0(Bravyi et\u00a0al., 2024).\nOur Bypass layer differs from this approach in the following points. Programmability: The Bypass layer dynamically determines connecting paths by selecting the stabilizers to activate, whereas the connections between cells through long-range couplers are fixed at the design phase. Path length limit: \nThe Bypass layer achieves LS paths of any length through local connections between adjacent qubits.\nFor long-range couplers, as stated in the concluding remarks of Ref.\u00a0(Bravyi et\u00a0al., 2024), connections that exceed a certain length determined by the frequency of qubits significantly increase PER. Combined with the above differences, when considering each approach as a communication component, the long-range coupler acts merely as wiring, whereas the Bypass layer functions as a network switch and relay.\nThe sparsely placed qubits in the Bypass layer programmatically determine the LS path and connect longer paths without significant impact on the operation error rate. \n6. LER evaluation \n6.1. Simulation setup and error model We perform circuit-level stabilizer simulations using Stim\u00a0(Gidney, 2021) and PyMatching\u00a0(Higgott and Gidney, 2023) to estimate the LERs of MEAS_ZZ operations with path length L\ud835\udc3fLitalic_L on the Bypass architecture.\nFor simplicity, we describe 3\u2062d3\ud835\udc513d3 italic_d-code-cycle protection of long SC patches, as shown at the bottom of Fig.\u00a07, using the Stim circuit format.\nWe assume a circuit-level noise model that applies a depolarizing noise channel after all physical gates and adds measurement errors.\nFor a given physical error rate (PER), we repetitively sample the error patterns, simulate the propagation of errors and the decoding procedure, and evaluate the probability of logical failure. The MEAS_ZZ operation using the Bypass involves inter-layer CNOT operations, as indicated by the red lines in Fig.\u00a06\u2009(b).\nAlthough it significantly depends on the implementation, these inter-layer two-qubit gates may exhibit higher PER compared to intra-layer operations.\nTo evaluate the impact of inter-layer gates, we assume p\u2032superscript\ud835\udc5d\u2032p^{\\prime}italic_p start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT as the PER of inter-layer operations, while all other operations have a PER of p\ud835\udc5dpitalic_p.\nWe assume several values for p\u2032superscript\ud835\udc5d\u2032p^{\\prime}italic_p start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT based on the infidelity of inter-chip communication with flip-chip bonding from Refs.\u00a0(Gold et\u00a0al., 2021; Smith et\u00a0al., 2022).\nOptimistically, we assume p\u2032=psuperscript\ud835\udc5d\u2032\ud835\udc5dp^{\\prime}=pitalic_p start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = italic_p.\nFor a practical scenario, we assume p\u2032=5\u2062psuperscript\ud835\udc5d\u20325\ud835\udc5dp^{\\prime}=5pitalic_p start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = 5 italic_p, based on the ratio of inter- and intra-chip average PERs reported in Ref.\u00a0(Gold et\u00a0al., 2021).\nPessimistically, we assume p\u2032=10\u2062psuperscript\ud835\udc5d\u203210\ud835\udc5dp^{\\prime}=10pitalic_p start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = 10 italic_p, which is double the practical value. \n6.2. LER evaluation results Figures\u00a010\u2009(a) and (b) compare the LER of MEAS_ZZ operations with and without a Bypass layer, as shown in Fig.\u00a07.\nIn the case of (a) L=3\ud835\udc3f3L=3italic_L = 3, the optimistic scenario with the Bypass layer (dotted lines) shows almost the same LER as that without the Bypass layer (solid lines).\nBy contrast, the pessimistic scenario (dashed lines) has a degraded performance due to the inter-layer operation compared to the solid lines.\nIn the case of (b) L=10\ud835\udc3f10L=10italic_L = 10, the optimistic scenario with the Bypass layer achieves lower LERs than those without Bypass.\nFor the pessimistic scenario with the Bypass layer, the performance degradation is moderate. Figure\u00a010\u2009(c) shows the path length sensitivity of the LER of MEAS_ZZ operations using a Bypass layer relative to that without a Bypass layer.\nHere, the PER p\ud835\udc5dpitalic_p is fixed to 0.001.\nIn the figure, values on the vertical axis below 1 indicate that using a Bypass layer results in a lower LER compared to that without a Bypass layer.\nThe results show that the use of the Bypass layer is beneficial in terms of LER for longer MEAS_ZZ operations even in the pessimistic scenario. Figure\u00a010\u2009(d) shows the minimum path length L\ud835\udc3fLitalic_L of MEAS_ZZ operation where the use of the Bypass layer becomes advantageous in terms of LER, given a specific PER and code distance.\nAs the code distance decreases or the PER increases, the required L\ud835\udc3fLitalic_L for the Bypass layer to be advantageous also decreases.\nIn the evaluated parameter combinations, even in the pessimistic case, the Bypass layer is advantageous for MEAS_ZZ with L\ud835\udc3fLitalic_L of 31 or more. \n7. FTQC performance evaluation \n7.1. QPE as benchmark program QPE is a quantum algorithm designed to estimate the ground state energy of a given Hamiltonian and is expected to demonstrate practical quantum advantages at an early stage, attracting attention across various research fields\u00a0(Reiher et\u00a0al., 2017; Yoshioka et\u00a0al., 2024; Babbush et\u00a0al., 2018; Kivlichan et\u00a0al., 2020; Lee et\u00a0al., 2021).\nIn particular, Ref.\u00a0(Yoshioka et\u00a0al., 2024) focuses on QPE based on qubitization\u00a0(Low and Chuang, 2019) for applications in condensed matter physics, including detailed resource estimation and the acceleration of its major subroutine, called the SELECT circuit.\nFigure\u00a0S11 of Ref.\u00a0(Yoshioka et\u00a0al., 2024) shows that over 80% of QPE execution time is consumed by the SELECT.\nThe SELECT circuit consists of a structure with multi-controlled gates, which can be decomposed with Toffoli gates and frequently appear in many quantum algorithms.\nIn addition, since quantum singular value transformation, an extension of qubitization, is known to include many other quantum algorithms, optimizing qubitization can lead to the acceleration of a wide range of quantum applications(Martyn et\u00a0al., 2021).\nThus, SELECT is well-suited as a benchmark because of its versatility for various FTQC applications and its significant contribution to execution time. For the benchmark programs, we use SELECT circuits for Fermi-Hubbard (FH) and Jellium models, which are commonly used for condensed matter physics\u00a0(Yoshioka et\u00a0al., 2024; Kivlichan et\u00a0al., 2020).\nFor chemistry applications, we use SELECT circuits for H4subscript\ud835\udc3b4H_{4}italic_H start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT molecule, where the Hydrogen atoms are located on 2\u00d72222\\times 22 \u00d7 2 grid with a distance of 1.45 Angstrom, with the basis cc-pVDZ, similar to the resource estimation in Ref.\u00a0(Lee et\u00a0al., 2021).\nTable\u00a02 summarizes these circuit characterizations.\nWe mainly focus on the FH model with a problem size Ns\u2062i\u2062z\u2062esubscript\ud835\udc41\ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc52N_{size}italic_N start_POSTSUBSCRIPT italic_s italic_i italic_z italic_e end_POSTSUBSCRIPT of 200, which represents the lattice size of the Hamiltonian.\nNote that QPE problems using the FH model with Ns\u2062i\u2062z\u2062e\u226572subscript\ud835\udc41\ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc5272N_{size}\\geq 72italic_N start_POSTSUBSCRIPT italic_s italic_i italic_z italic_e end_POSTSUBSCRIPT \u2265 72 are expected to demonstrate quantum advantages, as shown in Fig.\u00a05 of Ref.\u00a0(Yoshioka et\u00a0al., 2024).\nWe utilize the parallelization technique of the SELECT circuit in Ref.\u00a0(Yoshioka et\u00a0al., 2024) with 16 threads for all simulations.\nUnless otherwise specified, the code distance d\ud835\udc51ditalic_d is assumed to be 25, as in Ref.\u00a0(Yoshioka et\u00a0al., 2024). \n7.2. Simulation setup We evaluate the FTQC performance on the Bypass architecture using a cycle-accurate LS simulator.\nTable\u00a03 summarizes the parameters used in the simulation.\nThe underlined values in the table are used for the simulation in Fig.\u00a015.\nThe bold values are the initial values for the sensitivity experiment in Fig.\u00a016 and also used for the evaluation in Figs.\u00a01 and 14. Our simulation uses greedy instruction scheduling, where previously executed instructions occupy the required cells, and any executable instructions are always executed within a code beat.\nEach LS operation chooses the shortest available path.\nOur greedy instruction scheduling avoids the situation where multiple operations attempt to occupy the same cells simultaneously. Our simulation compares the performance and hardware resources of four different qubit layouts: 1L, 2L-DD, 2L-DP, and Bypass, as shown in Fig.\u00a011\u2009(a).\nThe 1L-D layout consists of a single Logic layer with data cells.\nThe 2L-DD and 2L-DP layouts consist of two Logic layers; 2L-DD allocates data cells in both layers with the same Rd\u2062a\u2062t\u2062asubscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4eR_{data}italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT, while 2L-DP allocates data cells in one layer with Rd\u2062a\u2062t\u2062asubscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4eR_{data}italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT and uses the other as a pathway for LS operations.\nThe Bypass layout consists of one logic layer and one Bypass layer, enabling horizontal LS paths to be efficiently connected through it.\nFor 2L-DD, 2L-DP, and Bypass layouts, the approximate number of total physical qubits relative to that of the 1L-D layout with the same Rd\u2062a\u2062t\u2062asubscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4eR_{data}italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT, is shown at the bottom of Fig.\u00a011\u2009(a).\nAlthough we simulate the 2L-DD and 2L-DP layouts, note that their implementation feasibility is not addressed in Sec.\u00a05.3. Our Bypass architecture enables the execution of LS instructions with effectively shorter paths because of two key factors: 1) the availability of shorter path options via 3D-stacked layers, and 2) the sparsity of the Bypass layer, as explained in Sec.\u00a05.2.1 and 5.2.2, respectively.\nBy comparing 1L-D and 2L-DP (2L-DP and Bypass) layouts, the impact of Factor 1 (Factor 2) on L\u2032superscript\ud835\udc3f\u2032L^{\\prime}italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT is estimated.\nIn addition, comparing 1L-D and 2L-DD estimates the impact of increasing the dimensionality of the qubit layout while maintaining the data cell density on L\u2032superscript\ud835\udc3f\u2032L^{\\prime}italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT.\nThus, the difference between the results of 1L-D and Bypass shows a comparison of the impact of the dimensional increase and the two Factors on L\u2032superscript\ud835\udc3f\u2032L^{\\prime}italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT. In our simulation, as shown in the floor plan in Fig.\u00a011\u2009(b), the qubit plane is divided into three parts: Factories, Pools, and Cells.\nThe Factories part contains nFsubscript\ud835\udc5b\ud835\udc39n_{F}italic_n start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT MSD circuits, each of which has a corresponding magic-state pool in the Pools part.\nData cells are allocated in the Cells part with variations in the data cell ratio Rd\u2062a\u2062t\u2062asubscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4eR_{data}italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT from 25%percent2525\\%25 % to 50%percent5050\\%50 %, as shown on the right side of Fig.\u00a011\u2009(b).\nIn these data cell arrangements, the instructions described in Sec.\u00a02.4 can always be executed on any data cell as long as hazards do not occur.\nWe call such arrangements immediate operation (IO) capable, and Rd\u2062a\u2062t\u2062asubscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4eR_{data}italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT for IO-capable arrangements is at most 50%, as proven in App.\u00a0A.\nThus, the arrangement with Rd\u2062a\u2062t\u2062a=50%subscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4epercent50R_{data}=50\\%italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT = 50 %, which we newly found in this paper, represents one of the densest IO-capable arrangements. The positions of data cells remain fixed during computation, and each logical qubit is randomly assigned to a data cell at the start of the program.\nTo evaluate the impact of logical qubit assignment on performance, each simulation is repeated 1000 times with different random assignments.\nThe average result is used for evaluation, and the standard deviation is depicted as an error bar on a CBPI stack. Typically, data cells are arranged by repeating the pattern shown on the right side of Fig.\u00a011\u2009(b) an equal number of times vertically and horizontally to form a square-shaped Data cells area, which we refer to as the square arrangement.\nFor the Bypass layout, we consider a horizontally elongated arrangement of the Data cell area to leverage its characteristics, referred to as the wide arrangement, although this slightly increases the total number of cells compared to the square arrangement.\nIn wide arrangements, the height of Data cell area is determined such that the average effective path length between any two data cells is minimized for a given number of data cells, as explained in Sec.\u00a07.3.\nHowever, if this height is smaller than nFsubscript\ud835\udc5b\ud835\udc39n_{F}italic_n start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT, it is set to nFsubscript\ud835\udc5b\ud835\udc39n_{F}italic_n start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT. Based on the two-level 15-to-1 distillation protocol proposed in Ref.\u00a0(Litinski, 2019a), we assume each MSD circuit generates a single magic state per 15 code beats.\nMagic states generated by each MSD circuit are stocked in the corresponding magic pool and consumed by non-Clifford operations via gate teleportation.\nNote that the generation rate per MSD circuit may be improved by using more efficient strategies, such as the constructions proposed in Refs.\u00a0(Litinski, 2019b; Hirano et\u00a0al., 2024; Gidney et\u00a0al., 2024). During the LS simulation, the decoding tasks associated with each operation are considered as follows.\nAs shown in Fig.\u00a05\u2009(d), whenever an instruction is executed, a decoding task associated with it is added to the decoding task queue.\nNote that the decoding process is also required to protect data cells (NOP operations).\nWe assume that the difficulty of a decoding task for an operation is proportional to both the number of data qubits involved in the operation and its duration.\nThe decoder processes tasks sequentially from the top of the queue, provided that sufficient decoding capacity remains per code beat.\nWe assume that the decoding capacity is proportional to the total number of qubits in the system.\nThe parameter T\u2062Pd\u2062e\u2062c\ud835\udc47subscript\ud835\udc43\ud835\udc51\ud835\udc52\ud835\udc50TP_{dec}italic_T italic_P start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT represents the decoding capacity per code beat per cell required for adequate online decoding.\nSpecifically, T\u2062Pd\u2062e\u2062c=0.5\u2062(1.0)\ud835\udc47subscript\ud835\udc43\ud835\udc51\ud835\udc52\ud835\udc500.51.0TP_{dec}=0.5(1.0)italic_T italic_P start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT = 0.5 ( 1.0 ) represents an FTQC system with sufficient decoding resources to execute online decoding for half (all) of the cells.\nNote that the decoding task difficulty for a single cell in the Bypass layer is 1/d1\ud835\udc511/d1 / italic_d of that for a single cell in the Logic layer.\nFor simplicity, decoding tasks required for Factories are not considered. \n7.3. LS path length and program fidelity First, Fig.\u00a012\u2009(a) shows the average effective distance L\u2032superscript\ud835\udc3f\u2032L^{\\prime}italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT between any two data cells for each qubit layout and data cell arrangement across various numbers of data cells.\nThe 1L-D and 2L-DD layouts exhibit similar trends: for Rd\u2062a\u2062t\u2062asubscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4eR_{data}italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT ranging from 25% to 44%, the average L\u2032superscript\ud835\udc3f\u2032L^{\\prime}italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT decreases as the total number of cells decreases with higher Rd\u2062a\u2062t\u2062asubscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4eR_{data}italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT. However, at Rd\u2062a\u2062t\u2062a=50%subscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4epercent50R_{data}=50\\%italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT = 50 %, roundabout paths significantly increase the average L\u2032superscript\ud835\udc3f\u2032L^{\\prime}italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT.\nIn contrast, the 2L-DP layout utilizes its pathway layer to avoid roundabout paths, leading to a reduction in L\u2032superscript\ud835\udc3f\u2032L^{\\prime}italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT as Rd\u2062a\u2062t\u2062asubscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4eR_{data}italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT increases.\nFor sparse data cell arrangements with Rd\u2062a\u2062t\u2062a=25%subscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4epercent25R_{data}=25\\%italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT = 25 %, where no roundabout paths exist even in the 1L-D layout, the average L\u2032superscript\ud835\udc3f\u2032L^{\\prime}italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT is nearly identical to that of the 2L-DP layout (see the red and green crosses in the zoomed-in region on the right). The Bypass layout, compared to other layouts, effectively reduces the average L\u2032superscript\ud835\udc3f\u2032L^{\\prime}italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT, particularly in high-density data cell arrangements.\nMoreover, the advantage of the Bypass layout over the other three layouts scales with the number of data cells.\nThis trend is particularly pronounced in the wide arrangement with Rd\u2062a\u2062t\u2062a=50%subscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4epercent50R_{data}=50\\%italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT = 50 % (blue line with stars).\nFigure\u00a012\u2009(b) shows the relationship between the height of the wide arrangement and the average L\u2032superscript\ud835\udc3f\u2032L^{\\prime}italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT for the Bypass layout with Rd\u2062a\u2062t\u2062a=50%subscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4epercent50R_{data}=50\\%italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT = 50 %.\nFor each data cell number, we choose the height that minimizes average L\u2032superscript\ud835\udc3f\u2032L^{\\prime}italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT to generate the plot of Fig.\u00a012\u2009(a). Next, Fig.\u00a013 presents histograms of L\u2032superscript\ud835\udc3f\u2032L^{\\prime}italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT for QPE with the FH model (Ns\u2062i\u2062z\u2062e=200subscript\ud835\udc41\ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc52200N_{size}=200italic_N start_POSTSUBSCRIPT italic_s italic_i italic_z italic_e end_POSTSUBSCRIPT = 200) across the four qubit layouts.\nWe choose Rd\u2062a\u2062t\u2062asubscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4eR_{data}italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT for each layout that minimizes average L\u2032superscript\ud835\udc3f\u2032L^{\\prime}italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT in Fig.\u00a012\u2009(a).\nOther simulation parameters are set to the bolded values in Tab.\u00a03.\nAs indicated in the legend of Fig.\u00a013, the total L\u2032superscript\ud835\udc3f\u2032L^{\\prime}italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT in the Bypass layout is approximately 30% (half) of that in the 1L-D (2L-DD) layout. The code distance d\ud835\udc51ditalic_d is determined based on the PER p\ud835\udc5dpitalic_p, the SC threshold pt\u2062hsubscript\ud835\udc5d\ud835\udc61\u210ep_{th}italic_p start_POSTSUBSCRIPT italic_t italic_h end_POSTSUBSCRIPT, and the LER pLsubscript\ud835\udc5d\ud835\udc3fp_{L}italic_p start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT required for the FTQC program, as described by the following formula: pL\u2248const\u00d7(p/pt\u2062h)(d\u22121)/2subscript\ud835\udc5d\ud835\udc3fconstsuperscript\ud835\udc5dsubscript\ud835\udc5d\ud835\udc61\u210e\ud835\udc5112p_{L}\\approx\\text{const}\\times(p/p_{th})^{(d-1)/2}italic_p start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT \u2248 const \u00d7 ( italic_p / italic_p start_POSTSUBSCRIPT italic_t italic_h end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT ( italic_d - 1 ) / 2 end_POSTSUPERSCRIPT.\nAs explained in Sec.\u00a05.2.2, the LER of the entire FTQC program is proportional to the total path length of all LS operations.\nThus, if the total L\u2032superscript\ud835\udc3f\u2032L^{\\prime}italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT of the FTQC program is reduced by a factor of 1/X1\ud835\udc4b1/X1 / italic_X, the pLsubscript\ud835\udc5d\ud835\udc3fp_{L}italic_p start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT of the overall program also decreases by a factor of 1/X1\ud835\udc4b1/X1 / italic_X, allowing the code distance d\ud835\udc51ditalic_d to be reduced according to the value of p/pt\u2062h\ud835\udc5dsubscript\ud835\udc5d\ud835\udc61\u210ep/p_{th}italic_p / italic_p start_POSTSUBSCRIPT italic_t italic_h end_POSTSUBSCRIPT.\nAs a result, assuming an SC threshold pt\u2062hsubscript\ud835\udc5d\ud835\udc61\u210ep_{th}italic_p start_POSTSUBSCRIPT italic_t italic_h end_POSTSUBSCRIPT of 0.01 and a PER p\ud835\udc5dpitalic_p of approximately 0.003, the Bypass architecture enables a reduction in the code distance d\ud835\udc51ditalic_d by 2 compared to the 1L-D, because of the total L\u2032superscript\ud835\udc3f\u2032L^{\\prime}italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT results in Fig.\u00a013.\nThe impact of reducing d\ud835\udc51ditalic_d on the FTQC performance and scalability is discussed in Sec.\u00a07.5. \n7.4. CBPI evaluation results Figure\u00a014 presents performance evaluation results of various QPE applications using the bolded parameters in Tab.\u00a03, except for Rd\u2062a\u2062t\u2062asubscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4eR_{data}italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT.\nThe top and bottom plots show the results for Rd\u2062a\u2062t\u2062a=44%subscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4epercent44R_{data}=44\\%italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT = 44 % and 50%percent5050\\%50 %, respectively.\nFor Rd\u2062a\u2062t\u2062a=44%subscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4epercent44R_{data}=44\\%italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT = 44 %, the path and decoding penalties slightly degrade the performance of the 1L-D layout but are completely mitigated by the other three layouts across all applications.\nWhen Rd\u2062a\u2062t\u2062a=50%subscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4epercent50R_{data}=50\\%italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT = 50 %, the impact of these penalties increases in the 1L-D layout.\nHowever, the 2L-DP layout and the Bypass with wide arrangement successfully eliminate them. Since Fig.\u00a014 shows a similar trend across all applications, we focus on the FH model for a more detailed evaluation.\nFigure\u00a015 presents the performance evaluation results for various parameter combinations of the underlined values in Tab.\u00a03.\nThe results for parameters not underlined are omitted, as their selection had little to no impact on performance.\nFor example, increasing T\u2062Pd\u2062e\u2062c\ud835\udc47subscript\ud835\udc43\ud835\udc51\ud835\udc52\ud835\udc50TP_{dec}italic_T italic_P start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT greater than 0.5 only eliminates the small impact of decoding hazards on CBPI in the cases with Rd\u2062a\u2062t\u2062a=50%subscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4epercent50R_{data}=50\\%italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT = 50 % in the figure.\nSimilarly, increasing nFsubscript\ud835\udc5b\ud835\udc39n_{F}italic_n start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT from 12 to 16 only reduces the minor impact of magic hazards in any case.\nFor the cases with Rd\u2062a\u2062t\u2062a=25%subscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4epercent25R_{data}=25\\%italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT = 25 %, decoding and path hazards have no impact on the CBPI. For the cases of T\u2062Pd\u2062e\u2062c=0.4\ud835\udc47subscript\ud835\udc43\ud835\udc51\ud835\udc52\ud835\udc500.4TP_{dec}=0.4italic_T italic_P start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT = 0.4 (top row of the figure), increasing Rd\u2062a\u2062t\u2062asubscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4eR_{data}italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT from 44% to 50% significantly reduces performance due to the decoding penalty, except in the 2L-DP layout and the Bypass with wide arrangement.\nNote that the decoding resource is assumed to be proportional to the number of qubits in the layouts.\nThe 2L-DP layout has nearly twice the decoding resources of the other three layouts for the same T\u2062Pd\u2062e\u2062c\ud835\udc47subscript\ud835\udc43\ud835\udc51\ud835\udc52\ud835\udc50TP_{dec}italic_T italic_P start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT and Rd\u2062a\u2062t\u2062asubscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4eR_{data}italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT.\nAs the decoding resources increase (middle and bottom rows), the impact of the decoding penalty is significantly reduced in all cases.\nAt T\u2062Pd\u2062e\u2062c=0.45\ud835\udc47subscript\ud835\udc43\ud835\udc51\ud835\udc52\ud835\udc500.45TP_{dec}=0.45italic_T italic_P start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT = 0.45 (middle row), sufficient decoding throughput is achieved to mitigate the decoding penalty for the Bypass layout with Rd\u2062a\u2062t\u2062a=50%subscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4epercent50R_{data}=50\\%italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT = 50 % wide arrangement.\nBy contrast, the penalty persists in the 1L-D and 2L-DD layouts with the dense data cell arrangements. The path penalty affects the performance only on the 1L-D layout with Rd\u2062a\u2062t\u2062a=50%subscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4epercent50R_{data}=50\\%italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT = 50 % for the QPE programs.\nThe other three layouts completely mitigate the hazards because of their 3D structures. In cases with a limited number of MSDs, such as nF=4subscript\ud835\udc5b\ud835\udc394n_{F}=4italic_n start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT = 4, the performance improvement with the Bypass layout is small, except when T\u2062Pd\u2062e\u2062c=0.4\ud835\udc47subscript\ud835\udc43\ud835\udc51\ud835\udc52\ud835\udc500.4TP_{dec}=0.4italic_T italic_P start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT = 0.4.\nThis occurs because the slow supply of magic states limits the number of LS instructions that can be executed simultaneously, reducing the chance of path and decoding hazards.\nAs nFsubscript\ud835\udc5b\ud835\udc39n_{F}italic_n start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT increases, these hazards degrade performance in the 1L-D and 2L-DD layouts, while the 2L-DP and Bypass mitigate them. In all cases, the error bars, which represent the impact of logical qubit assignment on performance, in the Bypass layout with a wide arrangement are smaller than those for the other layouts, suggesting that the Bypass layout may reduce the burden of placement optimization in the LS compilation process, thereby simplifying the overall workflow and improving the efficiency of LS-based FTQC. Next, Fig.\u00a016 summarizes the CBPI sensitivity to the number of factories, decoder resources, data cell arrangements, and problem size of QPE using various combinations of the values in Tab.\u00a03.\nOne parameter is varied while the others are fixed to the bolded values in the table, represented as dashed lines in the figure.\nIn each plot, the changes from left to right on the graph represent design modifications that increase the required hardware resources.\nNote that the Bypass layout with Rd\u2062a\u2062t\u2062a=50%subscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4epercent50R_{data}=50\\%italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT = 50 % uses wide arrangements, while the others use square arrangements. The Factory column shows that increasing nFsubscript\ud835\udc5b\ud835\udc39n_{F}italic_n start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT may not improve FTQC performance due to the impact of path and decoding penalties in the 1L-D and 2L-DD layouts.\nFor nF=16subscript\ud835\udc5b\ud835\udc3916n_{F}=16italic_n start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT = 16 in the Bypass layout, the decoding penalty slightly increases because the height of the wide arrangement is set to nFsubscript\ud835\udc5b\ud835\udc39n_{F}italic_n start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT, reducing the number of horizontally aligned cells in the Cells part and limiting the opportunity to leverage the characteristics of the Bypass layer.\nThe Decoder and Density columns suggest that increasing classical and quantum resources can enhance FTQC performance when the magic state generation rate is sufficient for applications.\nMeanwhile, the Bypass layout achieves high performance even with limited hardware resources.\nIn addition, the Problem size column indicates that this trend remains consistent regardless of the problem size. \n7.5. Tradeoff between performance and resource This subsection presents the tradeoff between classical and quantum hardware resources and FTQC performance.\nEach graph in Fig.\u00a017 illustrates the various layouts, with the number of physical qubits in the Cells and Pools on the horizontal axis and CBPI on the vertical axis.\nNote that the simulation assumes decoding resources to be proportional to the number of physical qubits.\nThus, the horizontal axis of each graph represents both the quantum hardware resources and the classical decoding resources. As representative configurations, three cases are selected: limited resources (nF=8,T\u2062Pd\u2062e\u2062c=0.4formulae-sequencesubscript\ud835\udc5b\ud835\udc398\ud835\udc47subscript\ud835\udc43\ud835\udc51\ud835\udc52\ud835\udc500.4n_{F}=8,\\ TP_{dec}=0.4italic_n start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT = 8 , italic_T italic_P start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT = 0.4), moderate resources (12,0.45120.4512,0.4512 , 0.45), and rich resources (16,1.0161.016,1.016 , 1.0), as shown in Fig.\u00a017.\nParticularly, we focus on comparing the 1L-D layout with Rd\u2062a\u2062t\u2062a=44%subscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4epercent44R_{data}=44\\%italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT = 44 % data cell arrangement and the Bypass layout with Rd\u2062a\u2062t\u2062a=50%subscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4epercent50R_{data}=50\\%italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT = 50 % wide arrangement.\nThese are referred to as the base and proposed layouts, represented by the red triangles and blue stars in the figure, respectively. In the limited resources case, the proposed layout achieves a 2.16\u00d7\\times\u00d7 speedup with only 1.7% additional hardware resources compared to the base layout.\nIn the moderate (rich) resources case, the proposed layout breaks the tradeoff between resources and performance, achieving both a 1.60\u00d7\\times\u00d7 (1.23\u00d7\\times\u00d7) speedup and a 2.6% (7.2%) reduction in hardware resources compared to the base.\nIn addition, the proposed layout achieves either higher performance, reduced hardware resources, or both, compared to any other layouts with 3D-stacked layers (orange and green markers) in all cases. As discussed in Sec.\u00a07.3, the Bypass architecture contributes to reducing the code distance by shortening L\u2032superscript\ud835\udc3f\u2032L^{\\prime}italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT for LS operations.\nThe hardware resources and performance of the proposed layout, with the code distance reduced to 23, are represented as the purple stars in Fig.\u00a017.\nFor the limited, moderate, and rich cases, the proposed layout with d=23\ud835\udc5123d=23italic_d = 23 achieves 2.35\u00d7\\times\u00d7, 1.73\u00d7\\times\u00d7, and 1.33\u00d7\\times\u00d7 speedup and 13%, 17%, and 21% reduction in hardware resources compared to the base, respectively, demonstrating the further potential of the Bypass architecture for high-performance and scalable FTQC. \n8. Related work FTQC resource estimation: The depth of non-Clifford gates, such as T\ud835\udc47Titalic_T-gates, has often been used as a metric for FTQC resource estimation because each application of T\ud835\udc47Titalic_T-gates requires a time-consuming procedure consisting of magic-state injection and distillation in typical FTQC schemes, including LS\u00a0(Gidney and Eker\u00e5, 2021; Tannu et\u00a0al., 2017b; Ding et\u00a0al., 2018).\nWhile the T\ud835\udc47Titalic_T-count can capture the time-scaling of given programs when magic-state preparations are the most time-consuming factors in large-scale FTQC, this estimation omits several vital factors in time analysis.\nAs a result, the actual execution time can vary by a few orders of magnitude by the FTQC architecture and compilation. LS Compilation schemes: To achieve high-throughput LS operations, optimal compilation, including data cell arrangement and LS instruction scheduling, is important; however, finding optimal LS compilation is known to be NP-hard\u00a0(Herr et\u00a0al., 2017; Molavi et\u00a0al., 2023).\nThus, fast and near-optimal approximation strategies for scheduling are well-studied.\nOne possible strategy is to map the compilation problems into well-known NP-hard instances; Lao et al.\u00a0(Lao et\u00a0al., 2018) map the problems into the quadratic assignment problem, and Molavi et al.\u00a0(Molavi et\u00a0al., 2023) into SAT problems.\nAs another approach, Hamada et al.\u00a0(Hamada et\u00a0al., 2024) proposed a new scheduling method leveraging a technique to split large LS instructions into several smaller ones, such as Bell state preparation and measurements, and executing a part of them in advance. Novel qubit layouts for LS-based FTQC: Regarding architectural approaches for high-performance LS-based computation, Viszlai\u00a0et al.\u00a0(Viszlai et\u00a0al., 2023) proposed a 3D LS architecture with multiple effective qubit layers leveraging the degrees of freedom in manipulating neutral atoms.\nTheir architecture achieves a throughput improvement by reducing path conflicts during LS operations and implementing transversal CNOT gates.\nDuckering et al.\u00a0(Duckering et\u00a0al., 2020) proposed a novel FTQC architecture named virtualized logical qubits, which supports transversal logical-CNOT gates and achieves hardware minimization through the combination of transmon and cavity qubits.\nNote that these approaches do not con\ufb02ict with our proposal, and they are expected to be even more ef\ufb01cient when combined. \n9. Conclusion In this paper, we proposed a performance evaluation methodology named CBPI stack that breaks down the impact of each hazard on LS-based FTQC.\nBased on the bottleneck analysis with the CBPI stack, we proposed the Bypass architecture to achieve high-performance and scalable LS-based FTQC by suppressing LS path length.\nIn our simulation, the Bypass architecture achieves both a 1.73\u00d7\\times\u00d7 speedup and a 17% reduction in classical and quantum hardware resources compared to the 2D baseline in the moderate resources case, demonstrating its potential for high-performance and scalable FTQC. Acknowledgement This work was partly supported by MEXT Q-LEAP Grant Numbers JPMXS0120319794 and JPMXS0118068682, JST Moonshot R&D, Grant Numbers JPMJMS2061, JPMJMS2067-20 and JPMJMS226C-107, JST CREST Grant Numbers JPMJCR23I4 and JPMJCR24I4, JSPS KAKENHI Grant Numbers 22H05000, 22K17868, and 24K02915, RIKEN Special Postdoctoral Researcher Program. \nAppendix A Dense data cell arrangement This section introduces a novel data cell arrangement to achieve an LS-based FTQC architecture with minimal hardware resources.\nFor simplicity, this paper focuses on the data cell arrangement where the instructions in the instruction set described in Sec.\u00a02.4 can always be executed on any data cell, as long as hazards do not occur.\nWe call such arrangements as Immediate operation (IO) capable.\nThe necessary conditions for IO-capable data cell arrangements are as follows: Each data cell has at least one ancillary cell to its left and right, and one above and below. Any two ancillary cells have a path connecting them through ancillary cells. The arrangements in Fig.\u00a018\u2009(a), including the novel one that asymptotically achieves an Rd\u2062a\u2062t\u2062asubscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4eR_{data}italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT of 50%percent5050\\%50 %, satisfy the conditions above.\nBy contrast, the arrangement in Fig.\u00a018\u2009(b)\u00a0(Lee et\u00a0al., 2021) violates condition\u00a0(I).\nAs a result, it may be necessary to execute data cell rotation or movement protocols, which consume several code beats\u00a0(Litinski, 2019a), before performing certain instructions, leading to performance degradation. Here, the Rd\u2062a\u2062t\u2062asubscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4eR_{data}italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT for any IO-capable data cell arrangement is at most 50%, as proven in Thm.\u00a0A.1.\nThus, the new arrangement is one of the densest arrangements. Rd\u2062a\u2062t\u2062asubscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4eR_{data}italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT of any IO-capable arrangements is at most 50%percent5050\\%50 %. Let Ndsubscript\ud835\udc41\ud835\udc51N_{d}italic_N start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT and Nasubscript\ud835\udc41\ud835\udc4eN_{a}italic_N start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT be the numbers of data and ancillary cells in a certain region of the qubit plane.\nLet Rd\u2062a\u2062t\u2062a=NdNa+Ndsubscript\ud835\udc45\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4esubscript\ud835\udc41\ud835\udc51subscript\ud835\udc41\ud835\udc4esubscript\ud835\udc41\ud835\udc51R_{data}=\\frac{N_{d}}{N_{a}+N_{d}}italic_R start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT = divide start_ARG italic_N start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT + italic_N start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_ARG be the data cell ratio of the region.\nLet Disubscript\ud835\udc37\ud835\udc56D_{i}italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, Aisubscript\ud835\udc34\ud835\udc56A_{i}italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, and Eisubscript\ud835\udc38\ud835\udc56E_{i}italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT represent the numbers of data cells, ancillary cells, and cells outside the region in the four adjacent cells of the i\ud835\udc56iitalic_i-th ancillary cell in the region, respectively. From Condition\u00a0(I), we have: From Condition\u00a0(II), we have: Combining Eqs.\u00a0(1) and (2), we get: References"}
{"text": "UVLLM: An Automated Universal RTL Verification Framework using LLMs\n\n Verifying hardware designs in embedded systems is crucial but often labor-intensive and time-consuming. While existing solutions have improved automation, they frequently rely on unrealistic assumptions. To address these challenges, we introduce a novel framework, UVLLM\u2009 which combines Large Language Models (LLMs) with the Universal Verification Methodology (UVM) to relax these assumptions. UVLLM\u00a0significantly enhances the automation of testing and repairing error-prone Register Transfer Level (RTL) codes, a critical aspect of verification development. Unlike existing methods, UVLLM\u00a0ensures that all errors are triggered during verification, achieving a syntax error fix rate of 86.99% and a functional error fix rate of 71.92% on our proposed benchmark. These results demonstrate a substantial improvement in verification efficiency. Additionally, our study highlights the current limitations of LLM applications, particularly their reliance on extensive training data. We emphasize the transformative potential of LLMs in hardware design verification and suggest promising directions for future research in AI-driven hardware design methodologies.\nThe Repo. of dataset and code:\nhttps://anonymous.4open.science/r/UVLLM/. \nI Introduction\n Hardware design verification in embedded systems remains heavily dependent on human expertise, making it a tedious and error-prone process that often incurs significant costs\u00a0[1], as Fig.\u00a0LABEL:fig:wf_fe illustrates.\nA critical aspect of this process is debugging and repairing errors, an area where automated program repair (APR) can contribute.\nOriginally developed for software\u00a0[2, 3, 4, 5, 6], APR uses automated tools to fix errors with minimum human intervention and is now being adapted for Hardware Description Language (HDL) design verification due to its potential to reduce human errors and verification costs. APR systems, as depicted in Fig.\u00a0LABEL:fig:wf_apr, receive design codes and test cases, and attempt to enact targeted modifications with predefined templates to ensure all tests are passed.\nInnovations such as Cirfix\u00a0[7], Strider\u00a0[8], and RTLrepair\u00a0[9] demonstrate the potential of APR to reduce the labor and time required for hardware design verification.\nHowever, these APR methodologies predominantly rely on fixed templates and focus on addressing functional error, limiting their scope and effectiveness of the repairs. Fortunately, recent advancements in LLMs such as generating hardware code from natural language specifications\u00a0[10, 11, 12, 13], and debugging hardware designs for both functional and syntax errors\u00a0[14, 15, 16, 17, 18], have demonstrated promising results in bridging this gap.\nHowever, existing solutions are still insufficient for hardware design verification due to their reliance on unreliable assumptions and lack of consideration for the limitations inherent in LLMs.\nFor instance, despite demonstrations of high fix rates for both syntax and functional errors\u00a0[17], our analysis indicates that approximately 10% of the benchmarks manage to bypass the testbench without undergoing any repairs, and the reliability of some repairs remains questionable owing to insufficient coverage of test cases. These findings underscore the need for more robust solutions capable of effective deployment in real-world verification scenarios. To address these shortcomings, we propose a comprehensive end-to-end hardware design verification framework, Universal Verification via Large Language Model (UVLLM). This framework integrates the established UVM with LLMs. Our approach enables the first automated hardware design verification framework that operates with practically assumptions, while also managing uncertainties associated with LLM behaviors. This systematic verification strategy, which encompasses testing and repairing, ensures a more robust solution than those currently highlighted in LLM-aided debugging research.\n The main contributions of this paper are: A comprehensive testing: Utilizing the UVM, our approach enables flexible test modes and efficient coverage collection. Additionally, the reference models generated by LLMs provide a robust foundation for testing across diverse input scenarios. An open-sourced tooling: We have created an open-source toolset to enable the broad and early adoption of UVLLM, thereby easing its integration. These tools are publicly available at https://anonymous.4open.science/r/UVLLM/. Extensive empirical validation: We present an open-source error dateset derived from verified projects, containing 331 code instances with realistic errors across various modules, generated by our paradigm error generator. We will continue to organize and update this dataset periodically. Demonstrated performance improvement: UVLLM, incorporating GPT-4-turbo, significantly enhances verification automation, achieving a syntax error fix rate of 86.99% and a functional error fix rate of 71.92%, exceeding MEIC\u00a0[17] in terms of repair rates and execution time under realistic testing scenarios.\nIt delivers up to 48x speedup in debugging processes when compared with experienced engineers. Organisation. The structure of this paper is as follows: Section\u00a0II outlines the fundamental concepts of UVLLM, while Section\u00a0III delves into the details of UVLLM\u00a0and their underlying reasons.\nSection\u00a0IV assesses our framework and compares it with existing methods.\nSection\u00a0V provides conclusions and make discussions. \nII UVLLM: An Overview\n Designed to enhance the hardware design verification phase, UVLLM\u00a0aids hardware developers in detecting and correcting common errors in RTL code. The UVLLM\u00a0is applicable to various hardware environments, in this work, we illustrate the usage of UVLLM\u00a0in Verilog. As depicted in Fig.\u00a02, this framework combines traditional UVM with LLMs and assumes following inputs: Design specifications that outline the intended and expected behavior of the hardware component; RTL codes that contain the untested RTL code of the initial hardware design, i.e., Design Under Test (DUT). Reflecting advancements in state-of-the-art (SOTA) research, it is noted that using LLMs for hardware design verification requires an iterative approach\u00a0[17] and is more effective when LLMs are provided with detailed error information\u00a0[19, 20, 21].\nHowever, using LLMs still presents certain shortcomings, as current applications of LLMs struggle with reliability, applicability, and processing long code sequences.\nAdditionally, the use of SOTA LLMs can be costly(e.g., the GPT-4-Turbo model, charges $0.01 per 1\u2062K1\ud835\udc3e1K1 italic_K input tokens and $0.03 per 1\u2062K1\ud835\udc3e1K1 italic_K output tokens\u00a0[22]).\nTo address these challenges, UVLLM\u00a0introduces a cost-efficient, structured four-step process for verifying RTL code against design specifications, as illustrated in Fig.\u00a02. 1) \u2003Pre-processing:  Starting with raw RTL code as the input, this stage utilizes linters such as Verilator to pre-process the code, removing syntax errors and addressing timing-related functional errors using a combined LLM-script method.\nThe output is syntax-correct DUT, setting a solid foundation for further functionality testing. 2) \u2003UVM Processing:  Pre-processed DUT code is then tested against a pre-built UVM testbench to identify behavioral discrepancies. Outputs include detailed logs that either confirm alignment with the reference model or highlight deviations with specific signal values and test pass rates, facilitating targeted repairs. 3) \u2003Post-processing:  Utilizing the UVM logs as input, this stage analyzes the logs to extract critical error data using a localization engine and the Abstract Syntax Tree (AST). The output isolates mismatch signals and specific error paths, preparing them for precise correction in the repair stage. 4) \u2003Repair:  The final stage takes the design description, the DUT code and the detailed error information from the post-processing as input. Utilizing the information, the debug agents offer candidate patches to correct the errors.\nThe repaired DUT code is then synthesized as the stage output for further iteration. The termination conditions for the framework loop are: 1) no errors are detected (Success), or 2) the maximum number of iterations is reached (Failure). If any of the above conditions are met, the iteration stops. All history files are stored for reference. Modularization. UVLLM\u00a0uses modular design which allows it to adapt to a wide range of verification scenarios by enabling the use of different tools based on the needs. For example, one can replace an LLM with a more advanced model or use a different linter if required. This flexibility is made possible by standard interfaces between the pipelines, which simplify the integration of different tools via adjustments to the API or by keeping a consistent format. \nIII UVLLM: The Framework Pipeline\n As for the operation of the framework, we introduce the joint LLM-Script pre-processing stage (Section III-A), processing stage with tests (Section III-B), post-processing stage for error location (Section III-C), followed by the discussion on the microsystems integrated with the LLM agents (Section III-D). At last, we present our effort for human-like error generation (Section III-E) for evaluation. \nIII-A Pre-processing via the Linter\n To ensure compliance with best practices and avoid obvious functional defects, the code is pre-processed using Verilator for linting before UVM testbench evaluation.\nThrough static code analysis and slicing, the method pinpoints potential error sources without executing the code.\nLLMs have proven effective in Verilog debugging, especially in repairing syntax errors and refining code with offered error sources.\nBy resolving syntax errors and identifying semantic issues early on, the subsequent need for employing LLMs for debugging is minimized, reducing the costs for the use of LLMs.\n Combined LLM-Script Pre-pocessing. To further reduce costs, as detailed in Algorithm 1, we employ a cost-effective strategy that combines LLMs with supplementary scripting to minimize unnecessary LLM usage. In the pre-processing stage, LLMs are only utilized to help identify and correct syntax errors. These models draw on their extensive training across diverse codebases, efficiently recognizing and amending a broad spectrum of common coding errors and integrating essential error information. Additionally, the pipeline incorporates scripts designed to address specific warnings that, while not syntax errors, could lead to potential runtime issues, especially some timing-related ones. For instance, in combinational logic circuits, blocking assignments are typical, and Verilator issues warnings for using non-blocking assignments in this case. Through predefined templates, it is possible to systematically identify and modify issues where a non-blocking assignment \u201c<=<=< =\u201d should be replaced with a blocking assignment \u201c===\u201d, ensuring the code adheres to expected timing behaviors. The pre-processing stage iterates until all syntax errors and focused timing-related warnings are resolved. Integrating LLM agents with scripts establishes a robust foundation for subsequent verification stages, ensuring that the DUT encounters only functional errors. \nIII-B UVM Processing\n Recent studies, such as MEIC\u00a0[17], validate LLMs\u2019 effectiveness in hardware debugging.\nHowever, these studies neglect the importance of testbench construction and employ finite test cases, which restricts the test coverage and leads to overfitting on specific cases. This limited scope significantly undermines the general applicability of the results, as evidenced by a 10% reduction in the actual fix rate reported by MEIC due to many error instances escaping detection.\nTo overcome these limitations, the UVM framework, depicted in Fig.\u00a03, is employed as the testbench for UVLLM\u00a0to verify RTL codes, due to its robust support for flexible and complete testing modes. UVM Construct.\nTo verify complex hardware system, the UVM offers a formal verification structure significantly advanced over simpler testbenches, incorporating components like Agents, Environments, Sequencers, Drivers, and Monitors. Each agent encapsulates a sequencer, driver, and monitor, enabling direct interaction with the DUT. The Sequencer organizes transactions generated from Sequence that simulate real-world operations, which the Driver then translates into pin-level actions on the DUT.\nCentral to UVM\u2019s effectiveness is the Scoreboard, which compares actual results with expected outcomes to ensure the DUT performs correctly under various conditions.\nAdditionally, UVM supports various test modes and coverage collection techniques that further enhance testing thoroughness.\nThis method helps identify discrepancies and potential failures, enhancing the reliability and accuracy of the verification process. Reference Model Generation.\nIn UVM, reference models play a crucial role in verifying complex designs, such as those used in digital signal processing and cryptography, by providing high-level abstractions of the DUT. These models enhance simulation accuracy and efficiency, contributing to a more streamlined verification process. Traditionally, C/C++ is preferred for reference models in industry due to its seamless integration with SystemVerilog via Direct Programming Interfaces (DPI), which accelerates verification cycles\u00a0[24, 25, 26].\nIn this context, the capabilities of LLMs are especially relevant. Given the abundance of open-source datasets, LLMs have shown remarkable proficiency in generating C/C++ code, making them well-suited to assist in crafting adaptable, high-quality reference models. These LLM-generated models can dynamically respond to the intricate demands of verification, continuously updating to support high-fidelity simulations and robust design validation. Extensibility.\nThe extensibility of the UVM is particularly evident when considering the integration of automated assertion generation. UVM\u2019s structured, modular framework for verification is optimally configured to incorporate advanced enhancements such as AI-driven assertions, which can systematically verify that the design behaves as expected across various protocols like APB (Advanced Peripheral Bus) and AHB (Advanced High-Performance Bus)\u00a0[27]. \nIII-C Post-processing via Localization Engine\n Current LLM-aided verification methods tend to use minimally processed logs as inputs, which are often low in information density, thereby diminishing the efficiency of LLMs in diagnosing and fixing errors. To address this, we adopted time-aware dynamic error localization\u00a0[8] to extract more concrete and high-value information from these logs with methods.\nThis method, tailored for HDL environments, surpasses the static localization method described in Section\u00a0III-A by providing greater precision and temporal sensitivity. The localization engine leverages dynamic analysis and temporal insights to detect discrepancies between expected and actual signal outputs as recorded in the UVM log. These discrepancies are crucial for performing dynamic slicing through data flow graphs (DFGs), as outlined in Algorithm\u00a02.\nTo optimize token usage, UVLLM\u00a0adopts a segmented information extraction strategy. Initially, mismatch signals are input into the LLM\u2019s prompt as indicators of potential errors. If subsequent repair attempts fail, this indicates that relying solely on mismatched signals may be insufficient. To increase diagnostic precision, the system then incorporates actual execution paths\u2014identified as suspicious\u2014into the analysis alongside the error signals.\nThis approach focuses on actual execution paths in operation, leading to more precise identification of errors. Rollback Mechanism.\nDuring the development of UVLLM, a Rollback mechanism was implemented to address the issue of inaccuracies in LLM outputs, often termed \u201challucination\u201d\u00a0[28, 29, 30, 31, 32]. This feature is crucial for preventing the accumulation of errors across iterations due to reliance on flawed candidate repairs. Despite previous studies utilizing LLMs as reward models to assess repair quality[17, 33], there is a lack of robust quantitative metrics to effectively measure the correctness of these modifications. This gap can lead to inefficiencies in the rollback process, potentially triggering false positive rollbacks. Within the UVM framework, the quality of Verilog code iteration is evaluated using a scoreboard that assigns the test pass rate. We assume that higher scores correlate with fewer errors and better functionality.\nThe Rollback mechanism functions by preserving a history of code versions and their scores. If a new iteration scores lower than a previous one, indicating a decline in code quality, the mechanism reverts to the highest-scoring version, as illustrated in Fig.\u00a02. The alterations that led to the decrement in score are thus recorded as \u201cdamage repairs\u201d, which is utilized in Fig.\u00a04. \nIII-D Repair Agent\n The LLM agent functions as an adept RTL repair expert, leveraging three key inputs: the design specification, which outlines intended functionality and port definitions; RTL code; and error information. To enhance the repair process and facilitate iterative improvements, the system incorporates \u201cdamage repairs\u201d as an additional input, crucial for preventing the recurrence of unsuccessful corrections when the rollback mechanism is activated.\nIn a multi-agent setup, specific modifications to the prompts for each agent address different aspects of the debugging process, enabling a more nuanced and thorough error analysis in the RTL code. The primary prompts that guide the debugging activities of these agents are illustrated in Fig.\u00a04, emphasizing a tailored approach to error resolution. Formalizing agent\u2019s outputs.\nIt\u2019s commonly noticed that LLM-generated responses tend to include detailed explanations during debugging, which can clutter the main objective of code debugging.\nIn light of our observations, it becomes evident that LLMs exhibit enhanced debugging capabilities with superior reasoning process.\nTo prevent the inclusion of irrelevant details and erase the hallucination during the iterative cycle, a method for distilling the responses generated by the LLM is adopted. Utilizing the Structured Outputs method enables adherence to JSON Schema\u00a0[34], thereby ensuring that responses are consistently formatted according to predefined structures. By requiring the response to be in JSON format and to contain an element labeled \u201ccorrect\u201d which consists of pair of wrong codes and right codes, the code sections accentuated in Fig.\u00a04 are refined and carried into the subsequent iteration. \nIII-E Benchmark Generation\n The incidence of errors in module code is closely related to specific attributes such as code length and functional complexity\u00a0[35, 36, 37]. To evaluate the efficacy of the verification methodology, a well constructed evaluation dataset was developed through a systematic selection of samples from validated open-source datasets, representing a diverse range of codebases. These samples were then deliberately infused with typical errors to simulate real-world coding mistakes. In design bases combining both commercial and open-source IPs, a comparative analysis was performed on two consecutive versions of the code: one immediately before and another after code repository commits. This analysis focused on identifying discrepancies and documenting changes made during the design process, effectively highlighting the differences between pre and post-commit versions. These error-modification pairs, detailed in Table\u00a0I, were crucial for developing prompts for LLM and for terms used in pattern matching, showcasing common human-made errors such as misuse of assignments and the mismatch port in instantiation.\nThis approach showcases common human-made errors, such as misuse of assignments and mismatches in port instantiation, and provides a crucial benchmark for evaluating the verification effectiveness. \nIV Evaluation\n This section presents our experimental setup, evaluation metrics research questions, and discussions to the results. Setup.\nIn our experiment, we employed LLM agents via the OpenAI API, with GPT-4-turbo as the default model.\nAn evaluation benchmark was then constructed using the extensively verified RTLLM dataset\u00a0[38], which encompasses a diverse array of real-world errors. The initial code\u2019s ability to pass the compiler is indicated by a syntax error or functional error.\nWe utilized a range of simulation tools including VCS\u00a0[39], Iverilog\u00a0[40], Modelsim\u00a0[41], Yosys\u00a0[42], and the linting tool Verilator\u00a0[43] to ensure comprehensive verification and analysis.\nWe set the threshold of iterations to 5, as the improvement is hardly observed after that.\nAll experiments were conducted on an AMD EPYC 7763 2.45GHz CPU.\nFor each instance, we asked LLMs for 5 times to reduce the randomness of the response. \nIV-A Evaluation Metrics\n Recent work\u00a0[44, 45], tended to use pass@k metrics to assess functional correctness.\nFor each problem in the problem set, k code samples are generated at a time, and the problem is considered solved if any sample passes the simulation test. Hit Rate (HR). Specifically, our framework quantifies effectiveness using Hit Rate (HR)\u00a0[46]. For erroneous code \u03b8isubscript\ud835\udf03\ud835\udc56\\theta_{i}italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and its corrected version \u03b8i\u2217superscriptsubscript\ud835\udf03\ud835\udc56\\theta_{i}^{*}italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT, we evaluate a set of test cases {(xi1,yi1),\u2026,(xim,yim)}superscriptsubscript\ud835\udc65\ud835\udc561superscriptsubscript\ud835\udc66\ud835\udc561\u2026superscriptsubscript\ud835\udc65\ud835\udc56\ud835\udc5asuperscriptsubscript\ud835\udc66\ud835\udc56\ud835\udc5a\\{(x_{i}^{1},y_{i}^{1}),\\ldots,(x_{i}^{m},y_{i}^{m})\\}{ ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) , \u2026 , ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) }. The corrected code \u03b8i\u2217superscriptsubscript\ud835\udf03\ud835\udc56\\theta_{i}^{*}italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT must produce the correct output yijsuperscriptsubscript\ud835\udc66\ud835\udc56\ud835\udc57y_{i}^{j}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT for each input xijsuperscriptsubscript\ud835\udc65\ud835\udc56\ud835\udc57x_{i}^{j}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT, ensuring all cases pass. That is, \u22c0j=1ma\u03b8i\u2217\u2062(xij)=yijsuperscriptsubscript\ud835\udc571\ud835\udc5asubscript\ud835\udc4esuperscriptsubscript\ud835\udf03\ud835\udc56superscriptsubscript\ud835\udc65\ud835\udc56\ud835\udc57superscriptsubscript\ud835\udc66\ud835\udc56\ud835\udc57\\bigwedge_{j=1}^{m}a_{\\theta_{i}^{*}}\\left(x_{i}^{j}\\right)=y_{i}^{j}\u22c0 start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT italic_a start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ) = italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT. The overall rate for n corrcted versions is calculated as: HR measures the proportion of instances resolved under test cases. Fix Rate (FR).\nTo overcome limitations in test coverage, our framework includes Fix Rate (FR), which involves independent expert validation of the proposed fixes \u03b8i\u2217superscriptsubscript\ud835\udf03\ud835\udc56\\theta_{i}^{*}italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT. After expert review, if the fix is confirmed effective across additional scenarios, it is designated as \u03b8^i\u2217superscriptsubscript^\ud835\udf03\ud835\udc56\\hat{\\theta}_{i}^{*}over^ start_ARG italic_\u03b8 end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT: FR reflects the framework\u2019s repair effectiveness in broader conditions. Execution Time.\nIn evaluating the framework\u2019s performance, this paper emphasizes execution time as a critical metric, defined as the time interval from the initial design input into UVLLM\u00a0to the completion of the final code output. \nIV-B Results and Discussions\n Result 1:The UVLLM\u00a0framework\u2019s effectiveness in enhancing hardware design verification is demonstrated through a comparative evaluation of Fix Rates (FRs) among various methodologies, as depicted in Figures 6 and 6. This analysis encompasses traditional script-based approaches\u00a0[8, 9], current LLM-aided methods\u00a0[17], and a baseline incorporating GPT-4-turbo for benchmark repairs. Across all categories of errors\u2014both syntax and functional\u2014the UVLLM\u00a0framework consistently achieves higher FRs. Specifically, for syntax errors, the UVLLM\u00a0framework records an average FR of 87.6%, representing a significant 26.9% improvement over the second performing method, MEIC. For functional errors, UVLLM\u00a0continues to surpass other methods, registering a FR of 67.3%, which is a 36.3% enhancement relative to MEIC. Notably, in the case of Incorrect bitwidth as illustrated in Fig.\u00a06, UVLLM\u2019s FR is more than double that of the next best method, RTLrepair. Moreover, even in less favorable scenarios, such as Declaration errors, UVLLM\u2019s FR remains approximately 5% above that of MEIC, its nearest competitor. These results clearly demonstrate that UVLLM\u00a0consistently outperforms other methods, achieving significantly higher FRs across diverse scenarios. Result 2: The evaluation of the UVLLM\u00a0framework\u2019s capability to ensure that repaired code adheres to specification requirements is conducted through an examination of the correctness of repaired code. Although many methods achieve high hit rates (HRs), their fixes often overfit to specific input-output (IO) pairs, revealing a discrepancy between HRs and FRs. Figures 6 and 6 illustrate this disparity; the shaded areas represent the deviation between HR and FR, highlighting the failure of these methods to detect numerous errors, which leads to false negatives and insufficient coverage. Specifically, for syntax errors, UVLLM\u00a0demonstrated no deviation across all scenarios, while deviations for other methods were observed in 4 out of 5 scenarios with an average of 5% variations, confirming the achievement of high coverage for syntax errors. In contrast, for functional errors, the deviations for other methods were notably higher (all above 30%) whereas UVLLM\u00a0maintained a minimal deviation of only 1.4%. Notably, UVLLM\u00a0had a maximum deviation of just 5.6% for Logic errors, while the other methods displayed deviations exceeding 40% for Flawed conditions. These results suggest that while other methods struggle to achieve comprehensive coverage for functional errors, UVLLM\u00a0effectively mitigates this limitation. These findings indicate that UVLLM\u00a0significantly enhances the practicality of hardware design verification by integrating formal verification processes, to meet the specification requirements to the greatest extent possible. Result 3: The UVLLM\u00a0framework\u2019s repair capabilities across a diverse range of hardware modules were evaluated by analyzing the FRs of 27 common modules, each injected with nine distinct types of syntax and functional errors. These modules were categorized into ten representative types, such as adders, counters, and FSMs, to establish a comprehensive benchmark for evaluating the framework\u2019s generalization in various verification scenarios. As illustrated in Fig.\u00a07, where the framework\u2019s FRs were depicted using color coding, UVLLM\u00a0exhibited exceptional adaptability, achieving robust FRs in simpler modules like counters. For instance, the FRs for syntax errors and functional errors in these modules reached 100% and 95%, respectively. In contrast, the FRs were lower in more complex modules, such as FSMs, with FRs for syntax errors and functional errors at 89% and 32%, respectively. This indicates that repairing more complex designs remains challenging. Across the same types of module, syntax errors consistently exhibited higher FRs than functional errors, reflecting UVLLM\u2019s proficiency in addressing syntactic issues.\nThis advantage stems from the extensive training of the LLM on a substantial corpus of HDL code data, enhancing its syntactic understanding.\nAdditionally, the compiler and linter contribute detailed localization information that aids in the repair of syntax errors. Overall, the framework achieved an FR of 86.99% for syntax errors and 71.92% for functional errors, representing its reliability across diverse modules and error scenarios. The repair operation of UVLLM\u00a0comprises three stages: Pre-processing, Repair in Mismatch Signal (MS) Mode, and Repair in Suspicious Line (SL) Mode. The columns labeled F\u2062R\ud835\udc39\ud835\udc45FRitalic_F italic_R and Te\u2062x\u2062e\u2062csubscript\ud835\udc47\ud835\udc52\ud835\udc65\ud835\udc52\ud835\udc50T_{exec}italic_T start_POSTSUBSCRIPT italic_e italic_x italic_e italic_c end_POSTSUBSCRIPT indicate the contributions of each stage to the fix rate and execution time, respectively. The column labeled UVLLM\u00a0summarizes the total contributions across all stages of the repair operation. Modules are grouped as Arithmetic (Accumulator, Adder, Divider, Multiplier), Control (Counter, FSM), Memory, and Miscellaneous (other modules). Errors are categorized as syntax (\u201cs\u201d) and function (\u201cf\u201d). Result 4: The UVLLM\u00a0framework\u2019s evaluation primarily focuses on how its distinct stages contribute to the fix rate and execution time during the repair operation, providing insights into each segment of the verification process. Table\u00a0II details the repair process, which unfolds in several stages, each playing a different role in resolving syntax and functional errors. The Pre-processing stage was particularly effective in addressing syntax errors, successfully resolving 74.72% of these cases as highlighted. For functional errors, the Mismatch Signal (MS) mode in the Repair stage was most effective, correcting 41.46% of the instances. The adoption of segmented steps enables UVLLM\u00a0to work effectively and adapt flexibly to various verification scenarios. For errors strictly related to syntax, the majority were successfully corrected during the pre-processing stage. However, 11.29% of syntax-only errors persisted and advanced to the subsequent repair stage in MS mode. Similarly, the attempt to resolve 25.96% of functional errors inadvertently introduced new syntax issues, which were then addressed by the pre-processor. This demonstrates UVLLM\u2019s ability to compensate for new errors introduced in earlier stages and mitigate the uncertainties associated with LLMs. In terms of execution time, the segmented repair operation shows that the pre-processing stage, despite handling over 50% of all benchmark repairs, typically requires less time than repairs conducted in MS mode. This demonstrates the efficiency benefits of incorporating a robust pre-processing stage. Result 5: The UVLLM\u00a0framework\u2019s execution efficiency was evaluated against existing methods from a multidimensional perspective, mainly focusing on two key metrics: Failure Rates (FRs) and execution time Te\u2062x\u2062e\u2062csubscript\ud835\udc47\ud835\udc52\ud835\udc65\ud835\udc52\ud835\udc50T_{exec}italic_T start_POSTSUBSCRIPT italic_e italic_x italic_e italic_c end_POSTSUBSCRIPT, as detailed in Table\u00a0II. While Fig.6 and Fig.6 show the variations in FR performance across different error types, UVLLM\u00a0consistently surpassed MEIC across all module types. For instance, within the Miscellaneous modules for syntax errors in Table\u00a0II, UVLLM\u00a0achieved an FR of 88.50%, which is 21.83% higher than MEIC\u2019s 66.67%. In handling functional errors within Arithmetic modules, characterized by their complex logic, UVLLM\u00a0maintained an FR of 66.23%, significantly outperforming MEIC\u2019s 40.53%. These results underscore UVLLM\u2019s robust capability to effectively resolve a wide spectrum of errors across different modules compared to existing methods, thus proving its effectiveness and reliability in boosting system performance. In terms of operational efficiency, UVLLM\u00a0also demonstrated a substantial reduction in execution time. For example, when processing syntax errors within the Miscellaneous modules, UVLLM\u00a0recorded an average execution time of 13.47s, marking a speedup of 4.65x compared with MEIC. This advantage was even more pronounced when addressing complex functional errors, where UVLLM\u00a0achieved a speedup of up to 16.56x over MEIC in Arithmetic modules. On average, UVLLM\u00a0operated 10.42x faster than MEIC, while simultaneously achieving higher test coverage and pass rates. These findings highlight UVLLM\u2019s potential to significantly enhance the design verification process for practical deployment by merging increased automation with superior efficiency. \nIV-C Ablation Study\n Our research includes the ablation study designed to evaluate the impact of iteration strategies on the effectiveness of the framework. Repair generation form. We initially employ an approach that uses original-repair code pairs to facilitate the generation of new code by leveraging outputs from LLMs. However, the ablation study examines an alternative method where entire code snippets are directly produced by the LLMs, omitting the generation of repair pairs. As shown in Table\u00a0III, generating complete code snippets resulted in a slight decline in repair accuracy and an increase in execution time compared to generating original-repair code pairs. Nevertheless, there were specific scenarios where this direct generation method proved superior. This advantage is mainly due to the ability of the direct generation method to handle minor errors that pose significant challenges for LLMs in terms of search efficiency. In some cases, regenerating the entire code is more effective than trying to modify or replace segments of the existing code. For instance, correcting the error \u201cm\u2062o\u2062d\u2062u\u2062l\u2062e\u2062a\u2062(A);\u2026\u2062(M\u2062i\u2062s\u2062s\u2062i\u2062n\u2062g\u2062D\u2062e\u2062f\u2062i\u2062n\u2062i\u2062t\u2062i\u2062o\u2062n\u2062o\u2062f\u2062P\u2062o\u2062r\u2062t\u2062A)\u2062\u2026\u2062e\u2062n\u2062d\u2062m\u2062o\u2062d\u2062u\u2062l\u2062e\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc62\ud835\udc59\ud835\udc52\ud835\udc4e\ud835\udc34\u2026\ud835\udc40\ud835\udc56\ud835\udc60\ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc54\ud835\udc37\ud835\udc52\ud835\udc53\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc5c\ud835\udc53\ud835\udc43\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc34\u2026\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc62\ud835\udc59\ud835\udc52module\\ a(A);...(Missing\\ Definition\\ of\\ Port\\ A)...endmoduleitalic_m italic_o italic_d italic_u italic_l italic_e italic_a ( italic_A ) ; \u2026 ( italic_M italic_i italic_s italic_s italic_i italic_n italic_g italic_D italic_e italic_f italic_i italic_n italic_i italic_t italic_i italic_o italic_n italic_o italic_f italic_P italic_o italic_r italic_t italic_A ) \u2026 italic_e italic_n italic_d italic_m italic_o italic_d italic_u italic_l italic_e\u201d proves challenging for the replacement strategy, primarily because the essential context is frequently overlooked, whereas the reproduction method handles it more straightforwardly. \nV Conclusion\n In this work, an automated universal verification framework, UVLLM, which comprehensively addresses main phases of hardware design verification, including testbench construction, test execution, result analysis, and repair, is proposed.\nTested on the proposed benchmark, UVLLM\u00a0achieves average syntax and functional error fault rates of 86.99% and 71.92%, respectively, while maintaining nearly 100% test coverage. Additionally, UVLLM\u00a0performs 10.42 times faster than the previous MEIC framework.\nThe framework demonstrates that it is feasible to employ the LLMs for the purpose of Verilog code verification, irrespective of the initial code state. The utilization of reasonable segmentation and feedback engineering leads to an improvement in the verification efficiency of the framework. References"}
{"text": "Performance Implications of Multi-Chiplet Neural Processing Units on Autonomous Driving Perception We study the application of emerging chiplet-based Neural Processing Units to accelerate vehicular AI perception workloads in constrained automotive settings. The motivation stems from how chiplets technology is becoming integral to emerging vehicular architectures,\nproviding a cost-effective trade-off between performance, modularity, and customization; and from perception models being the most computationally demanding workloads in a autonomous driving system.\nUsing the Tesla Autopilot perception pipeline as a case study, we first breakdown its constituent models and profile their performance on different chiplet accelerators. From the insights, we propose a novel scheduling strategy to efficiently deploy perception workloads on multi-chip AI accelerators. Our experiments using a standard DNN performance simulator, MAESTRO, show our approach realizes 82% and 2.8\u00d7\\times\u00d7 increase in throughput and processing engines utilization compared to monolithic accelerator designs. \nI Introduction\n The landscape of the automotive industry is being transformed through software-defined vehicles (SDVs) that enable integrating sought-out features of Advanced Driver Assistance Systems (ADAS), autonomy and infotainment (AR/gaming) [1, 2, 3, 4].\nTo maintain flexibility and manage rising compute demands, emerging vehicular systems are shifting towards new cross-domain, centralized Electrical/Electronic (E/E) architectures with a few powerful processing computers and zone ECUs [1], enabling easier integration of new features and updates, and allowing automakers to adopt customized chip design tailored to their needs.\nAs a result, automakers like Tesla, GM Cruise, Volkswagen have entertained the adoption of customized System-on-Chips (SoCs) for their automotive compute requirements [5, 6, 7].\nStill, as automotive AI workloads continue to evolve and rise in complexity, advancements in automotive SoC hardware also become a necessity, and that presents a considerable challenge given the long design cycle, high manufacturing costs, and Moore\u2019s law stagnation. Chiplets technology presents a viable solution for the automotive industry [2, 4, 8, 9]. Owing to chiplets\u2019 composability property, a scalable, customizable approach becomes feasible supporting the integration of individual hardware modules on the package level, potentially from different technology node generations, and enabling individual component updates to be incorporated with a faster turnaround time than monolithic SoC approaches. Even more so, recent advancements has seen Neural Processing Engines (NPUs) \u2013 integral to autonomous driving systems \u2013 implemented through consolidating multiple small accelerator chiplets on the package to form a scalable AI inference engine [10, 11, 12, 13, 14, 15], providing flexible means to control different design parameters (accelerator chips number, connection topology, and heterogeneity) [13, 14]. Still, NPUs constructed as multi-chiplet modules (MCM) remain largely understudied in the context of automotive AI workloads despite the challenges and potential gains. On the one hand, automotive AI workloads exhibit unique execution flows characterized by intricate dependencies, concurrencies, and feature fusion nodes [16, 17, 18]. On the other, AI computing kernels exhibit varying affinities towards different accelerator types [19, 13, 20, 21, 22]. Though multiple works [23, 24, 25, 26] have investigated the architectural implications for improving automotive AI workloads\u2019 performance, the architectural implications of adopting chiplets technology remains to be studied. This work aims to investigate such implications when an MCM AI accelerator is employed as the automotive NPU to accelerate AI perception workloads. Specifically, we follow the architectural template of Tesla\u2019s FSD chip [27], and simulate an industry-grade MCM inference accelerator engine \u2013 Simba [10, 15] \u2013 for the system\u2019s NPU (Figure 1). For the automotive workloads, we focus on those from the compute-intensive perception pipeline [24, 28, 16]. We implement and analyze such workloads following the Tesla Autopilot system [5] which entail HydraNets, spatio-temporal fusion, and multi-task heads (lane detection, occupancy networks). From the insights, we devise a novel scheduling methodology to enhance performance efficiency of perception workloads on the MCM-NPU. In summary, our key contributions are as follows: We characterize the key workloads existing in a SOTA perception pipeline (Tesla Autopilot), from the early feature extraction stage till the final multi-task heads models. We conduct a thorough performance breakdown of perception workloads to understand their execution properties and hardware acceleration affinities using the standard DNN performance simulator, MAESTRO [29, 30]. From our analysis, we implement a low-cost scheduling algorithm for mapping perception workloads onto MCM-NPUs given added Network-on-Package overheads and the diversity of models within and across the pipeline stages. We evaluate our solution against existing NPU baselines showcasing performance improvement trade-offs between utilization, energy, and pipelining latency. \nII Background and Preliminaries\n \nII-A Anatomy of a self-driving platform architecture: Tesla FSD\n We take the Tesla FSD (Full Self Driving) SoC as our reference self-driving architecture template. As illustrated in Figure 1, the FSD integrates the following units: CPU clusters. For general purpose compute. Each cluster constitutes a quad-core Cortex A72 in the Tesla FSD. Memory. Main memory component of the chip (e.g., LPDDR4 memory) Cameras I/F and ISP. High speed camera serial interface and image signal processor for image preprocessing NPUs. Custom-designed hardware accelerators for efficient processing of AI workloads GPU. For light-weight post-processing Other. Video encoder, safety component. For a chiplets technology variant, a system-in-package (SiP) can be constructed from this template integrating multiple smaller chiplets, each covering a subset of components and communicating over package via high-speed interconnects. Moreover, recent prototypes like Simba [10] have shown that the NPU component can also be implemented as MCMs integrating accelerator chiplets on the package level. \nII-B Autonomous Driving System Pipeline\n Autonomous driving systems (ADS) map sensory input data to vehicle control outputs through 3 stages: perception for contextual understanding and objects tracking in the driving scene; planner for trajectory paths generation; control for providing the necessary control outputs. Perception represents the most compute intensive stage [24, 17], and supports multiple driving tasks (detection, lane prediction). Recent perception modules have seen the adoption of the HydraNets architecture [16], with a shared backbone for extracting low-level, common features, and specialized heads for driving tasks specialization. In Figure 2, we illustrate the HydraNet architecture from the Tesla Autopilot system[31] discussed in the following: Sensor inputs. Tesla Autopilot perception relies on collecting images from 8 installed cameras. Typically images can be 720p resolution at around 30 FPS [24]. Stage 1: Feature Extraction (FE+BFPN). Each raw input image is pre-processed and passed through a feature extractor (FE) followed by a bidirectional feature pyramid network (BFPN) [32], to generate multi-scale feature representations.\nFollowing [28], the FE can be a ResNet18 architecture with 4 multiscale features (90x160x256, 45x80x512, 23x40x1024, and 12x20x2048) generated throughout its ResNet blocks, which are then passed through 2 Bi-directional FPN blocks (BFPN). At the final output, multiscale features are concatenated from each FE+BFPN pipeline forming the 8x20x80x256 outputs. Stage 2: Multi-cam Spatial Fusion (S_FUSE). Generated embeddings from each camera are fused onto a combined spatial representation in vector space (2D/3D grid map). A transformer with multi-attention head is employed, computing attention scores between every grid cell and detected features from each camera.\nThe attention module [33] comprises: QKV projection to generate Query (Q), Key (K), and Value (V) vectors; Attention with two matrix multiplications for (QKT)\u22c5\u22c5\\cdot\u22c5V; FFN (Feed-forward Network) comprising two linear layers. For a 20\u00d7\\times\u00d780 2D grid [31, 28], the output becomes a fused projection of the 8 camera features onto a 1x20x80x256 grid. Stage 3: Temporal Fusion (T_FUSE). The 1x20x80x256 spatial representation is fed into a video queue to be fused with a feature queue of N previous representations to accommodate necessary temporal features (e.g., seen signs or lane markings), and telemetry information. Another attention module can be used with N=12 for temporal fusion [31, 28], each undergoing QKV projection, attention, and FFN transformation till the final fused spatio-temporal 1x20x80x300 representation. Stage 4: Trunks and Heads (TR). The 1x20x80x300 representation is fed to branched trunks and heads including: Occupancy network predicting the grid\u2019s continuous occupancy probability and continuous semantics, involving 4 spatial deconvolution layers with 16x upscaling Lane prediction involving a combination of self-attention and cross-attention layers repeated 3 times for 3 levels of point predictions, and having 3 classifier predictors Object detection Multiple detection heads (traffic, vehicle, pedestrians) are supported. Each detector head entails separate class and box prediction networks using a sequence of 3 convolution layers and fully connected layer. \nIII Analysis of Perception Module Workloads\n To derive design insights for MCM-based NPUs, We first analyze the perception workloads\u2019 performance on various accelerator architectures from Section II-B using the analytical DNN cost model simulator, MAESTRO [29, 30] (known to have achieved 96% accuracy compared to RTL simulations). We consider the following: We set the number of PEs in each accelerator to 256 PEs. That way, when accelerators are assimilated as chiplets into a 6\u00d7\\times\u00d76 MCM like Simba [10], a total of 9,216 PEs will be available equivalent to that of the original Tesla NPU [27]. We use the same operating frequency at 2 GHz [27]. We analyze the workloads on weight stationary (WS) and output stationary (OS) accelerator types \u2013 like NVDLA [34] and Shidiannao [35]) \u2013 given their proven superiority over other accelerator types [19, 36, 13]. \nIII-A Coarse-grained Performance Analysis\n In Figure 3, we breakdown the contributions of the perception stage models in terms of latency and energy on NVDLA- and Shidiannao-like accelerators and deduce the following: OS dataflow suits latency-critical workloads. Across all workloads, the Shidiannao OS dataflow offers 6.85\u00d7\\times\u00d7 speedups over its WS counterparts, making it the prime candidate for latency critical automotive workloads [24]. WS offers energy efficiency opportunities. On average, the WS dataflows can lead to 1.2\u00d7\\times\u00d7 energy efficiency gains compared to OS. The gains are even more significant (1.55\u00d7\\times\u00d7) if we omit S_FUSE and T_FUSE from our analysis, which are more affine towards OS dataflow. This opens the door for potential heterogeneous integration (mix of OS and WS chiplets) to realize interesting performance trade-offs. Fusion Modules are computational bottlenecks. The S_FUSE and T_FUSE modules constitute respective 25%-28% and 52%-54% of the overall perception module latency under both dataflow scenarios. This is attributed to the feature aggregation from multiple sources (8 cameras and 12 temporal frames) onto a shared projection space (200\u00d7\\times\u00d780\u00d7\\times\u00d7256 grid). FE+BFPN Scaling. Evaluations for the FE+BFPN in Figure 3 are for a single camera and to be multiplied by the 8 cameras. \nIII-B Fine-grained Performance Analysis\n We analyze the individual layers affinities towards OS and WS from the various models in Figure 4, where we compare the difference in values between the OS and WS dataflows, \u0394\u2062V\u2062a\u2062l\u2062u\u2062e\u0394\ud835\udc49\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52\\Delta Valueroman_\u0394 italic_V italic_a italic_l italic_u italic_e = V\u2062a\u2062l\u2062u\u2062eO\u2062S\ud835\udc49\ud835\udc4e\ud835\udc59\ud835\udc62subscript\ud835\udc52\ud835\udc42\ud835\udc46Value_{OS}italic_V italic_a italic_l italic_u italic_e start_POSTSUBSCRIPT italic_O italic_S end_POSTSUBSCRIPT - V\u2062a\u2062l\u2062u\u2062eW\u2062S\ud835\udc49\ud835\udc4e\ud835\udc59\ud835\udc62subscript\ud835\udc52\ud835\udc4a\ud835\udc46Value_{WS}italic_V italic_a italic_l italic_u italic_e start_POSTSUBSCRIPT italic_W italic_S end_POSTSUBSCRIPT, for latency and energy (-ve values imply OS affinity and +ve values imply WS affinity). FE+BFPN tradeoffs. The FE+BFPN stage exhibits a direct trade-off between latency and energy across all layers. Yet, the non-uniform distribution of performance gains, the complex dependencies of the FE and BFPN networks, and the concurrent execution requirement for 8 FE+BFPN models impose strict requirements on accelerator chiplets assignment process. S_FUSE and T_FUSE tradeoffs. The negative evaluations of \u0394\u0394\\Deltaroman_\u0394Latency and \u0394\u0394\\Deltaroman_\u0394Energy across all layers indicate a strong affinity towards the OS dataflow for the fusion modules. We also observe significant performance bottlenecks exist at the self-attention layers (QKV calculations at layer ids 1 and 2). Trunks tradeoffs. The diversity of trunk models lead to varying affinities: the lane prediction trunk is completely skewed towards the OS dataflow (due to attention modules), unlike other trunks which can provide exploitable trade-offs. \nIII-C Design Insights.\n Based on our analysis, we derive the following insights onto scheduling perception workloads onto MCM-based NPUs: Any scheduling configuration is to target maintaining a consistent throughput (pipelining latency) across the various perception stages for streamlined input processing. Heterogeneous chiplets integration can be supported as long as the latency constraint is not violated. Given the dominance of the OS dataflow with regards to execution latency, we specify the pipelining latency of the OS dataflow as the reference latency constraint. Any optimizations for the FE+BFPN stage must be applied for the 8 concurrent models for simultaneous execution. The computational bottleneck of the fusion layers are confined within a small number of layers (see Figure 4), making them targets for parallelization optimizations. Heterogeneous chiplets integration can be particularly beneficial for the diverse trunk workloads to elevate efficiency. \nIV Proposed Scheduling Methodology\n We propose to schedule the perception workloads on MCM-NPU through a throughput matching mechanism as follows: Specify initial execution latency estimates and chiplet allocation per layer in each stage Allocate chiplet resources across the various workload stages based on the latency estimates Alleviate bottleneck stages impact via data sharding Update latency estimates and chiplet assignments Repeat steps 2-4 until pipelining latencies are matched or no further sharding is possible In Algorithm 1, we achieve this sequence through a nested greedy algorithm whose outer loop alleviates bottlenecks across stages, and inner loop alleviates bottlenecks across layers in the bottleneck stages. Line 2 demonstrates how chiplets are initially allocated to each stage based on a function of the model execution latencies, the number of workloads, and the number of models per each stage workload. We initially adopt uniform partitioning of chiplet resources across the 4 stages, where each is assigned a separate quadrant of chiplets. The reasons being: (i) having 4 distinctive perception stages; (ii) having 8 processing parallel model instances of the FE+BFPN; (iii) the latency bottlenecks being in the fusion modules with small number of layers; (iv) the diversity of the trunk models. \nIV-A Choosing the FE+BFPN for the base pipelining latency \n Given 8 model instances of the FE+BFPN stage where each possesing a compute latency around 1818\\frac{1}{8}divide start_ARG 1 end_ARG start_ARG 8 end_ARG of the T_FUSE bottleneck (Figure 3), at least 8 chiplets need to be initially allocated to each model to maintain concurrent execution.\nFrom the latency contribution breakdown in Figure 3 across the 4 stages, assigning additional resources to the FE+BFPN would lead to suboptimal solution since it leaves just above half of the total number of chiplets for processing >>>90% of the overall workloads.\nTherefore, it becomes reasonable to specify the initial base pipelining latency to that of the FE+BFPNs models (L\u2062a\u2062tb\u2062a\u2062s\u2062e\ud835\udc3f\ud835\udc4esubscript\ud835\udc61\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52Lat_{base}italic_L italic_a italic_t start_POSTSUBSCRIPT italic_b italic_a italic_s italic_e end_POSTSUBSCRIPT=82.7 ms) as shown in Figure 5. \nIV-B Alleviating S_FUSE and T_FUSE bottlenecks\n We alleviate the attention fusion bottlenecks through recursive sharding until the stages\u2019 latency equates that of L\u2062a\u2062tb\u2062a\u2062s\u2062e\ud835\udc3f\ud835\udc4esubscript\ud835\udc61\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52Lat_{base}italic_L italic_a italic_t start_POSTSUBSCRIPT italic_b italic_a italic_s italic_e end_POSTSUBSCRIPT. S_FUSE Bottleneck. This attention module operates on 8 feature input sets from the FE+BFPN stage outputs. Given 200\u00d7\\times\u00d780\u00d7\\times\u00d7256 attention grid dimensions, S_FUSE stage incurs 78.7 ms, 20.5 ms, and 236 ms latency overheads for the QKV projection, self-attention, and Feed-Forward (FFN) layers when each is processed on an individual chiplet. Accordingly, the spatial FFN layer is sharded in a four-folded manner, replicating the FFN weights on 4 chiplets, each processing features from two FE+BFPNs. This brings FFN latency close to the QKV projection (78.7 ms) and L\u2062a\u2062tb\u2062a\u2062s\u2062e\ud835\udc3f\ud835\udc4esubscript\ud835\udc61\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52Lat_{base}italic_L italic_a italic_t start_POSTSUBSCRIPT italic_b italic_a italic_s italic_e end_POSTSUBSCRIPT (82.7 ms). As S_FUSE is still left with 4 chiplets (one is surplus from the FE+BFPN stage), an additional sharding step can be performed, leading to the final configuration shown in Figure 6. T_FUSE Bottleneck.\nPrior to sharding the 12 frames, the T_FUSE took 165.6 ms, 36.4 ms, and 490.2 ms for the QKV projection, self-attention, and FFN blocks, respectively. Our algorithm in this case involves two inner loop iterations, first distributing the FFN block workloads across 6 chiplets, leading FFN layer pipeline latency to drop down to 81.7 ms close to L\u2062a\u2062tb\u2062a\u2062s\u2062e\ud835\udc3f\ud835\udc4esubscript\ud835\udc61\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52Lat_{base}italic_L italic_a italic_t start_POSTSUBSCRIPT italic_b italic_a italic_s italic_e end_POSTSUBSCRIPT. Then, the QKV projection is partitioned across two chiplets dropping the layer pipelining latency to 78.7 ms. All 9 chiplets in the quadrant are utilized as shown in Figure 7. \nIV-C Design Space Exploration for the Trunks stage\n Given the diversity of the trunk models and their customization, we employ a design space exploration for this stage to manage the mapping of workloads onto chiplets. Additionally, we consider heterogeneous integration options based on the analysis in Figure 4 to leverage potential energy efficiency gains. In particular, we consider two configurations, Het(2) and Het(4), which integrate 2 and 4 WS chiplets within the OS-dominated 3\u00d7\\times\u00d73 chiplets quadrant, respectively. We specify the following scoring function for our search: where a scheduling configuration (c\u2062o\u2062n\u2062f\u2062i\u2062g\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc53\ud835\udc56\ud835\udc54configitalic_c italic_o italic_n italic_f italic_i italic_g) is only considered on the basis of its Energy-Delay Product evaluation as long as the pipeline latency (Lc\u2062s\u2062t\u2062r)L_{cstr})italic_L start_POSTSUBSCRIPT italic_c italic_s italic_t italic_r end_POSTSUBSCRIPT ) is not violated by any chiplet.\nGiven the relatively small size of the search space (9 chiplets, 3 models, and <<< 100 layers), we perform a brute force search with the results provided in Table I at Lc\u2062s\u2062t\u2062rsubscript\ud835\udc3f\ud835\udc50\ud835\udc60\ud835\udc61\ud835\udc5fL_{cstr}italic_L start_POSTSUBSCRIPT italic_c italic_s italic_t italic_r end_POSTSUBSCRIPT = 85 ms. From the table, we can see that the heterogeneous configurations, Het(2) and Het(4) realize performance efficiency improvements in energy and EDP compared to the OS only configuration, achieving average 1.1% and 6.2% reductions in raw energy; 17.4% and 12.0% reductions in EDP, respectively.\nUpon careful inspection, we found the WS chiplets are predominantly assigned to the DET_TR layers, leading DET_TR independently to achieve a 35% reduction in energy. \nIV-D NoP Cost Modeling\n We model the NoP data movement overheads using a cost model built around MAESTRO [29, 30] and the microarchitecture parameters from Simba [10] scaled to 28 nm as follows: NoP Interconnect BW: 100 GB/s/chiplet NoP Interconnect Lat: 35 ns/hop NoP Interconnect Ergy: 2.04 pJ/bit Transmission latency is computed as a function of feature map size over the NoP bandwidth, multiplied by the number of hops from source to destination. Whereas transmission energy is the multiplication of feature map size, interconnect bit transmission energy, and the number of hops. We analyze the NoP data movement overheads in Figure 9 across the different layer workloads following the scheduling configuration of the previous subsections and make the following observations: (i) Large feature map outputs (as from QKV_Proj layers) have high transmission costs (latency and energy) and are placed in close proximity to their destinations to limit overheads (Figures 6 and 7); (ii) The gathering of sharded outputs from multiple far nodes (S_FFN) can cause rise in NoP traffic and the corresponding latency. (iii) Most importantly, the overall perception latency bottleneck does not lie in NoP transmission overheads as they are at least two orders of magnitude less than the computational costs in Figure 3. \nV Evaluation\n We evaluate our MCM scheduling solution against baselines with the same number of PEs: A single accelerator chiplet with 9,216 PEs (Regular) Two accelerator chiplets with 4,608 PEs each Four accelerator chiplets with 2,304 PEs each We consider two pipelining schemes for the evaluation: (1) stagewise, and (2) layerwise.\nWe also analyze how our solution can scale to two NPUs (2\u00d7\\times\u00d76\u00d7\\times\u00d76 Simba MCMs for a total of 72 chiplets).\nUnless otherwise stated, we focus our analysis on the multi-chiplet NPU with OS only dataflow. We perform comparisons on the first 3 bottleneck stages of the perception pipeline. A separate ablation study is performed for the trunks. \nV-A Comparison against Baselines\n We show the comparison against the baselines in Table II. Though the 6\u00d7\\times\u00d76 solution achieves the best pipelining latency scores with 87 ms, it incurs additional energy consumption from the overhead associated with the NoP data transmission, leading it to incur a 10.9% increase in energy consumption compared to the single chiplet solution. Still, the 6\u00d7\\times\u00d76 solution achieves the lowest EDP of 69 J*ms as a result of low end-to-end latency from the cross-chiplet sharding.\nIn terms of PEs utilization, the 6\u00d7\\times\u00d76 solution achieves an average of 54.19% utilization across all chiplets\u2019 PEs, constituting a 2.8\u00d7\\times\u00d7, 2.1\u00d7\\times\u00d7, and 1.7\u00d7\\times\u00d7 increase over the three respective baselines. \nV-B Scaling to 2 Multi-chiplet NPUs\n We explore how our solution scales to 72 chiplets if the two NPUs on the FSD are active and processing the same workloads (recall Fig 1). We assume in this analysis the number of trunks is doubled (assigned 2\u00d7\\times\u00d79 chiplets), and that they incur a fixed performance overhead not becoming the the latency bottleneck. In Figure 10, we show how our algorithm progresses to reduce the pipelining latency and assigns chiplet resources. First, our algorithm extends the sharding of the temporal projection layer T_QKV from 2 to 4, reducing its pipelining latency to 41.1 ms. Then, T_FFN layer is assigned an additional 6 chiplets to bring its latency down to 40.8 ms. At this point, the sharding is exhausted for the T_FFN as each temporal frame is processed independently on a separate chiplet. The FE+BFPN layer is then partitioned into two pipelining stages at the fourth convolutional ResNet-18 block, yielding two equivalent partitions with pipelining latency of 40.04 ms. Lastly, S_PROJ is divided across two chiplets to yield a latency at 39.4 ms. The final pipelining latency is 41.1 ms, almost 2\u00d7\\times\u00d7 that of the 36 chiplet case. \nV-C Ablation on Optimizing Trunk models Design\n We performed additional ablation studies on the trunk models to pinpoint their performance bottlenecks. Occupancy Trunk. Deconvolution operations are the bottleneck of the occupancy trunk. In Table III, we showcase the non-linear increase in latency incurred in the occupancy network with each added upsampling layer, with the final upsampling layer contributing the most to the overall latency (\u223csimilar-to\\sim\u223c75%). This underpins an exploitable trade-off between resolution granularity and performance efficiency. Lane Prediction Trunk. \nIn the Tesla Autopilot System, the lane prediction network adopts a form of context-aware computing for efficiency, where rather than running the lane prediction algorithm for every region, processing is only performed for relevant regions in the grid [37]. In Figure 11, we show a similar pattern in our implementation where processing across all regions leads latency to exceed the 82 ms pipelining latency. Around 60% computing satisfies the latency constraint. \nVI Related Works\n \nVI-A Multi-chiplet Modules\n Accelerating AI workloads through a chiplets system involves combining on package a host system with an acceleration platform which can be based on GPU [38, 39], FPGA [40], or NPUs [10, 15, 11].\nMore sophisticated architectures can entail the acceleration platform comprising multiple smaller modules consolidated on package to form a multi-chiplet module (MCM) [38, 10, 13, 15, 11, 12]. \nVI-B Autonomous Driving Systems Efficiency\n The works in\n[17, 26, 41, 42] study how distributed scheduling of ADS perception tasks across multiple platforms elevates efficiency.\nOther works in [24, 23] explore how different hardware architectures and platforms (GPUs, FPGAs, ASICs) influence efficiency in the autonomous driving SoC. \nVII Conclusion\n We have studied the performance implications from adopting chiplet-based NPUs to accelerate perception AI workloads through the Tesla Autopilot use case. Our findings demonstrate that the combination of throughput matching algorithm and heterogeneous integration offer desirable performance trade-offs from the multi-chiplet module despite the added NoP costs, motivating further studies on adopting forthcoming chiplet-based NPU architectures in the automotive setting. References"}
{"text": "Anda: Unlocking Efficient LLM Inference with a Variable-Length Grouped Activation Data Format The widely-used, weight-only quantized large language models (LLMs), which leverage low-bit integer (INT) weights and retain floating-point (FP) activations, reduce storage requirements while maintaining accuracy. However, this shifts the energy and latency bottlenecks towards the FP activations that are associated with costly memory accesses and computations. Existing LLM accelerators focus primarily on computation optimizations, overlooking the potential of jointly optimizing FP computations and data movement, particularly for the dominant FP-INT GeMM operations in LLM inference. To address these challenges,\nwe investigate the sensitivity of activation precision across various LLM modules and its impact on overall model accuracy. Based on our findings, we first propose the Anda data type: an adaptive data format with group-shared exponent bits and dynamic mantissa bit allocation. Secondly, we develop an iterative post-training adaptive precision search algorithm that optimizes the bit-width for different LLM modules to balance model accuracy, energy efficiency, and inference speed. Lastly, a suite of hardware optimization techniques is proposed to\nmaximally exploit\nthe benefits of the Anda format.\nThese include a bit-plane-based data organization scheme, Anda-enhanced processing units with bit-serial computation, and a runtime bit-plane Anda compressor to simultaneously optimize storage, computation, and memory footprints.\nOur evaluations on FP-INT GeMM operations show that Anda achieves a 2.4\u00d7\\times\u00d7 speedup, 4.0\u00d7\\times\u00d7 area efficiency, and 3.1\u00d7\\times\u00d7 energy efficiency improvement on average for popular LLMs including OPT, LLaMA, and LLaMA-2 series over the GPU-like FP-FP baseline.\nAnda demonstrates strong adaptability across various application scenarios, accuracy requirements, and system performance, enabling efficient LLM inference across a wide range of deployment scenarios. \nI Introduction\n Large language models (LLMs) [11, 86, 72, 70, 63] have demonstrated remarkable proficiency in a wide array of natural language processing tasks, including text generation, question answering, and automatic summarization.\nThe extraordinary success of LLMs can be attributed to the scaling law [3], which posits that performance improves dramatically with\nthe ever-increased model size, training data volume, and computational resources.\nThe evolution of the GPT series [63, 38, 5] on model size strikingly illustrates the scaling law: while GPT-1 comprised a modest 117 million parameters, its successor GPT-4 is speculated to encompass over a trillion parameters.\nHowever, the exponential growth in LLM model sizes has created substantial deployment challenges, imposing enormous demands on storage and computational resources. To address these challenges, quantization techniques [12, 52, 16, 51, 66, 24, 78] have been widely adopted in LLMs to reduce memory footprint and lower deployment costs.\nTo maximally shrink the model size, the most common strategy for LLMs today is weight-only quantization\u00a0[64, 17, 8, 51, 66, 24, 47, 34, 35, 78, 81], which aggressively lowers the precision of the weights while maintaining high precision for activations due to the presence of outliers [16, 77].\nIn particular, the widely adopted W4A16 scheme\u00a0[66, 24], which quantizes weights into 4-bit integers (INT4) while retaining activations in 16-bit floating-point format (FP16), significantly reduces memory requirements and bandwidth, lowering GPU memory usage by nearly 4\u00d7\\times\u00d7\u00a0[83] and facilitating deployment to smaller devices\u00a0[51, 62]. With the increasing importance of weight-only quantization, FP-INT GeMM operations\u00a0[32] have become indispensable in LLM inference.\nAs illustrated in Fig.\u00a02, FP-INT GeMMs constitute a significant portion of the computational workload across various weight-only quantized LLMs and context lengths in text generation tasks. They dominate in typical applications with sequences under 4K tokens, comprising over 90% of operations on average, and remain substantial even for context lengths exceeding 10K tokens in applications from LongBench [4].\nSuch prevalence highlights the urgent need to optimize FP-INT GeMMs for efficient LLM inference. NVIDIA\u2019s new FP-INT GeMM kernel\u00a0[62], as well as some specialized GPU kernels\u00a0[35, 78, 24, 57], are a consequence of this evolution.\nHowever, these optimized GPU kernels still rely on using FP units by converting the INT weights to FP values for execution in FP GeMM operators\u00a0[52].\nEfforts have been made to develop dedicated FP-INT arithmetic units\u00a0[32], while the additional costs of exponent alignment and normalization persist, resulting in complicated hardware implementation.\nTo reduce hardware cost,\nanother optimization approach is to convert the FP activations to a block floating point (BFP) data format for computation\u00a0[44, 19, 13].\nSince grouped BFP elements share an exponent, the overhead of exponent alignment and normalization within a group disappears, simplifying the operation to INT\narithmetic.\nHowever, to mitigate accuracy loss when converting FP activations to BFP format on a pre-trained network, costly retraining [12, 14, 13, 39, 85, 44, 61, 26] is required, hindering agile LLM deployment. Alternatively, accuracy can be preserved by using a large mantissa field conversion\u00a0[32, 42], but this significantly increases the energy consumption due to computational and memory access overhead of the additional bits. In summary, processing\nFP activations remains a major bottleneck in weight-only quantized LLM inference, and existing methods struggle to balance model accuracy, computational efficiency, and energy consumption.\n\nTo overcome the above limitations, we propose Anda to unlock efficient LLM inference. Anda introduces a novel variable-length grouped activation data format, coupled with\nthe algorithm innovation\nand specialized hardware optimizations.\nAs shown in Fig.\u00a01, Anda first employs a fast, training-free adaptive precision search algorithm during compile time,\nusing the same calibration data as post-training weight-only quantization\u00a0[25].\nGuided by user-defined accuracy constraints, our one-shot process identifies the desired mantissa bit length, which instructs the activation precision across various LLM modules during inference.\nCombining the flexible\nAnda data format with our specialized hardware architecture allows running these dominant FP-INT GeMM operations at lower precision,\nsignificantly improving inference speed and energy efficiency for weight-only quantized LLMs.\nMore concretely, our contributions are as follows: We investigate the potential of BFP activation quantization across popular LLM models within different modules. Based on these insights, we propose Anda: a variable-length grouped data format with shared exponents and adjustable mantissa widths for activations. We develop an adaptive search algorithm to optimize the mantissa widths for different LLM modules without retraining.\nThis algorithm balances model accuracy, energy efficiency, and inference speed based on a user-defined accuracy loss tolerance. We design\nan efficient Anda-aware hardware architecture\nfeaturing (a) a bit-plane-based data layout scheme in\u00a0memory, (b) Anda-enhanced bit-serial processing units, and (c) a runtime bit-plane data compressor.\nExtensive evaluations of the Anda system across popular LLMs demonstrate an average improvement in processing speed of 2.4\u00d7\\times\u00d7, area efficiency of 4.0\u00d7\\times\u00d7 and energy efficiency of 3.1\u00d7\\times\u00d7 compared to existing SotA hardware. The paper is organized as follows: Sec.\u00a0II reviews the\u00a0benefits and remaining bottlenecks of weight-only quantized\u00a0LLMs and BFP formats, and quantifies the sensitivity of LLM\u00a0inference accuracy to the shared exponents and reduced mantissas sizes. Based on these findings, Sec.\u00a0III features the proposed Anda data format and presents the algorithm to rapidly optimize mantissa length under given accuracy constraints. Next, an Anda-optimized hardware architecture is proposed in Sec.\u00a0IV, which allows us to derive system-level gains in Sec.\u00a0V and benchmark it against the SotA solutions. Sec.\u00a0VII concludes the paper. \nII Background and Motivation\n \nII-A Weight-only Quantized LLMs\n Weight-only quantization\u00a0[64, 17, 8, 51, 66, 24, 47, 34, 35, 78, 81] has emerged as a pivotal technique for efficient LLM inference.\nUnlike weight-activation quantization\u00a0[12, 52, 16, 79, 89], which reduces precision for both weights and activations, weight-only quantization focuses solely on compressing model parameters using a much more aggressive quantization scheme. Fig.\u00a03 illustrates the architecture of a weight-only quantized LLM, composed of a series of Transformer blocks that each contains an attention layer and a feed-forward layer.\nThe light blue background highlights the dominant computational modules involving FP-INT GeMM operations, which can be categorized into four module types based on the positions of the FP activations: the first type involves Aq\u2062k\u2062vsubscript\ud835\udc34\ud835\udc5e\ud835\udc58\ud835\udc63A_{qkv}italic_A start_POSTSUBSCRIPT italic_q italic_k italic_v end_POSTSUBSCRIPT interacting with Wqsubscript\ud835\udc4a\ud835\udc5eW_{q}italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT, Wksubscript\ud835\udc4a\ud835\udc58W_{k}italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, and Wvsubscript\ud835\udc4a\ud835\udc63W_{v}italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT to compute the query (Q\ud835\udc44Qitalic_Q), key (K\ud835\udc3eKitalic_K), value (V\ud835\udc49Vitalic_V) matrices, respectively; the second type involves Aosubscript\ud835\udc34\ud835\udc5cA_{o}italic_A start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT multiplying with Wosubscript\ud835\udc4a\ud835\udc5cW_{o}italic_W start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT to compute the output matrix;\nthe other two types are up-projection and down-projection modules of the feed-forward layer, respectively, involving Ausubscript\ud835\udc34\ud835\udc62A_{u}italic_A start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT and Adsubscript\ud835\udc34\ud835\udc51A_{d}italic_A start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT with interacting to corresponding weights. Weight-only quantized LLMs offer significant advantages in storage efficiency\u00a0[62, 66].\nCompared to W8A8 weight-activation quantized LLMs\u00a0[79], W4A16 weight-only quantized LLMs\u00a0[51] achieve similar model accuracy while reducing storage requirements of model parameters by nearly half\u00a0[83], making them particularly suitable for deployment on resource-constrained devices in edge computing scenarios.\nHowever, under current GPU computing schemes, computing a W4A16 FP-INT operations consumes approximately 1.7\u00d7\\times\u00d7 more energy than W8A8 INT-only operations\u00a0[42].\nThis can be explained by accessing FP activations incurs higher energy costs than INT weights\u00a0[31], and FP-INT operations require complicated hardware implementations\u00a0[32].\nHence, optimizing FP activations emerges as a key opportunity to improve the overall efficiency of weight-only quantized LLMs. \nII-B Block Floating Point\n Reducing the computation and storage overhead of FP16 activations is crucial for optimizing the efficiency of LLMs.\nBFP\u00a0[19] offers a promising solution by sharing exponents within groups of values, preserving dynamic range while mitigating the impact of outliers and simplifying computations.\nThe BFP format can characterized by two key parameters: group size and mantissa length.\nFig.\u00a04 shows the process of converting FP16 tensors to BFP numbers using two different instances of the BFP format.\nInitially, FP16 tensors are divided into groups. Within each group, the largest exponent is selected as the shared exponent and other mantissas are right-shifted based on their exponent differences. Bits exceeding the specified mantissa length are truncated, and zero is represented by all mantissa bits being 0.\nAs illustrated in Fig.\u00a04, this conversion process can lead to precision loss due to mantissa truncation, with some elements becoming zero, thereby posing a significant challenge to maintaining model accuracy. Current approaches to address this fall into two categories.\nOn the one hand, BFP-aware training fine-tunes the model after the quantization\u00a0[12, 14, 13, 39, 85, 44, 61, 26, 41, 23], at the expense of a\ncostly training process, making it rather impractical for agile LLM deployment.\nOn the other hand, direct conversion of pre-trained FP models to BFP formats\u00a0[22, 50, 44, 23, 61] requires long mantissas to avoid the significant accuracy loss, which increases computation and storage overhead, diminishing the advantages of BFP.\nTo avoid the storage of these long mantissas, methods like FIGNA\u00a0[32] and\u00a0[42] propose dynamic conversion to BFP during computation.\nThis approach stores activations in FP16 format and expands to long mantissas with shared exponents before computations to maintain model accuracy.\nHowever, this also prevents FIGNA from obtaining activation memory footprint savings. To avoid both costly retraining and large activation memory footprints, we seek a solution that can rapidly convert FP activations to BFP activations without retraining, while also leveraging the computational and storage advantages of BFP for LLM inference.\nTo achieve this goal, it is necessary to explore opportunities for reduced mantissa length BFP under the unique characteristics of LLMs. \nII-C Opportunities towards Activation Optimizations\n We explore opportunities for LLM activation optimization by investigating the sensitivity of model accuracy to reduced mantissa lengths in BFP formats. This study converts FP-INT GeMM activation tensors (Aq\u2062k\u2062vsubscript\ud835\udc34\ud835\udc5e\ud835\udc58\ud835\udc63A_{qkv}italic_A start_POSTSUBSCRIPT italic_q italic_k italic_v end_POSTSUBSCRIPT, Aosubscript\ud835\udc34\ud835\udc5cA_{o}italic_A start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT, Ausubscript\ud835\udc34\ud835\udc62A_{u}italic_A start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT, Adsubscript\ud835\udc34\ud835\udc51A_{d}italic_A start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT) from FP16 to BFP format, as shown in Fig.\u00a04. Model accuracy is evaluated using perplexity (PPL) on the WikiText2 dataset, with lower PPL indicating higher accuracy. We assume a 1% accuracy loss tolerance in practical scenarios.\nWe aim to uncover efficient activation representations while maintaining LLM performance within acceptable limits. Sensitivity to group size:\nFig.\u00a05 illustrates the sensitivity to shared exponent group size for two different LLM models across various mantissa lengths. The experiments reveal a clear trade-off between group size and the minimum required mantissa length to maintain model accuracy.\nLarger activation group sizes allow more efficient parallel computations, yet at a greater accuracy tolerance or increased mantissa lengths.\nBased on these observations, we select a group size of 64 for subsequent experiments, as it offers a good balance between computational efficiency and accuracy tolerance. Sensitivity to LLM model: With this group size of 64, we continue our exploration across a wider range of recent LLMs, to derive their sensitivity to reduced mantissa lengths.\nFig.\u00a06 reveals varying sensitivities among different models.\nNotably, models such as OPT-2.7B, OPT-6.7B, OPT-13B, and OPT-30B are less sensitive to mantissa reduction, allowing for the direct removal of 5 mantissa bits, while other models could only tolerate the removal of 4 mantissa bits.\nAs more mantissa bits are removed, differences in accuracy sensitivity become more pronounced.\nThis insight inspires us to consider a variable-length BFP datatype, potentially enabling more aggressive compression in less sensitive models while employing a more conservative one for others.\nIt also prompts us to explore whether activations in different modules within one LLM have varying sensitivities. Sensitivity to LLM inner module:\nWe finally explore the impact of different mantissa lengths of the activations of different modules within the same LLM. More specifically, we examine the Aq\u2062k\u2062vsubscript\ud835\udc34\ud835\udc5e\ud835\udc58\ud835\udc63A_{qkv}italic_A start_POSTSUBSCRIPT italic_q italic_k italic_v end_POSTSUBSCRIPT, Aosubscript\ud835\udc34\ud835\udc5cA_{o}italic_A start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT, Ausubscript\ud835\udc34\ud835\udc62A_{u}italic_A start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT, and Adsubscript\ud835\udc34\ud835\udc51A_{d}italic_A start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT modules of the OPT-6.7B, LLaMA-7B, and LLaMA2-7B models. The mantissa length of each module is swept while keeping the lengths of other modules fixed at 13 bits. Fig.\u00a07 summarizes the results, revealing that activations from different modules have varying impacts on model accuracy across all three models. Aq\u2062k\u2062vsubscript\ud835\udc34\ud835\udc5e\ud835\udc58\ud835\udc63A_{qkv}italic_A start_POSTSUBSCRIPT italic_q italic_k italic_v end_POSTSUBSCRIPT consistently shows the most significant influence, while Adsubscript\ud835\udc34\ud835\udc51A_{d}italic_A start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT demonstrates low sensitivity in OPT-6.7B but has a more pronounced effect in the LLaMA series models. Our study reveals several key insights into the application of BFP in LLMs:\n(a) LLMs can maintain good performance with reduced mantissa lengths.\n(b) Different LLM models exhibit varying sensitivities to mantissa reduction.\n(c) Within a single LLM, different modules have distinct sensitivities to precision reduction.\nThese observations motivate us to introduce the new variable-length grouped data format for FP activations, along with a methodology for post-training quantization (PTQ) and rapid selection of tolerable reduced mantissa lengths for any LLM. \nIII Anda Data Format\n In this section, we present unique features of the Anda data format and demonstrate\nits benefits towards FP-INT operations\nin weight-only quantized LLM inference. Furthermore, we introduce\na mantissa bit-width search method\nto efficiently identify the optimized Anda precision combinations that satisfy a user-defined accuracy drop. \nIII-A Anda Format Features\n Based on the findings of our previous study, we propose the Anda format: an innovative variable-length mantissa BFP scheme designed for efficient LLM inference.\nAnda\u2019s structure comprises a sign bit, a shared exponent, and a variable-length mantissa, building upon traditional BFP conversion processes as previously shown in Fig.\u00a04.\nIts key feature is the ability to dynamically select mantissa lengths for different tensors based on their precision sensitivity, maintaining consistency within each tensor while optimizing the accuracy-efficiency trade-off. Table\u00a0I compares Anda with prior BFP formats, categorizing them based on supported mantissa lengths.\nUni-length formats, such as VS-Quant\u00a0[12] and FIGNA\u00a0[32], use fixed mantissa lengths, while multi-length formats like FAST\u00a0[85] and DaCapo\u00a0[41] offer limited flexibility with 2\u223csimilar-to\\sim\u223c3 predefined lengths. Anda surpasses both by providing a continuous range of mantissa lengths, allowing fine-grained precision control across different LLM modules.\nEnabled by specialized hardware units, as detailed in Sec.\u00a0IV, smaller mantissa widths result in a lower inference latency, computational cost and memory storage cost. This allows Anda format to carefully balance model precision and computational efficiency, providing a more aggressive compression in less sensitive model\nparts while preserving critical precision elsewhere.\n \nIII-B Efficient FP-INT GeMM Using Anda Format\n We then compare the workflows of GeMM workloads of several SotA approaches to illustrate the advantages of replacing FP16 activations with the Anda data format. Taking the W4A16 quantization scheme as an example, we examine the FP-INT GeMM computation process (a) on existing GPU platforms\u00a0[52]; (b) on GPU platforms with dedicated FP-INT processing units; (c) using FIGNA\u2019s dynamic conversion scheme\u00a0[32]; and (d) with our proposed Anda approach. Fig.\u00a08 depicts the four schemes, with colors indicating the data types used throughout the computational process. Fig.\u00a08(a) shows the workflow of W4A16 LLMs on common GPU platforms.\nThe absence of dedicated FP-INT computation units in GPU necessitates converting INT4 weights to FP16 before processing, with tensor cores operating in FP16 mode.\nThis scheme not only brings additional format conversion overheads, but requires costly FP computations. GPU platforms equipped with dedicated FP-INT processing units, as illustrated in Fig.\u00a08(b), can eliminate the need for converting INT4 weights to FP16, thereby reducing data conversion overheads and computation costs.\nHowever, as pointed out by FIGNA\u00a0[32], the high alignment and normalization overhead associated with FP-INT processing units still results in high computational expenses. To efficiently deploy W4A16 LLMs, FIGNA proposes a computation scheme using a BFP variant with corresponding hardware support to overcome the issues with dedicated FP-INT units.\nAs depicted in Fig.\u00a08(c), activations are stored in FP16 format in memory, converted to the FIGNA format before computation, after which a 14-bit mantissa is multiplied with INT4 weights for GeMM computation. The final results are then converted again to FP16 and written back to memory.\nThis scheme reduces the computation overhead by converting costly FP GeMM to INT operations.\nHowever, since FP16 activations need to be repeatedly accessed during computation, frequent data conversion from FP16 to FIGNA introduces additional overhead, affecting overall efficiency. As presented in Fig.\u00a08(d), our proposed Anda format computation scheme offers some unique advantages in contrast with the previous approaches.\nFirstly, the activations are no longer stored in memory in FP16 format, but directly in the Anda data format, reducing storage overhead and data access overhead while avoiding frequent data conversion.\nSecondly, the shared exponent enables INT dot-product operations within a group, followed by FP32 accumulation across groups, reducing the computational overhead of FP-INT GeMMs.\nThirdly, the variable-length mantissa considerably decreases dot-product operations and memory accesses use the minimal necessary word length.\nFinally, converting only the final FP32 results back to Anda format before writing to memory minimizes the storage requirement and the additional overhead from switching data format. \nIII-C Adaptive Precision Combination Search\n To leverage the Anda format for fast deployment and hardware performance gains, we propose an adaptive precision search algorithm for offline compile-time optimization of activation precisions in weight-only quantized LLMs.\nOur algorithm is built around two key strategies.\n(a) We narrow the search space to the precision of only four key tensor types\n,i.e., Aq\u2062k\u2062vsubscript\ud835\udc34\ud835\udc5e\ud835\udc58\ud835\udc63A_{qkv}italic_A start_POSTSUBSCRIPT italic_q italic_k italic_v end_POSTSUBSCRIPT, Aosubscript\ud835\udc34\ud835\udc5cA_{o}italic_A start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT, Ausubscript\ud835\udc34\ud835\udc62A_{u}italic_A start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT, and Adsubscript\ud835\udc34\ud835\udc51A_{d}italic_A start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT,\nbased on their sensitivity to model accuracy as demonstrated in Fig.\u00a07. This precision combination is represented as a 4-tuple [Mq\u2062k\u2062v,Mo,Mu,Md]subscript\ud835\udc40\ud835\udc5e\ud835\udc58\ud835\udc63subscript\ud835\udc40\ud835\udc5csubscript\ud835\udc40\ud835\udc62subscript\ud835\udc40\ud835\udc51[M_{qkv},M_{o},M_{u},M_{d}][ italic_M start_POSTSUBSCRIPT italic_q italic_k italic_v end_POSTSUBSCRIPT , italic_M start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT , italic_M start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT , italic_M start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ].\n(b) We employ a training-free, one-shot calibration process reusing the small amount of calibration data from the post-training weight-only quantization process, being several thousands of tokens with hundred batches\u00a0[24, 51, 66].\nThough prior layer-wise methods\u00a0[28, 76, 18] may achieve finer precision adjustments, their prolonged search times significantly extend the deployment process.\nIn contrast, our module-wise approach rapidly assigns mantissa lengths while maintaining consistency across layers and can easily be integrated into standard post-training deployment workflows.\n As outlined in Algorithm\u00a01, we take the LLM model\u00a0L\ud835\udc3fLitalic_L, a calibration dataset D\ud835\udc37Ditalic_D, an accuracy loss tolerance \u03b4\ud835\udeff\\deltaitalic_\u03b4, and a maximum number of iterations N\ud835\udc41Nitalic_N as inputs.\nThe accuracy tolerance \u03b4\ud835\udeff\\deltaitalic_\u03b4 specifies the acceptable level of performance degradation, while the maximum number of iterations N\ud835\udc41Nitalic_N serves as a termination criterion, ensuring the algorithm concludes within a reasonable time frame. With these inputs, our algorithm finds the optimal 4-tuple precision combination within the given iterations that best balances model accuracy and inference efficiency across the model\u2019s key activation components.\nThe search process consists of three key steps. Step 1: Initialize search starting points. A priority\u00a0queue with precision combinations of equal precision across all modules is initialized first. These precision combinations range from aggressive (e.g., [4,4,4,4]4444[4,4,4,4][ 4 , 4 , 4 , 4 ]) to conservative (e.g., [13,13,13,13]13131313[13,13,13,13][ 13 , 13 , 13 , 13 ]). This strategy enables the rapid discovery of efficient combinations while ensuring the existence of feasible solutions, as validated by our prior experiments in Fig.\u00a06. Step 2: Check the promising combination.\nIn each iteration, the combination with the lowest bit operations (BOPs) is extracted from the priority queue and added to the visited set.\nThe BOP metric\u00a0[1, 43, 49, 71] quickly estimates computational cost by calculating the total number of bit operations for the necessary multiplications under a given combination.\nThis allows us to efficiently prioritize promising combinations without a full model evaluation.\nThe accuracy of the promising combination is then examined on the calibration dataset. Step 3: Update and relax the best combination. If the evaluated combination yields lower BOPs than the current best while maintaining accuracy within the specified tolerance, it becomes the new best combination.\nTo generate nearby precision candidates, the algorithm then relaxes this best combination by decreasing the mantissa length of each tensor type by one, while keeping the other tensor types unchanged.\nFor example, if the current best combination is [6,7,5,5]6755[6,7,5,5][ 6 , 7 , 5 , 5 ], the generated candidates will be [5,7,5,5]5755[5,7,5,5][ 5 , 7 , 5 , 5 ], [6,6,5,5]6655[6,6,5,5][ 6 , 6 , 5 , 5 ], [6,7,4,5]6745[6,7,4,5][ 6 , 7 , 4 , 5 ], and [6,7,5,4]6754[6,7,5,4][ 6 , 7 , 5 , 4 ].\nThe generated candidates that have not been visited before are added to the priority queue.\nIf the accuracy constraint is not met, no update is made.\nStep 2 and 3 are repeated until the maximum number of iterations is reached or the search space is exhausted. \nIII-D Precision Combination Search Efficiency\n Our algorithm aims to efficiently optimize FP activations in weight-only quantized LLMs during the post-training phase. Most weight-only quantization processes\u00a0[51, 24, 66] rely on a small calibration dataset, which we can reuse in the activation precision search. Ensuring a rapid search process is critical to avoid extending post-training deployment time. Therefore, the algorithm is designed to find a near-optimal solution quickly, within an acceptable accuracy tolerance, to enable efficient hardware deployment. The efficiency of our algorithm is enhanced by two key mechanisms: First, we introduce a constraint that updates the best combination only when a new precision combination offers a lower computational cost, employing a relaxation strategy similar to gradient descent to accelerate convergence. While this may miss the global optimum, it ensures a high-performance combination within limited iterations. Second, we set an iteration limit to complete the search within a reasonable timeframe, avoiding deployment delays. It is here important to note that the relatively limited search space of only 4 precision variables allows for fast convergence with just a few iterations.\nThe execution time of each iteration is roughly the time of a forward pass over the calibration dataset to validate the precision combination. To demonstrate our algorithm\u2019s search efficiency, we compare it with the conventional brute-force approaches\u00a0[12, 13, 14] on the OPT-125M model.\nAs shown in Fig.\u00a09, the search space for OPT-125M contains over 10,000 possible combinations, and our algorithm identifies the precision combination [7,7,6,5]7765[7,7,6,5][ 7 , 7 , 6 , 5 ] in just 10 iterations, maintaining accuracy within 1% loss.\nIn practice, we limit the search to 32 iterations, ensuring that time overhead remains minimal while achieving a near-optimal precision combination. By avoiding time-consuming backward propagation or complex solving processes, our algorithm operates approximately twice as fast as Omniquant\u00a0[66] and ten times faster than GPTQ\u00a0[24], the current SoTA methods for post-training weight-only LLM quantization. \nIV Anda Architecture\n In this section, we first present the three key components of the Anda architecture: (a) A variable-length activation data layout in on-chip memory storage, (b) an Anda-enhanced bit-serial processing unit, and (c) a runtime bit-plane compressor for output activations.\nThese components collectively enhance storage efficiency, computational performance, and energy conservation.\nFinally, we present how these components integrate to form the overall Anda architecture, a computing system optimizing LLM inference using the Anda format. \nIV-A Bit-plane Data Layout Scheme\n Anda-based activation values feature a variable-length mantissa, necessitating careful data layout arrangement in the on-chip buffer to maintain regular memory access. Otherwise, irregular memory accesses caused by an ineffective data layout could completely undo the benefits provided by Anda. To tackle these challenges, we propose the bit-plane data layout scheme as illustrated in Fig.\u00a010.\nUnlike prior fixed-length data arrangement methods\u00a0[61, 41, 67, 30], which treat each FP data element as an atomic\nunit,\nour approach separates and reorganizes the sign bit, mantissa, and exponent of FP numbers within grouped data blocks from a bit-plane view.\nA transposed data arrangement\u00a0[48] is introduced where bits of the same significance across multiple numbers are packed together to keep the regularity of memory access.\nTaking the common memory bank word width into account, 64 Anda-type values are grouped to implement the bit-plane data layout scheme.\nAs shown in Fig.\u00a010, Group #0 shows the layout for 4-bit mantissa Anda numbers, while Group #1 presents the arrangement for 5-bit mantissa Anda numbers.\nThe variable mantissa length only reflects on the different memory address depths, without impacting memory bandwidth utilization, and can be easily managed during address generation.\nHence, in both cases, the bit-plane data layout efficiently accommodates these formats with varying lengths, maintaining consistent access patterns.\nFurthermore, the bit-plane organization inherently facilitates parallel processing, inspiring the design of a novel processing unit for the Anda data format to enhance LLM inference in both computing and energy efficiency. \nIV-B Anda-enhanced Bit-serial Processing Unit\n The Anda-enhanced bit-serial processing unit (APU), as depicted in Fig.\u00a011, serves as the key computational element of the Anda architecture, embracing Anda processing element (PE) and an FP accumulator. Anda PE efficiently executes dot-product operations between variable-length Anda format activations and INT weights, seamlessly integrating with the bit-plane data layout scheme to enhance performance.\nThe FP accumulator follows the PE to complete the APU functionality by accumulating the cross-group dot-product results. The computation process begins with the Anda PE storing the sign and exponent in internal registers.\nConcurrently, the INT weights are stored in the PE using a double-buffer design, allowing overlapped weight loading and computation to minimize loading latency.\nThe PE then loads the bit-plane mantissas and performs computations with the INT weights. By employing bit-serial processing of mantissas, the Anda PE can adapt to Anda format data of varying lengths without additional hardware overhead. To further optimize hardware efficiency, the Anda PE implements a first-element-then-bit-plane reduction pattern. In this approach, a partial sum is obtained for each bit-plane by accumulating all elements within that bit-plane using an adder tree. This method reduces storage requirements by storing only one partial sum per bit-plane instead of all intermediate results. It also minimizes data movement and processing overhead by performing subsequent shift operations only on the single partial sum rather than individual elements. Furthermore, it significantly reduces hardware resource consumption by using a single shared accumulator for all bit-plane accumulations. The bit-plane partial sums are then sequentially accumulated to complete the dot-product operation. Upon completion, the Anda PE dynamically shifts the dot-product result based on the Anda mantissa length and converts it to FP16 using the shared exponent. The result is then multiplied with the group-wise scale factor of the INT weights, followed by cross-group accumulation using the FP accumulator. Finally, the accumulated FP32 result is converted to FP16 for output. \nIV-C On-the-fly Bit-plane Compressor\n The bit-plane compressor (BPC) is a critical component of the Anda architecture, enabling on-the-fly conversion of FP16 activation values into the compressed Anda format. It efficiently addresses the challenges of variable-length Anda activation storage and transfer in LLM inference by processing a large number of activation values in parallel and outputting them in a bit-serial manner. Fig.\u00a012 illustrates the architecture of the proposed BPC.\nIt consists of 16 parallel lanes, each capable of processing 64 grouped FP16 values simultaneously. Within each lane, the FP field extractor decomposes the FP16 inputs into their sign, exponent, and mantissa components.\nThe maximum exponent catcher identifies the maximum exponent within a grouped lane, and then calculates the difference of each exponent to the shared maximum exponent. The core of the compression process lies in the mantissa alignment performed by the parallel-to-serial mantissa aligner.\nAs shown in Fig.\u00a012, in each cycle, each element\u2019s exponent difference decreases by one until it reaches zero. When the exponent difference is zero, the most significant mantissa bit of that element should be shifted out each cycle; otherwise, it remains unchanged and output zero.\nThe shifted-out bits among each element in the lane are packed into the bit-plane aligned mantissa.\nThis process continues for multiple cycles until the number of output bit-planes reaches the configurable mantissa length.\nThe parallel-to-serial mantissa alignment process generates compressed bit-planes directly. The resulting bit-serial output, along with the sign bits and shared maximum exponents from all lanes, is passed to the data packager unit. This unit assembles the final compressed output in a format compatible with the proposed bit-plane compression scheme.\nThe proposed bit-serial mantissa aligner is more area-efficient compared to existing bit-parallel aligners\u00a0[85, 32], requiring only a comparator and shifter. In contrast, bit-parallel designs need multiple shifters and comparators for single-cycle dynamic shifting\u00a0[15]. While our bit-serial aligner introduces some latency, it can largely overlap with APU computations, with little impact on overall system performance. \nIV-D Overall Architecture\n Fig.\u00a013 illustrates the overall architecture of Anda, which includes the top controller, address generator, activation buffer, weight buffer, matrix computation unit (MXU), vector unit, and bit-plane compressor.\nThe LLM inference is orchestrated as follows: \u2776 Initially, the instruction memory is programmed through the I/O interface of the top controller, which governs the address generator during operation.\n\u2777 The address generator produces read and write addresses for both activation and weight buffers. Both the activation buffer and weight buffer follow the proposed bit-plane-based data layout for efficient data handling.\n\u2778 The MXU, featuring a 16\u00d7\\times\u00d716 APU array, performs FP-INT GeMM operations following typical output stationary dataflow\u00a0[45].\nThe weight data dispatcher, equipped with registers, allows overlapping weight loading and computation, broadcasting weights row-wise to each APU for data reuse.\nThe activation data dispatcher supplies a bit-plane vector of activations each cycle, sequentially feeding it into the MXU and sharing it across columns to maximize input reuse and enable multiple calculations with the same input.\nUpon completing the GeMM computation, the output results are delivered to the BPC via the output data dispatcher.\n\u2779\u00a0Complementing MXU, the vector unit processes the non-linear functions of the transformer block.\n\u277a\u00a0FP16 outputs of MXU or vector unit can be optionally compressed to Anda format by the BPC, optimizing storage efficiency.\n\u277b\u00a0Processed outputs are written back to the activation buffer.\n\u277c\u00a0Finally, activation results are transferred to external memory for subsequent operations. For fair comparison in the PTQ scenario, we directly apply VS-Quant\u2019s 4-bit data format without the costly retraining typically required by this method. \nV Evaluation\n \nV-A Experimental Setup\n LLM Benchmarks: To demonstrate the wide applicability of our proposed method, we benchmark various open-source LLMs using PyTorch and Hugging Face libraries.\nWe evaluate their performance on WikiText-2\u00a0[60], Penn Treebank (PTB)\u00a0[59], and C4\u00a0[65] datasets. The models used in our benchmarks range in size from 1.3B to 30B parameters and are selected from OPT\u00a0[86], LLaMA\u00a0[72], and LLaMA2\u00a0[73] families, enabling the effectiveness assessment of our method across different model scales and architectures. Quantization Baselines:\nTo validate the model accuracy when replacing FP activations with the proposed Anda format, we compare against the following SotA competitors:\n(a) Full-precision baseline where both activations and weights are represented in FP16.\n(b) Weight-only PTQ baseline using Omniquant\u00a0[66] with W4A16g128 scheme, which uses 4-bit weight quantization with a group size of 128.\n(c) Lossless BFP baseline that adopts FIGNA\u2019s\u00a0[32] approach of using extended mantissa lengths to maintain model accuracy.\n(d) Aggressive BFP baseline that employs 4-bit mantissa to activations using VS-Quant\u00a0[12] quantization scheme.\nFor fair comparison in the PTQ scenario, we directly apply VS-Quant\u2019s 4-bit data format without the typically required costly retraining.\nFor baseline (c), (d), and our Anda method, we use baseline (b) as the starting point, and the group size of BFP activations is uniformly set to 64. Hardware Baselines: To further highlight the advantages of the Anda scheme, we compare the Anda hardware\narchitecture against several SotA platforms:\n(a) FP-FP: A spatial accelerator based on FP16 Tensor Cores\u00a0[84], representative of current GPU architectures\u00a0[69].\n(b) FP-INT: An enhanced Tensor Core-based accelerator featuring specialized FP-INT computation units.\n(c) iFPU\u00a0[42]: A spatial accelerator employs bit-serial computation for INT weights and dynamically converts FP activations to BFP format with extended mantissa before computation.\n(d) FIGNA\u00a0[32]: An evolution of (c) leveraging bit-parallel computation and optimized mantissa length in BFP to enhance computational efficiency.\nAll systems are configured to have the same operating clock frequency of 285 MHz, equivalent peak throughput, and on-chip memory resources to ensure a fair comparison\u00a0[68]. Evaluation Methodologies:\nOur comprehensive assessment encompasses both model accuracy and hardware efficiency, where we focus on optimizing the dominated FP-INT GeMM operations, keeping the others, e.g., KV cache\u00a0[56], in FP16.\nFor model accuracy, we employ three key metrics: (a) perplexity (PPL)\u00a0[33] using a 2048 sequence length, where lower values indicate higher model accuracy; (b) relative accuracy loss of FIGNA, VS-Quant, and our Anda method compared to Omniquant, which demonstrates the accuracy drop during BFP conversion in the deployment process; and (c) bit-level operations (BOPs)\u00a0[1] reduction, quantifying the decrease in theoretical model computation.\nHere we consider one FP16-INT4 operation equivalent to 64 BOPs, as this approximates the bit-level computational complexity of an FP16-INT4 multiply-accumulate operation.\nHardware performance evaluation spans two levels: at the PE level, we analyze area and power along with corresponding efficiencies; at the system level, we compare Anda with baseline architectures, focusing on speedup and energy efficiency, when performing FP-INT GeMMs where the FP activations can be drop-in replaced by Anda data format.\nIn alignment with prior studies\u00a0[27, 79, 32, 42], system-level evaluation uses LLMs with a batch size of 1 and the maximum acceptable input sequence length under the WikiText2 dataset.\nPE-level baselines and Anda system are implemented in SystemVerilog RTL and synthesized using Cadence Genus\u00a0[6] at 16nm technology node, operating at a clock frequency of 285 MHz and 0.8 V nominal voltage.\nPower evaluation is performed using value change dump (VCD) files generated from synthesized netlist simulations and analyzed by Genus.\nA cycle-accurate simulator, rigorously verified against functional simulations, assesses the energy and performance of Anda and baseline accelerators.\nHBM2 memory is modeled with an access energy of 3.9 pJ/bit and a bandwidth of 256 GB/s\u00a0[36]. \nV-B Inference Accuracy\n We evaluate inference accuracy on validation datasets using the\nAnda format explored by adaptive precision search algorithm across all benchmarks. For each benchmark, 128 random sequences of length 2048 are sampled from the training dataset for calibration\u00a0[66]. We also limit the adaptive precision search algorithm to 32 iterations.\nNote that Anda is capable of adapting mantissa bits according to the user-defined accuracy tolerance. Therefore, we report results for two accuracy constraints: 0.1% representing minimal loss and 1% representing acceptable loss for most scenarios. Table\u00a0II compares Anda\u2019s performance against baseline quantization methods, where PPL values are shown in black, the relative accuracy drop is displayed in red, and the BOPs reduction is presented in green.\nNote that the occasional slight exceedance of the validation accuracy loss over the constraint is normal for Anda due to differences between the calibration and validation datasets.\nThe results demonstrate that Anda achieves significant BOPs reductions while maintaining accuracy close to the target constraints. For instance, on the WikiText2 dataset, Anda achieves 1.80\u223csimilar-to\\sim\u223c3.10\u00d7\\times\u00d7 and 2.44\u223csimilar-to\\sim\u223c3.31\u00d7\\times\u00d7 BOPs reduction under 0.1% and 1% accuracy loss, respectively, flexibly meeting varying accuracy-efficiency requirements.\nCompared to FIGNA, which yields a 1.23\u00d7\\times\u00d7 BOPs reduction with shorter bit-width multiplication, Anda further achieves 1.46\u223csimilar-to\\sim\u223c2.69\u00d7\\times\u00d7 BOPs reductions at similar accuracy loss levels by leveraging different mantissa lengths for activation tensors In contrast to VS-Quant, a BFP method requiring retraining, directly deploying it leads to severe accuracy degradation despite achieving 4\u00d7\\times\u00d7 BOPs reduction. For example, VS-Quant suffers a 27.96% accuracy loss on OPT-1.3B with WikiText2. Anda, however, achieves a much better accuracy-efficiency balance, obtaining nearly 3\u00d7\\times\u00d7 BOPs reduction with only a 0.74% accuracy loss in the same scenario.\nAnda\u2019s consistent performance improvements across various models and datasets showcase the effectiveness and robust generalization of our adaptive precision search algorithm. Fig.\u00a014 presents the best precision combination driven by 0.1% and 1% relative accuracy loss, revealing the patterns of quantization precision choices for different activation parts across models. The\nprecision combinations vary under different accuracy constraints, showing the importance of the adaptive precision combination search algorithm in automatically adjusting the mantissa lengths based on LLM module sensitivity.\nExamining the module types reveals that Aq\u2062k\u2062vsubscript\ud835\udc34\ud835\udc5e\ud835\udc58\ud835\udc63A_{qkv}italic_A start_POSTSUBSCRIPT italic_q italic_k italic_v end_POSTSUBSCRIPT, involved with the Q\ud835\udc44Qitalic_Q, K\ud835\udc3eKitalic_K, and V\ud835\udc49Vitalic_V projection layers, prefers higher precision due to higher sensitivity, while Ausubscript\ud835\udc34\ud835\udc62A_{u}italic_A start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT and Adsubscript\ud835\udc34\ud835\udc51A_{d}italic_A start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT in the feed-forward layers, especially Adsubscript\ud835\udc34\ud835\udc51A_{d}italic_A start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT, can be more aggressively quantized to lower precision, demonstrating higher tolerance. \nV-C PE-level Evaluation\n We quantitatively compare the proposed Anda PE with common FP-FP units\u00a0[58], enhanced FP-INT units, and dedicated PE units from iFPU\u00a0[42] and FIGNA\u00a0[32], respectively.\nWe also introduce FIGNA-M11 and FIGNA-M8 as baselines, representing bit-parallel PEs with 11-bit and 8-bit mantissas that achieve 0.1% and 1% accuracy degradation targets, respectively, based on the results from Fig.\u00a014. Here, M(x) denotes the number of preserved mantissa bits.\nTo ensure an equitable evaluation, all PEs are configured with equal computational throughput per cycle. We process an identical dot product workload across different PEs to measure area efficiency (TOPS/mm2) and energy efficiency (TOPS/W). Fig.\u00a015 (a) and (b) show the area and power consumption of Anda and baseline PEs.\nAnda presents significant reductions, consuming less than 60% of the power and area compared to FP-FP and FP-INT PEs.\nThis is primarily due to shared exponents, which eliminate complex alignment and normalization processes.\nCompared to iFPU\u00a0[42], Anda offers 12% and 29% reductions in area and power, respectively, by avoiding high-overhead ultra-wide multipliers and registers needed for maintaining FP16 precision.\nWhile Anda incurs a 27% power and 18% area overhead compared to FIGNA due to its bit-serial structure, its adaptive precision capability can significantly reduce execution time, leading to higher efficiency.\nFig.\u00a015 (c) and (d) further exhibit superior area and energy efficiency of Anda PE with variable-length mantissas.\nRefering back to Fig.\u00a014, the retained mantissa lengths of Anda typically range between 4\u223csimilar-to\\sim\u223c8 bits with negligible 1% accuracy impact, resulting in the area and energy efficiency improvements of 1.38\u223csimilar-to\\sim\u223c2.48\u00d7\\times\u00d7 and 1.52\u223csimilar-to\\sim\u223c2.74\u00d7\\times\u00d7 over FIGNA, respectively.\nMoreover, comparing FIGNA and Anda at fixed mantissa lengths, Anda introduces some control logic overhead due to its bit-serial design.\nAt 11 bits, Anda has 12% and 17% lower area and energy efficiency against FIGNA-M11; at 8 bits, it\u2019s 5% and 15% lower against FIGNA-M8. However, Anda\u2019s ability to dynamically adjust mantissa lengths based on model accuracy requirements allows it to potentially achieve higher utilization at the system level, which will be analyzed in the next subsection. \nV-D System-level Evaluation\n Fig.\u00a016 compares system-level speedup, area efficiency, and energy efficiency between Anda and several baselines across various LLM models.\nWe also introduce bit-parallel FIGNA-M11 and FIGNA-M8 as baselines for 0.1% and 1% accuracy loss. Anda enables precision-scalable inference within a single hardware architecture, in contrast to FIGNA\u2019s separate implementations for each precision level. Speedup:\nAnda, utilizing the precision combinations identified in Fig.\u00a014, implements scalable computation and achieves 2.14\u00d7\\times\u00d7 and 2.49\u00d7\\times\u00d7 speedups on average over the GPU-like FP-FP baseline at 0.1% and 1% accuracy loss, respectively.\nCompared to the corresponding FIGNA variants, Anda achieves 1.48\u00d7\\times\u00d7 and 1.25\u00d7\\times\u00d7 higher acceleration, benefiting from efficient utilization of varied mantissa precisions across tensor types. Area Efficiency:\nAnda improves area efficiency by 3.47\u00d7\\times\u00d7 and 4.03\u00d7\\times\u00d7 over the GPU-like FP-FP baseline at 0.1% and 1% loss, respectively, due to two factors: (a) shared exponent design simplifies alignment operations, improving computational unit efficiency; (b) bit-serial design fully utilizes mantissa widths of different tensor types.\nNotably, in 1% loss with LLaMA models, FIGNA-M8\u2019s area efficiency rivals or slightly exceeds Anda due to its alignment with 8-bit precision, where bit-parallel designs excel. However, Anda\u2019s scalable computation outperforms FIGNA by adopting more aggressive bit-widths in OPT models. Energy Efficiency:\nAnda achieves a 3.07\u00d7\\times\u00d7 improvement over the GPU-like FP-FP baseline at 0.1% accuracy loss, increasing to 3.16\u00d7\\times\u00d7 at 1% loss tolerance.\nUnlike iFPU\u00a0[42] and FIGNA\u00a0[32], which solely optimize energy during computation, Anda\u2019s bit-serial architecture skips redundant mantissa bit calculations to improve computational utilization, and the BPC compresses output, reducing memory access. FIGNA-M11 and FIGNA-M8 use reduced mantissa bit-parallel designs to improve computational efficiency, but rely on FP16 storage, leading to frequent data conversions, which offsets energy gains.\nFig.\u00a017 further presents that compared to the GPU-like FP-FP baseline on the LLaMA-13B model, Anda reduces energy consumption by 90%, 54%, and 50% for computation, SRAM, and DRAM access, respectively.\nWhile FIGNA achieves similar compute efficiency, Anda\u2019s architecture avoids redundant computations and FP-to-BFP conversion, reducing energy further. Moreover, Anda\u2019s bit-plane storage scheme and BPC compression reduce memory access overhead, improving SRAM and DRAM energy efficiency by 2.2\u00d7\\times\u00d7 and 2.0\u00d7\\times\u00d7 compared to FIGNA. \nV-E Power and Area Breakdown\n We conduct a detailed hardware analysis of the Anda architecture for LLaMA-13B inference within 1% accuracy loss. Table\u00a0III presents the area breakdown and power distribution.\nOperating at 285 MHz and 0.8 V, Anda occupies 2.17 mm2 with 81.18 mW power consumption.\nThe MXU, serving as the core computing component of the Anda architecture, consumes 66.94% of the total power despite occupying only 18.89% of the area.\nThe BPC unit, which enables efficient online compression from the full-precision FP outputs to the Anda format, costs a small portion of the total area (3.23%) and power consumption (1.31%).\nOn-chip SRAM is the primary area contributor, with the activation buffer and weight buffer accounting for 40.09% and 36.87% of the total area, respectively. Their power consumption ratios are relatively low at 20.87% and 9.81% because of efficient data reuse within the Anda system.\n \nV-F Accuracy-Performance Trade-off\n This section explores speedup and energy efficiency improvements of the Anda system over the FP-FP baseline with accuracy loss constraints ranging from 0.1% to 5%.\nAs shown in Fig.\u00a018, using LLaMA-13B as an example, Anda achieves a 1.73\u00d7\\times\u00d7 speedup and 2.95\u00d7\\times\u00d7 energy efficiency improvement with only 0.1% accuracy loss, increasing to 2.74\u00d7\\times\u00d7 and 3.22\u00d7\\times\u00d7, respectively, when the constraint is relaxed to 5%. All models exhibit significant acceleration and efficiency gains as the tolerated accuracy loss increases.\nNotably, OPT and LLaMA models exhibit distinct characteristics when using the Anda format.\nThis stems from OPT\u2019s lower sensitivity to bit-width reductions, allowing the use of shorter mantissa bit-widths with minimal accuracy sacrifice. Consequently, under tighter accuracy constraints, e.g., 0.1%\u223csimilar-to\\sim\u223c0.5%, OPT models achieve greater speedups and energy efficiency improvements compared to LLaMA models. However, as accuracy constraints relax, their performance gains gradually converge.\nBy integrating the adaptive precision combination search algorithm with the Anda format, our architecture achieves flexible balancing of system performance and accuracy across diverse practical application scenarios, enabling efficient LLM inference under different LLM architectures and varying requirements. \nVI Related Works and Discussions\n Bit-serial & bit-parallel computing.\nBit-serial computing [37, 2, 55, 46, 7, 68, 75] has long been explored in DNN acceleration, offering flexibility for variable precision computations. However, most prior work focuses on INT operations, limiting applicability to LLMs with FP activations.\nApproaches like Bitlet\u00a0[55] and Bitlet-X\u00a0[7] explore FP-based bit-serial computing but introduce complex hardware and dataflow designs due to bit-interleave schemes. In contrast, our Anda simplifies alignment overhead using variable-length grouped activation encoding, leading to a more efficient hardware design.\nAlthough bit-serial computing typically has lower area efficiency and higher latency than bit-parallel approaches\u00a0[67, 85, 41, 61] due to complex timing control logic, it offers higher utilization across precision-scalable scenarios.\nWhile Anda uses bit-serial units, its design principles can benefit bit-parallel computing as well. For instance, the proposed bit-precision combination search method can rapidly determine the required precision for bit-parallel applications, potentially improving efficiency while maintaining accuracy. PTQ & quantization-aware training (QAT).\nQuantization, a key compression technique, is generally categorized into PTQ and QAT.\nIn the LLM era, PTQ\u00a0[66, 79, 51, 24] is more popular, efficiently producing deployable models with good accuracy in hours on a single GPU.\nIn contrast,\nQAT for LLMs\u00a0[53, 74, 21, 9], while potentially more accurate, is often impractical due to its extensive computational demands, often requiring multi-GPU systems and hundreds of training hours.\nAnda adopts the PTQ approach, swiftly allocating mantissa lengths for FP activations, and can be integrated into existing deployment pipelines with minimal overhead. Future research could explore using Anda for QAT, potentially enhancing accuracy while reducing computational costs. KV cache optimization.\nIn long-context scenarios, KV cache\u00a0[82], which stores attention keys and values during generation to avoid re-computation, becomes a memory and speed bottleneck as its size grows linearly with the number of tokens.\nVarious techniques has been proposed to tackle this challenge including quantization\u00a0[54, 29], eviction\u00a0[88, 10], sliding window\u00a0[80, 20], and merging strategies\u00a0[87, 40].\nAnda, while focusing on FP activation compression, could synergize with these KV cache optimizations to significantly accelerate long-context LLM inference. \nVII Conclusion\n This work presents Anda, a variable-length grouped activation data format that addresses energy and performance bottlenecks of weight-only quantized large language model (LLM) inference by exploiting redundancy in floating point activations across different models and their modules.\nTo fully harness the potential of Anda, we develop an iterative post-training algorithm that optimizes bit-width allocation across LLM modules, balancing accuracy, energy efficiency, and inference speed.\nWe design complementary hardware optimizations to maximize the benefits of Anda, including a bit-plane-based data organization scheme in memory, Anda-enhanced bit-serial processing units, and a runtime bit-plane compressor.\nOur evaluations show that Anda achieves a 2.4\u00d7\\times\u00d7 speedup, a 4.0\u00d7\\times\u00d7 enhancement in area efficiency, and a 3.1\u00d7\\times\u00d7 improvement in energy efficiency on average for popular LLMs compared to\nthe GPU-like FP-FP baseline.\nWith its adaptability across various application scenarios and performance requirements, Anda enables efficient LLM inference in diverse deployment environments, paving the way for broader adoption of LLMs in resource-constrained settings. Acknowledgements This project has been partly funded by the National Key R&D Program of China under Grant 2022YFB4400600, the European Research Council (ERC) under grant agreement No. 101088865, the Flanders AI Research Program, the KU Leuven Hercules program, and the China Scholarship Council Program (Grant No. 202306190235). We would like to sincerely thank Mengzhao Chen at HKU, Zhe Wang at HONOR, Zhi Zhang at SenseTime for the helpful discussion, and the anonymous reviewers for their constructive feedback. References"}
{"text": "*\n\\AtPageUpperLeft\n\n\nPresented at the 42nd IEEE International Conference on Computer Design (ICCD 2024),\nMilan, Italy, November 18-20, 2024\n A Prototype-Based Framework to Design Scalable Heterogeneous SoCs with Fine-Grained DFS\n\u2020\u2020thanks: This work was partly funded by the ISOLDE project (Grant No. 101112274), supported by CHIPS Joint Undertaking.\n Frameworks for the agile development of modern system-on-chips are\ncrucial to dealing with the complexity of designing such architectures.\nThe open-source Vespa framework for designing large,\nFPGA-based, multi-core heterogeneous system-on-chips enables a faster and\nmore flexible design space exploration of such architectures and\ntheir run-time optimization.\nVespa, built on ESP, introduces\nthe capabilities to instantiate multiple replicas of\nthe same accelerator in a single network-on-chip node and to partition\nthe system-on-chips into frequency islands with independent dynamic frequency scaling actuators,\nas well as a dedicated run-time monitoring infrastructure.\nExperiments on 4-by-4 tile-based system-on-chips demonstrate\nthe possibility of effectively exploring a multitude of solutions that\ndiffer in the replication of accelerators, the clock frequencies of the frequency islands,\nand the tiles\u2019 placement, as well as\nmonitoring a variety of statistics related to the traffic on the interconnect and\nthe accelerators\u2019 performance at run time. \nI Introduction\n While general-purpose central processing units\u00a0(CPUs) transitioned to multi- and many-core architectures\nin the last decades, due to the slowdown in the improvement of\ntheir performance and efficiency with the end of Moore\u2019s law and Dennard scaling,\nhardware acceleration emerged as the standard solution to support\nmodern computationally intensive workloads ranging from cryptography\u00a0[1]\nto deep-learning\u00a0[2] ones. The increased complexity and development costs of system-on-chips\u00a0(SoCs),\nthat are commonly heterogeneous multi-core processors combining\ngeneral-purpose CPU cores and hardware accelerators,\nincluding those obtained through high-level synthesis\u00a0(HLS)\u00a0[3],\nhave imposed the research and the adoption of novel design methods for their architectural exploration,\nsystem integration, verification, validation, and physical design.\nAgile development through field-programmable gate array\u00a0(FPGA)-based prototyping has therefore emerged as\nthe standard paradigm to support the rapid design of\ncomplex heterogeneous platforms, offering\na complementary tool to classic cycle-accurate simulators.\nFor this reason, a multitude of frameworks to quickly deliver FPGA prototypes has been\nintroduced during the last decade, fueled by a drop in the cost per look-up table\nand by the availability of ever-larger FPGAs. While cycle-accurate simulators were traditionally used\nin the early stages of the architectural exploration, the specialization of\nthe current computing platforms has hindered their adoption.\nThey fail indeed to support the accurate simulation of complex computing platforms with\nhardware accelerators since the employed generic area and power models cannot\nprovide accurate estimates, whereas designing area and power simulation models\nfor each specific accelerator is a lengthy and difficult process.\nIn addition, simulators\u2019 performance is severely curbed\nby their single-thread software architecture. Enabling a quick and comprehensive design space exploration\u00a0(DSE) of\nsuch complex architectures, also accounting for their run-time optimization,\nis critical, and even the few available open-source frameworks\u00a0[4, 5, 6]\nlack such support.\nIndeed, the ESP\u00a0[4] state-of-the-art framework for the agile development\nof complex accelerator-centric multi-core heterogeneous SoCs\nnotably combines a scalable tile-based architecture and a flexible\nsystem-level design methodology to deliver prototypes on\nFPGA targets, but its tiles\u2019 limited configurability prevents an in-depth DSE\nand it lacks the dynamic frequency scaling\u00a0(DFS) actuators and a proper run-time monitoring infrastructure\nrequired to support an effective run-time optimization\u00a0[7]. This work introduces the Vespa framework, which extends ESP\u00a0[4]\nto enable both the DSE and the run-time optimization of large\nmulti-core heterogeneous SoCs, providing three main contributions. A design-time parameter and related hardware infrastructure enable\nscaling the throughput of third-party hardware accelerators,\nwithout altering the design of the latter or the network-on-chip\u00a0(NoC) interconnect,\nby instantiating multiple replicas of the same accelerator\nwithin a single computing tile assigned to an NoC node, The on-chip interconnect and the computing elements\ncan be configurably partitioned into frequency islands, each of whom is independent from the others\nand is fed a clock signal that is either fixed or generated by a DFS actuator. A run-time monitoring infrastructure enables the collection of various execution statistics,\nexposed through memory-mapped hardware counters, to support the DSE and\nthe run-time optimization of the SoC. The Vespa framework for the design space exploration and run-time optimization of large\nmulti-core heterogeneous SoCs is publicly available and released\nopen source111https://github.com/hardware-fab/vespa.\nwith the goal of fostering further research in the field. \nII Methodology\n The Vespa framework, depicted in Figure\u00a01,\nleverages ESP and enables\nthe design space exploration and run-time optimization of\ncomplex accelerator-based SoCs by means of\nflexible multi-replica accelerator tiles,\nconfigurable-DFS frequency islands, and\na dedicated run-time monitoring infrastructure. \nII-A Multi-replica accelerator-tile architecture\n A simple and time-efficient solution to increase the throughput of\nan existing hardware accelerator integrated into a computing platform\nis to instantiate multiple replicas of the former.\nIndeed, instantiating K such replicas intuitively provides\na K\u00d7 increase of the throughput of the baseline accelerator. Vespa\u2019s multi-replica accelerator tiles\u00a0(MRA in Figure\u00a01)\ncan instantiate multiple replicas\u00a0(Acci) of\na hardware accelerator within a single computing tile without any changes\neither to the accelerator or the size and topology of the NoC interconnect,\nand avoiding being forced to occupy multiple computing tiles with\nthe same accelerator in order to improve its performance.\nThe multi-replica accelerator tiles\nsupport any third-party hardware accelerator that exposes an AXI interface.\nTheir replication factor K, which\ncorresponds to the number of copies of\nan accelerator that are instantiated and operate in parallel,\nis configurable by the system designer to easily tune the\ntrade-off between throughput and area. A baseline accelerator to be integrated into an ESP SoC exposes four AXI4-Stream interfaces\nfor read control\u00a0(rdCtrl), write control\u00a0(wrCtrl),\nread data\u00a0(rdData), and write data\u00a0(wrData) purposes, respectively.\nA multi-replica accelerator tile still exposes the same AXI4-Stream interfaces\ntowards the NoC interconnect\u00a0(NoC) to guarantee\nfull compatibility with the ESP SoC architecture.\nThe AXI bridge component is therefore tasked with multiplexing\nthe four AXI4-Stream interfaces of each of the K accelerator replicas\ninto four corresponding buffers for the AXI4-Stream interfaces of the tile. \nII-B Configurable-DFS frequency-island architecture\n The Vespa framework allows partitioning the SoC into multiple frequency islands\nat design time. Every SoC tile and NoC router is assigned to\na frequency island that can group multiple computing and routing elements.\nEach frequency island is fed an independent clock signal that can either\nhave a fixed frequency or be generated by a dedicated DFS actuator. The architecture supporting such SoC partitioning can\nbe split into three main components, namely\na set of registers holding the frequency configuration of each\nisland\u00a0(Frequency registers in Figure\u00a01),\na DFS actuator\u00a0(DFS) for each of the frequency islands, and\nresynchronizers\u00a0(Resync) at the boundaries of the latter. Notably, the clock signal output by the mixed-mode clock managers\u00a0(MMCMs) of an AMD FPGA\nremains low during its reconfiguration, thus causing\na clock-gating effect on the computing platform.\nThe DFS actuator implemented in the Vespa framework avoids such negative effect by\nemploying two MMCM components. In each DFS actuator, an internal finite-state machine forces\nthe master MMCM to preserve the current output clock signal while\nthe slave one is under reconfiguration, after which their roles are swapped. \nII-C Run-time monitoring infrastructure\n Vespa\u2019s run-time monitoring infrastructure is crucial to collecting statistics from\nthe computing platform and thus supporting run-time optimization\npolicies and design space exploration.\nFor each accelerator, the run-time monitoring infrastructure can selectively\nenable the monitoring of up to four different statistics, each corresponding to\na specific counter in the tile: execution time, number of incoming and outgoing packets, and\nthe round-trip time.\nThe latter is defined as the time occurring between\na request for data from an accelerator to the main memory through\ndirect memory access and the ensuing arrival of such data to the accelerator. The execution time counter automatically resets when the\naccelerator tile starts its computation and stops when the latter is completed,\nwhile the other three counters are reset manually.\nAll counters instantiated on accelerator tiles expose\nmemory-mapped registers that can be accessed both via software executing on\nCPU cores of the SoC itself and via the USB-to-serial interface that connects\nthe platform to the host. \nIII Experimental evaluation\n The experimental campaign targeted a Siemens proFPGA quad\u00a0(mb-4m-r3)\nmotherboard222https://www.profpga.com/products/motherboards-overview/profpga-quad\nequipped with a Virtex-7 2000 FPGA\u00a0(fm-xctv2000t-r2)\nmodule333https://www.profpga.com/products/fpga-modules-overview/virtex-7-based/profpga-xc7v2000t,\nwhose AMD FPGA chip features\n1221600 look-up tables\u00a0(LUT), 2443200 flip-flops\u00a0(FF), 2584 18Kb blocks of\nblock RAM\u00a0(BRAM), 2160 digital signal processing\u00a0(DSP) elements, and 24 MMCMs. The accelerators instantiated in the SoC were obtained through\nHLS of applications from the CHStone benchmark suite\u00a0[8].\nWe employed Cadence Xcelium 20.09 to simulate SoC instances,\nAMD Vivado 2019.2 for HLS, RTL synthesis and implementation, and bitstream generation, and\nSiemens proFPGA Builder 2019A-SP2 for FPGA programming.\nArea and timing results refer to the post-implementation netlists of the prototypes,\nwhile system-level statistics from the SoC execution are collected from the prototyped SoCs\nthrough the run-time monitoring infrastructure described in Section\u00a0II. Without loss of generality,\nwe evaluated our methodology on 4-by-4 tile-based SoC instances\nthat feature a CVA6\u00a0[9] CPU tile, a DDR memory one, and an auxiliary tile,\nwhile the thirteen remaining tiles instantiate HLS-generated CHStone accelerators.\nEleven TG tiles\ngenerate traffic in the NoC interconnect and implement\ndfadd accelerators, which were\nempirically observed to be memory-bound,\nand two more accelerator tiles are placed in the A1 and A2 positions,\nthat are respectively close to\u00a0(A1) and far from\u00a0(A2) memory\u00a0(MEM).\nThe SoC is split into five separate frequency islands,\nnamely,\nthe A1 and A2 accelerators,\nthe NoC interconnect and memory controller\u00a0(MEM),\nthe traffic generation\u00a0(TG) cores,\nthe CPU core\u00a0(CPU), and\nthe auxiliary tile\u00a0(I/O).\nThe DFS actuator of the NoC island implements a range of\noperating frequencies comprised between 10MHz and 100MHz, while\nthe other four clock domains support clock signals with frequencies from 10MHz to 50MHz.\nThe clock frequency ranges include values at 5MHz steps.\nFigure\u00a02 depicts the FPGA floorplan of an SoC instance. \nIII-A Multi-replica accelerator tiles: area-performance analysis\n Each baseline HLS-generated accelerator\noccupies up to 1.4%, 0.6%, 1.0%, and 3.8% of the LUT, FF, BRAM, and DSP resources\navailable on the target FPGA, as listed in Table\u00a0I,\nthus justifying the possibility to improve their performance by\nexploiting the multi-replica accelerator-tile architecture.\nWe explore therefore the instantiation of 2\u00d7-\nand 4\u00d7-replication tiles and compare their\nresource utilization and throughput.\nThe throughput is measured on the accelerator instantiated in the A1 tile,\nwith clock frequencies of 100MHz and 50MHz for the NoC-MEM and A1-tile\nislands, respectively, and with all TG tiles disabled,\nthus providing the best performance achievable by\nthe accelerator due to its proximity to memory and\nno concurrent requests from the other computing tiles in the SoC. The increase of LUT, FF, and BRAM utilization is shown to be quite smaller than\nthe replication factor, since such resources are\nalso being employed in the additional logic of the multi-replica accelerator tile.\nOn the contrary, DSP resources end up being around 2 and 4 times\nas much as in the baseline versions.\nPerformance also scales correspondingly with\nthe replication of accelerators, as shown by the throughput columns in TABLE\u00a0I.\nThe 2\u00d7- and 4\u00d7-replication tiles have an average throughput increase\nof 1.92\u00d7\u2009 and 3.58\u00d7, respectively.\nThe larger average throughput improvement compared to the corresponding\nincrease in LUT, FF, and BRAM utilization justifies\nour multi-replica accelerator-tile solution. \nIII-B SoC accelerators: performance analysis\n We select adpcm and dfmul as representative examples of\ncompute- and memory-bound accelerators, respectively,\nand analyze the relation between their throughput and\nthe amount of traffic in the NoC interconnect generated by TG cores.\nFigure\u00a03 depicts the throughput of\nthe adpcm and dfmul accelerators\nwith a number of active TG cores ranging between\n0, i.e., when all of them are disabled, and\n11, i.e., when all the TG cores in the SoC are enabled.\nThe NoC interconnect runs at 10MHz while\nthe accelerators and TG cores operate at 50MHz. The compute-bound adpcm\nshows an almost constant throughput between 0 and 7 active TG cores while,\non the contrary, in the same range of X axis values,\nthe throughput of the dfmul accelerator drastically decreases,\nthus highlighting the memory-bound nature of the latter. \nIII-C Memory: traffic analysis\n Finally, we show the possibility of evaluating the memory traffic depending on\nthe clock frequencies of the various frequency islands.\nWe consider a specific SoC instance with A1 and A2 tiles\nboth instantiating the memory-bound dfmul and running concurrently.\nFigure\u00a04 depicts the temporal evolution of incoming data packets to memory,\nexpressed as millions of packets per second\u00a0(Mpkt/s) in Figure\u00a04(b),\nwhile varying the frequency islands\u2019 clock frequencies,\ndepicted in Figure\u00a04(a) in\nred for the A1 and A2 tiles,\nyellow for the NoC interconnect and memory controller, and\ngreen for the TG cores. Varying the clock frequency of the A1 and A2 tiles between\n10MHz, 30MHz, and 50MHz values is shown to have a negligible impact on the memory incoming traffic,\nwhile increasing the operating frequency of the TG cores drastically increases\nthe pressure on memory when the NoC interconnect and memory controller are\nalso running at a high clock frequency.\nSuch results demonstrate the possibility to analyze\nthe impact of DFS on the local and global traffic of the SoC. \nIV Conclusions\n This paper introduced the open-source Vespa framework for designing large tile-based\nmulti-core heterogeneous SoCs. Vespa extends the open-source ESP\ntoolchain by\n\n\ni) adding the possibility to configure the area-performance trade-off for each accelerator tile,\n\nii) partitioning the SoC into multiple frequency islands with independent true DFS actuators, and\n\niii) implementing a run-time monitoring infrastructure that enables the DSE of complex prototypes.\n\n The experimental campaign, targeting a large AMD Virtex-7 2000 FPGA chip,\ndemonstrated its effectiveness in\nanalyzing the area-performance trade-off by leveraging\nthe scalable throughput architecture, the DFS actuators, and\nthe run-time monitoring infrastructure, thus making\nthe proposed framework a valuable tool for designing large FPGA-based systems. References"}
{"text": "Automatic High-quality Verilog Assertion Generation through Subtask-Focused Fine-Tuned LLMs and Iterative Prompting \n  Formal Property Verification (FPV), using SystemVerilog Assertions (SVA), is crucial for ensuring the completeness of design with respect to the specification. However, writing SVA is a laborious task and has a steep learning curve. In this work, we present a large language model (LLM) -based flow to automatically generate high-quality SVA from the design specification documents, named AssertCraft.\nWe introduce a novel sub-task-focused fine-tuning approach that effectively addresses functionally incorrect assertions produced by baseline LLMs, leading to a remarkable 7.3-fold increase in the number of functionally correct assertions. Recognizing the prevalence of syntax and semantic errors, we also developed an iterative refinement method that enhances the LLM\u2019s initial outputs by systematically re-prompting it to correct identified issues. This process is further strengthened by a custom compiler that generates meaningful error messages, guiding the LLM towards improved accuracy. The experiments demonstrate a 26% increase in the number of assertions free from syntax errors using this approach, showcasing its potential to streamline the FPV process. \nI Introduction\n The slowdown in processing power following Moore\u2019s law has introduced a significant rise in hardware diversity, resulting in an increased need for design verification (DV) [1]. Originally, DV was performed by extracting the properties of the design and extensively trying to prove them by finding corner cases and new simulation testbenches. However, in the earlier cycles of DV, this tedious and uncertain task was replaced by formal verification [2]. The formal method replaced this uncertain task with the automatic process of proving that the property is correct under all circumstances or there is a counter-example proving otherwise. The properties (assertion statements) in the design are written in Property Specification Language (PSL), containing an assert keyword and a property inside them which are used to check the correctness of that property. These properties come from design specifications and show the expected functionality of the Design Under Test (DUT). However, each design has a separate specification and infinite ways of implementation. As a result, the formal verification engineers need to generate independent assertions by reading the design specification and the RTL design, which makes this process tedious and time-consuming. This hurdle prevented engineers from using it more commonly [3]. Given the recent increasing complexity of designs, the demand for more efficient methods to streamline this process has become critical. A number of studies have attempted to automate parts of the assertion extraction process. Earlier efforts in this domain sought to simplify this task by providing a new abstraction level that is closer to human language [4] [5]. However, full automation has not been achieved, as the specification must still be converted to this new abstraction level rather than directly to the assertion level.\nlately, large language models (LLM) have demonstrated significant promise in automating hardware design and assisting engineers throughout the design process [6]. This has led to promising developments in using LLMs to automate the verification process [7, 8, 9]. Kande et al. [7] illustrated how LLMs can generate new assertions from detailed comments, while Orenes et al. [9] showcased their effectiveness in creating new properties from the Register Transfer Level (RTL) code. Despite these advancements, no successful attempt has yet been made to generate high-quality assertions directly from specification documents. Our research, detailed in Section II-B, investigates the challenges faced by current state-of-the-art large language models in generating assertions directly from specification documents. This process requires complex, multi-step reasoning, which is a known challenge for Pretrained LLMs (PLLMs), as demonstrated in recent studies on other applications [10] [11] [12].\nAs a promising solution, fine-tuning LLMs is essential and widely adopted to unlock robust capabilities for complex tasks, enhance performance on downstream applications, and better align with human preferences [13]. However, this approach relies on the availability of \u201dlarge-scale, annotated task-specific training data\u201d accumulated over time. This dependency limits the practical use of LLMs-based assertion generation models in industrial verification scenarios where such training data is scarce. While fine-tuning the LLM for assertion generation from specifications may present significant challenges, creating a dataset for a subtask of the original problem is feasible. Therefore, we introduce a novel \u201csub-task-focused fine-tuning method,\u201d which involves dividing the task into subtasks that can be fine-tuned separately. While the method may yield sensible initial outputs, some of the assertions generated using this method can still contain compilation bugs that must be addressed.\nHowever, having enough assertions without compilation bugs is essential to hit a high level of coverage and enter the late cycles of the design [14][15]. So this challenge should be addressed efficiently.\nTo tackle these issues, we introduce a \u201diterative bug-fixing paradigm\u201d that tries to build upon its previous patch by using a refinement loop between the compiler and the debugger LLM. This paradigm, unlike the comment one-step paradigms [16] [17] would not miss the multi-location bugs and is good at catching small bugs [18].\nIn this work, for the first time, we brought this multi-step bug-fixing paradigm into the assertion generation flow by adding a customized compiler that can generate meaningful refinement prompts for the LLM. Alongside, to assess the proposed flow\u2019s ability to generate high-quality assertions, we developed a comprehensive evaluation tool-set that includes a dataset and a scoreboard for measuring assertion quality. Our experiments revealed an impressive improvement of 7.3 times in the number of functionally correct assertions compared to the baseline PLLM, i.e. GPT-3.5. Additionally, we achieved near-perfect (100%) stimuli coverage on several designs. The key contributions of this paper are summarized as follows: Launch of a fully automated toolset for RTL assertion generation from design specifications. Introduction of a novel sub-tasked focused fine-tuning method for assertion generation which improves the ability of GPT to produce functionally correction assertions by 7.3x. Development of an Iterative repair method for assertion generation using a specialized compiler engineered to produce meaningful error messages for LLMs, which is able to fix 26% assertions with bugs. Creation of a dataset for evaluating LLM-based assertion generation, which includes specifications and reference code to establish a robust benchmark for assessing the quality of generated assertions. \nII Related Work and Motivation\n \nII-A Automation in Assertion Generation\n Earlier works in automatic assertion generation, such as AutoSVA [4] and ILA [5], have focused on creating a new abstraction level closer to human language. Although they succeeded in making the assertion generation process easier, they lost their generality and did not entirely solve the process, as engineers still had to develop new properties at these abstraction levels. Another endeavor in assertion automation targets the common patterns in the waveform for assertion generation [19]. It first uses data mining techniques to discover patterns in simulation test benches, then formulates assertions to describe these patterns. Although this work achieves full automation in assertion generation, it suffers from a high number of generated assertions and the possibility of the assertions being wrong due to the waveform being generated using a design under test, which might have numerous bugs. In response, Liu et al. [20] and [21] sought to address these issues by utilizing higher abstraction levels for waveform generation, such as transaction-level modeling, and by enhancing their techniques for ranking generated assertions, thereby reducing their overall number. Despite these improvements, their approaches still fail to extract functionally correct assertions from specification documents and rely on additional assumptions, such as access to simulation traces or a higher abstraction level, rendering them semi-automated. Additionally, Witharana et al. [22] provide a comprehensive survey of recent advancements in automated techniques for hardware verification using simulation traces. With the rise of new LLMs [23, 24, 25] capable of accurately generating simple RTL code similar to humans, recent research has shifted focus to the use of LLMs for verification. Kande et al. [7, 8] addressed a fundamental research question: Can LLMs be used to generate assertions from comments? To explore this, they generated assertions at three levels of detail and assessed whether the LLM-generated assertions were accurate based on the provided comments. Their findings suggest that LLMs can produce correct assertions from comments, but only when the comments are detailed and include all relevant signal names and operations. The latest work in this area [9] demonstrated the capability of LLMs to generate assertions from RTL code by incorporating additional rules into the LLM\u2019s prompt. Although both works were successful in generating correct assertions, they made some impractical assumptions. For example, [7] uses a well-defined comment to produce assertions instead of extracting them from the specification, and [9] may generate assertions from buggy RTL code, which can lead to incorrect assertions. \nII-B Challenges in PLLM-Based Assertion Generation\n LLMs like GPT-2 and BERT are transformer-based artificial neural networks designed to operate on text datasets. These models contain millions to billions of parameters and are trained on vast amounts of text data. Both the inputs and outputs of an LLM consist of tokens, which are common sequences of characters. When given a sequence of tokens as a prompt, an LLM generates a probability distribution for the next token based on its vocabulary. Once a token is chosen according to specific criteria, it is added to the prompt, and the LLM proceeds to generate the next token in a process known as auto-regression. One way to generate assertions from the specification is to prompt the PLLM to create assertions without additional fine-tuning or details. We attempted to generate assertions directly from the specification document using PLLM, and the results are illustrated in Figure 2. As shown, only 240 assertions were generated for all the modules in the dataset, out of a potential 892, with only 19 of those being functionally correct. These poor results stem from the complexity of generating assertions from the specification. Listing 1 further illustrates why so few assertions were produced using this naive approach. In this example, we presented the PLLM with a simple Finite State Machine specification and asked it to generate assertions. However, instead of producing multiple assertions for each transition between states, it attempted to encapsulate the entire concept in a single assertion. As a result, the generated assertion was not only syntactically incorrect due to multiple implied statements, but also functionally incorrect according to the specification. In fact, PLLMs can only generate accurate assertions when features such as signal names and their relationships are described with precision. \nIII Proposed AssertCraft\n The overall proposed flow, named as AssertCraft, is illustrated in Figure 1. The automation process begins with a clear English specification outlining the RTL code\u2019s functionality. This specification must be comprehensive enough for the engineer to write the RTL code based solely on the provided information. Once the specification and the corresponding RTL design are established, the proposed automation flow verifies the design\u2019s implementation through the generated assertions. Essentially, the tool autonomously generates assertions and enhances their quality without requiring external input. The figure illustrates the assertion generation and evaluation flows of AssertCraft, presented separately on the left and right sides. In the assertion generation section, it accepts the design specification and RTL design as input and produces assertions. AssertCraft generates these assertions in two steps. In step (1), AssertCraft employs a focused fine-tuning method to generate an initial assertion from the design specification. This assertion then undergoes an iterative repair process, which aims to correct any syntax and semantic errors, marked as step (2) in the flow. On the right side of the figure, the evaluation flow is presented to assess the quality of the generated assertions. To achieve this, It first builds a novel dataset for this task in step (3). Ultimately, it evaluates the quality of the results on the dataset using a scoring system. \nIII-A Subtask-Focused Fine-Tuning\n AssertCraft\u2019s assertion generation process from the design specification and the RTL code has two main steps, as depicted in Figure 1. In the first step, Subtask-Focused fine-tuning, only the design specification is given to the tool for the initial assertion generation, and the tool is expected to generate assertions. We employed GPT-3.5-turbo[26] as the Language Model for its fine-tuning capabilities and cost-effectiveness compared to GPT-4 [27]. Additionally, its utilization is free if the API is not accessed.\nWe used the default fine-tuning setting recommended by the OpenAI and fine-tuned the model for 3 epochs. We propose to enhance the generation of assertions from specifications using LLMs by fine-tuning the model. However, we find this process challenging because a dataset containing both specifications and assertions does not exist, and generating one would consume an enormous amount of time and resources. Nevertheless, a robust dataset containing comments for the assertions and the assertions themselves can be created by scraping GitHub repositories that contain SystemVerilog files. As a result, we divided the task into two \u201dsub-tasks\u201d which can be individually improved using fine-tuning or zero-shot prompting techniques. First, we generate comments from the specification. Every individual comment is then used to make the assertions using a fine-tuned LLM which is trained for assertion generation from the comments. We use three questions to extract meaningful comments from the specification. Listings 2, 3, and 4 outline the inquiries used to break the specification into smaller components. The first question in Listing 2 is directed at FSMs in the design. For every FSM, we extract the states and the conditions to transfer from one state to another. This helps to break each FSM into multiple assertions, each indicating one transition between the states. Question B in Listing 3 was tailored to extract conditional statements, and Question C to extract the variable ranges. Each of these pieces of information was then treated as one assertion in the remainder of the flow. To fine-tune GPT, we first created a robust dataset containing comments and their corresponding assertions. Figure 1\u2013part (b) illustrates the creation process of this dataset. Two approaches were pursued to make this dataset. Initially, existing assertions within GitHub repositories were extracted to be used in the dataset. However, only some of the extracted assertions had well-defined comments, necessitating the process of data cleaning and selecting the best matching comments and assertions. Consequently, a sorting mechanism based on the cosine similarity of their word2vec vectors was implemented, setting a threshold from 0.6 onwards to filter out irrelevant assertions, as seen in the right section of Figure 1\u2013step(1)-part (b).\nUltimately, this process yielded 500 relevant examples. Additionally, another 500 assertions were incorporated, randomly generated by an automated script, which were then matched with predefined sentences for each operation to form the comment for the assertion. Our dataset served as the foundation for refining GPT3.5 Turbo over three epochs. Finally, we modified the system message for better results by integrating content from a SystemVerilog property specification cheat sheet [28]. Figure 2 demonstrates the impact of custom GPT on result quality. Notably, it reduces assertions with failed syntax errors by 33% while nearly doubling the number of functionally correct assertions. \nIII-B Iterative Repair\n Using a sub-task fine-tuned model significantly improved the quality of generated assertions, the assertions generated in this step had only access to specification. This prevents the model from generating incorrect assertions due to buggy RTL code but increases the risk of semantic mistakes since the model does not know about the implementation and the signal names. In our experiment, shown in Table I, we found that 78% of the assertions were failing due to syntax errors before reaching this step. As a result, we incorporated an iterative repair method to address this issue. This process is illustrated in Figure 1\u2014step (2). First, the RTL code and the assertions are provided to an LLM, which is tasked with correcting any semantic errors. Next, the assertions are submitted to a compiler for correctness verification. If the generated assertion is correct, it is output as the final assertion for the model. However, if the assertion is incorrect, the compilation error is sent back to the LLM for correction. This process is repeated until we obtain an assertion free of any syntax or semantic errors, or until we reach a predefined iteration threshold where a correct assertion cannot be generated. Our findings indicate that in such cases, the model may become stuck in an indefinite loop, repeatedly providing the same answer. Through our investigation, we found that GPT struggles with tasks that require step-by-step thinking. For example, it cannot detect the simple task of finding the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT word in the text, which is the most commonly used error message for humans. As a result, the first change in our custom compiler was the annotation of the error part without merely indicating the location of the error. The second change involved a simple addition for combinational circuits. Since our model was struggling to generate correct assertions for combinational circuits, we tasked it with converting the sequential assertion into a combinational assertion by adding a set of rules. At this stage, we instructed the LLM to delete all clock-related functions and sections in the assertion and convert the implications to the combinational mode. For example, if the compiler encountered the keywords (a|\u2212>ba|->bitalic_a | - > italic_b or a=>ba=>bitalic_a = > italic_b), we tasked it to convert them to (!a|b!a|b! italic_a | italic_b). \nIII-C Evaluation Flow\n Evaluating any LLM-based assertion generation process requires a structured flow due to the complexity and variability of the generated assertions, which complicate consistent accuracy assessment. To measure AssertCraft\u2019s effectiveness, we created an evaluation flow that includes a dataset and a scoreboard. This general approach can be applied to evaluate any LLM-based assertion generation process. This dataset helps us measure the quality of generated assertions by providing a TCL script, Specification, and the correct RTL design of that specification.\nThe dataset in Figure 1\u2013part(3) is derived from a subset of the HDLBits dataset [29], which includes specifications for various hardware designs. Several enhancements were made to adapt this dataset for verification purposes. First, the golden RTL code was added to the specification for each design from a GitHub repository [30]. Although these RTL codes were functionally correct in accordance with the simulation result from the HDLBits website, they were not all synthesizable. As a result, the first subtle modification was making the designs synthesizable and compatible with FPV engines. Subsequently, a TCL script was written for them to prepare it for the FPV engine execution. Table II enumerates all the Verilog designs within the HDLBits dataset. However, not all of the designs were suitable for verification tasks, as some had images in their specifications or lacked descriptive specifications from which to make assertions. The right-hand side of the table showcases the designs that were used in our dataset. Throughout this paper, these modules are referenced using a hierarchical numbering scheme (e.g., 1.2.0.3), which signifies the first entry of category 1, the second entry of category 2 (excluding category 3), and the third utilized module (in this case, \u201dReplicate\u201d). The final step in the evaluation platform is a pass to the FPV engine. This step determines the quality of results for the entire flow. The FPV engine categorizes the assertions into three main categories. The first type is the syntactically incorrect assertions. These assertions can have incorrect syntax, like a missing parenthesis, or be semantically incorrect, like incorrect naming for signals. The second category consists of the assertions with the correct syntax for which the FPV engine can find a counterexample. Since the RTL code is the golden RTL and it is in accordance with the specification, these assertions are labeled as functionally incorrect. Finally, the assertions with no syntax error or counterexample within the 20 cycles of the FPV run were labeled as functionally correct. We chose 20 cycles as the threshold since it was sufficiently accurate in its prediction. \nIV Experimental Results\n In this section, we share the results from the evaluation flow on different modules in table II. In the following sections, we showed the ability of AssertCraft to generate functionally correct assertions and assess the coverage of generated assertions. \nIV-A Correctness\n Figure 3 provides a more detailed overview of the generated assertions for each Verilog module in the dataset in table II. The y-axis illustrates the filename using the indexing format introduced in section III-C1, and the x-axis shows the number of assertions that are functionally correct (yellow bar), functionally incorrect (blue bar), and syntactically or semantically incorrect (red bar). The figure is divided into two general sections: modules with clock and reset signals and modules without clock or reset signals. This distinction is important in assertion generation as the patterns for combinational and sequential assertions are completely different. Our analysis reveals that in most modules, AssertCraft was able to generate at least one functionally correct assertion. Additionally, the number of failed assertions due to incorrect syntax is considerably higher in modules without a clock, as it is more challenging to generate these assertions for such modules compared to those with concurrent assertions. \nIV-B Coverage\n We computed the coverage for each module in the dataset and aggregated the results for each category of modules. Coverage is one of the fundamental metrics in formal verification which is used as an indicator to move to the next stages of the design. Figure 4 illustrates the coverage distribution for each category of data. As can be seen, the stimuli coverage is almost always between 80% and 100% for all categories, while the checker and formal coverages vary significantly based on the category. However, the mean of the distribution is more than 50% in most scenarios. \nV Conclusion\n In this paper, we showcased the current state of pretrained LLMs in the assertion generation process and identified the challenges faced by state-of-the-art LLMs in this complex task. We also introduced a novel sub-task focused fine-tuning method that can improve the accuracy of LLMs by 7.3 times on this task. Furthermore, we refined the final output of the model using an iterative refinement method which can direct the LLM toward assertions without syntax or semantic errors, which leads to a 26% increase in the number of correct assertions. While our framework demonstrates the potential of LLMs in generating high-quality assertions, further exploration is needed to fully leverage their capabilities in verification tasks. For instance, we could apply LLMs to automated theorem proving, where they can assist in formulating logical proofs. Additionally, exploring their application in formal methods for model checking could also yield insights into verifying system properties more efficiently. \nVI Acknowledgements\n We acknowledge the support from the Natural Sciences and Engineering Research Council (NSERC) of Canada, This work is funded by the Natural Sciences and Engineering Research Council of Canada NSERC https://www.nserccrsng.gc.ca/ under Grant No. NETGP485577-15 NSERC (COHESA project) and 341516 NSERC (RGPIN), along with in-kind support from AMD and Intel/Altera.\n\n\n References"}
{"text": "Exploring the Sparsity-Quantization Interplay on a Novel Hybrid SNN Event-Driven Architecture Spiking Neural Networks (SNNs) offer potential advantages in energy efficiency but currently trail Artificial Neural Networks (ANNs) in versatility, largely due to challenges in efficient input encoding. Recent work shows that direct coding achieves superior accuracy with fewer timesteps than traditional rate coding. However, there is a lack of specialized hardware to fully exploit the potential of direct-coded SNNs, especially their mix of dense and sparse layers. This work proposes the first hybrid inference architecture for direct-coded SNNs. The proposed hardware architecture comprises a dense core to efficiently process the input layer and sparse cores optimized for event-driven spiking convolutions. Furthermore, for the first time, we investigate and quantify the quantization effect on sparsity. Our experiments on two variations of the VGG9 network and implemented on a Xilinx Virtex UltraScale+ FPGA (Field-Programmable Gate Array) reveal two novel findings. Firstly, quantization increases the network sparsity by up to 15.2%percent15.215.2\\%15.2 % with minimal loss of accuracy. Combined with the inherent low power benefits, this leads to a 3.4\u00d73.4\\times3.4 \u00d7 improvement in energy compared to the full-precision version. Secondly, direct coding outperforms rate coding, achieving a 10%percent1010\\%10 % improvement in accuracy and consuming 26.4\u00d726.4\\times26.4 \u00d7 less energy per image. Overall, our accelerator achieves \u00a051\u00d751\\times51 \u00d7 higher throughput and consumes half the power compared to previous work. Our accelerator code is available at: https://github.com/githubofaliyev/SNN-DSE/tree/DATE25. \nI Introduction\n Spiking Neural Networks (SNNs) offer a biologically inspired, energy-efficient alternative to traditional Artificial Neural Networks (ANNs) by utilizing sparse, event-driven computations. SNNs mimic the brain\u2019s neurons by communicating through discrete pulses or \u201dspikes\u201d. However, achieving accuracy comparable to ANNs, especially in deep SNNs, is challenging due to the complexities of encoding input data into spikes [1, 2]. Direct coding [3] is a promising approach that addresses this challenge by repeatedly presenting input samples over multiple timesteps, improving SNN accuracy with fewer timesteps compared to the popular rate coding approach [4]. In direct coding, the raw, floating-point input data is directly fed into the first convolution layer, which generates floating-point outputs. These floating-point outputs are processed by a spiking neuron layer that implements a threshold-based spike generation mechanism to produce the binary spikes that drive the rest of the SNN. While prior work suggests that rate-coded networks may be more energy efficient due to the non-binary activations in direct-coded SNN input layers [4], we argue that the true energy-saving potential of direct coding remains largely untapped. Existing evaluations [4] are constrained by fixed timestep comparisons and platforms optimized for dense computations, overlooking the inherent benefits of variable sparse activity patterns characteristic of direct-coded SNNs. Moreover, traditional event-driven [5] or systolic array-based SNN implementations [6], while efficient for homogeneous computations, struggle to effectively handle the diverse layer characteristics present in direct-coded SNNs. This mismatch often leads to underutilization of processing elements in layers requiring fewer resources, further diminishing overall efficiency. To fully realize the potential of direct-coded SNNs, heterogeneous architectures with specialized hardware for both dense and sparse layers are essential. In this paper, we propose the first-of-its-kind hybrid architecture explicitly designed for direct-coded SNNs. Our architecture leverages workload partitioning, assigning the input layer\u2014characterized by the largest feature map dimensions, non-binary, and non-sparse activations\u2014to a dedicated dense core. The remaining network layers, exhibiting varying degrees of sparsity, are processed by specialized sparse cores. Our hardware design is further distinguished by novel, parameterized, fully-pipelined, and high-throughput datapaths. We establish fine-grained, layer-wise workload models that encompass individual layer sizes, input feature map dimensions, input activation sparsity, and other key factors. These models serve as the foundation for design-time parameter selection, enabling the optimal partitioning of heterogeneous hardware resources to maximize performance. Furthermore, we conduct the first ablation study to investigate the impact of quantization on the intrinsic sparsity behavior of direct-coded SNN models, an important step toward realizing energy-efficient SNN implementations. Our results on a Xilinx Virtex UltraScale+ FPGA across multiple datasets demonstrate the significant advantages of our hybrid architecture, quantization, and direct coding for SNNs. Quantization (with 4-bit integer weights and biases) substantially reduces spiking traffic, while direct coding achieves state-of-the-art accuracy with minimal timesteps, countering prior findings [4] and highlighting the potential for reduced hardware latency and improved energy efficiency. These results underscore the critical need for specialized architectures to fully unlock the efficiency potential of SNNs. Notably, compared to the most related work [7], our accelerator achieves 50%percent5050\\%50 % lower power and 51\u00d751\\times51 \u00d7 higher throughput for a spiking VGG9 implementation on CIFAR100. \nII Background\n \nII-A Spiking Neuron Model\n The spiking neuron model used in this work is a leaky integrate-and-fire (LIF) neuron [8], whose characteristics are shown in Equations 1 and 2. Equation 1 describes how the neuron\u2019s membrane potential (uj\u2062[t]subscript\ud835\udc62\ud835\udc57delimited-[]\ud835\udc61u_{j}[t]italic_u start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT [ italic_t ]) evolves over time. The decay factor (\u03b2\ud835\udefd\\betaitalic_\u03b2), ranging between 00 and 1111, controls how much the previous potential uj\u2062[t]subscript\ud835\udc62\ud835\udc57delimited-[]\ud835\udc61u_{j}[t]italic_u start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT [ italic_t ] affects the current potential uj\u2062[t+1]subscript\ud835\udc62\ud835\udc57delimited-[]\ud835\udc611u_{j}[t+1]italic_u start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT [ italic_t + 1 ]. A higher \u03b2\ud835\udefd\\betaitalic_\u03b2 value implies less decay, enabling the neuron to retain more of its previous state, resulting in sparser behavior. Incoming weighted spikes from other neurons (wi\u2062j\u22c5si\u2062[t]\u22c5subscript\ud835\udc64\ud835\udc56\ud835\udc57subscript\ud835\udc60\ud835\udc56delimited-[]\ud835\udc61w_{ij}\\cdot s_{i}[t]italic_w start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT \u22c5 italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT [ italic_t ]) increase the potential, while a threshold-based self-decay term (sj\u2062[t]\u22c5\u03b8\u22c5subscript\ud835\udc60\ud835\udc57delimited-[]\ud835\udc61\ud835\udf03s_{j}[t]\\cdot\\thetaitalic_s start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT [ italic_t ] \u22c5 italic_\u03b8) reduces it. Equation 2 determines when the neuron fires. If the membrane potential exceeds the threshold (\u03b8\ud835\udf03\\thetaitalic_\u03b8), the neuron generates a spike (sj\u2062[t]=1subscript\ud835\udc60\ud835\udc57delimited-[]\ud835\udc611s_{j}[t]=1italic_s start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT [ italic_t ] = 1). As such a lower \u03b8\ud835\udf03\\thetaitalic_\u03b8 value reduces the potential required for firing, thereby increasing the neuron\u2019s firing frequency. \nII-B Quantization-Aware Training\n We employ Quantization-Aware Training (QAT) [9] to quantize model weights and biases into integers, with the quantization error incorporated into the loss function during training. This enables the model to adapt to the quantization constraints, enhancing its performance when operating with quantized data. During evaluation, weights and biases are fully quantized, while neuronal parameters remain in floating-point due to the current lack of support for LIF neuron quantization. The accumulated membrane data undergoes dequantization back to floating-point for accurate spiking operations. \nIII Impacts of Quantization on Sparsity\n The potential efficiency gains of low-bit quantization raise an intriguing question: can quantization influence the sparsity of an SNN model? To investigate this, we empirically compared a 32-bit floating-point network (called fp32) with its 4-bit integer (int4) counterpart, both trained using QAT. Section V details our experimental setup. Figure 1 illustrates the variability in the VGG9 model\u2019s sparsity for SVHN, CIFAR10, and CIFAR100. Our experiments revealed two key insights: Comparable accuracy with both precisions: The int4 and fp32 networks demonstrate similar accuracy varying by only 0.5%,0.4%,percent0.5percent0.40.5\\%,0.4\\%,0.5 % , 0.4 % , and 3.1%percent3.13.1\\%3.1 % for SVHN, CIFAR10, and CIFAR100, respectively. Quantization increases sparsity: Compared to fp32, int4 yields 6.1%percent6.16.1\\%6.1 %, 10.1%percent10.110.1\\%10.1 %, and 15.2%percent15.215.2\\%15.2 % fewer spikes for SVHN, CIFAR10, and CIFAR100, respectively. We hypothesize that introducing quantization noise during training implicitly encourages sparsity by selectively deactivating neurons, leading to a sparser and more efficient representation without sacrificing accuracy. These findings suggest that quantization, beyond its traditional role in reducing model size and power consumption, may also induce sparsity. This opens exciting avenues for further exploring the synergy between quantization and sparsity-aware techniques to further enhance SNN efficiency. \nIV Hybrid Hardware Architecture\n Architecture Overview: Our hybrid architecture integrates specialized dense and sparse cores connected by on-chip FPGA memories.\nCore allocation follows a layer-wise strategy, with each core\u2019s size tailored to the corresponding layer\u2019s sparsity-driven workload needs. This layer-wise resource specialization is essential due to the potential for high sparsity variability across network layers. While our evaluation focuses on specific model architectures, the principles underlying this hybrid design\u2014specialized processing cores and layer-wise resource allocation\u2014can be readily adapted to support various SNN models and computational requirements. \nIV-A Dense Core\n Figure 2 depicts the block diagram of the dense core (dc), comprising three key components: a PE array, control units, and activation units. The PE array features a fixed column of 27 processing elements (PEs) and employs a weight stationary (WS) dataflow. We chose 27 PEs because of the WS requirements, specifically to handle 3333 input channels and 3\u00d73333\\times 33 \u00d7 3 filters. This choice minimizes memory footprint requirements, as the input layer\u2019s three channels lead to fewer weights compared to input or output activations. Since we employ tiling in the output channel, we parameterize the number of rows, meaning that each row handles one output feature map. Each row then sequentially moves onto the next feature map until all output channels are processed. As a result, partial sums flow horizontally (left to right) while input image pixels flow vertically (top to down) in a systolic manner. Each PE contains a Multiply and Accumulate (MAC) unit, along with a register for weight storage. All 27 PEs work in parallel to process input channels and generate a single output membrane potential per cycle. To maintain the systolic pipeline flow, we utilize independent registers/FFs to act as \u201cshift registers\u201d to provide a fixed delay for each of the 27 inputs in the array. These registers are controlled by the Staggering Routine within the Control unit. The right-most PE\u2019s shift register depth dictates the array\u2019s pipeline depth. The Control unit is implemented as a state machine and orchestrates three main tasks: Data management: It feeds image data into the PE array. The Address Generation Routine calculates read indices for the 27 top PEs, accessing image buffers (implemented with on-chip flip-flops for their small storage footprint). This enables row-major image storage while facilitating parallel weight access by all 27 PEs. The Staggering Routine manages \u201dshift registers\u201d that control the data flow from the image buffers. Activation synchronization: It starts/stops the Activ unit using the EN signal. Once the PE pipeline is filled (i.e., the first membrane potential accumulation is produced), the Control unit triggers the activation phase (responsible for spike train generation) with the signal EN. Output channel tiling: The Control unit coordinates the tiling of output channels, meaning that each row in the systolic array strides through the output channels. When the rows of the systolic array transition to the next output feature map computation, it resets the partial sums and membrane potentials within the PE Array and Activ units using the rst signal. Finally, the unit sets the layer avail signal high to indicate the completion of layer processing. The accumulated potential from each row flows into the Activ unit, where it performs two core functions. First, it modifies the incoming potential by adding the filter bias value, applying the leakage factor (\u03b2\ud835\udefd\\betaitalic_\u03b2) to mimic the leaky behavior, and then performs thresholding. If the membrane potential exceeds the threshold (\u03b8\ud835\udf03\\thetaitalic_\u03b8), the unit subtracts the threshold value from the membrane potential and sets the associated spike to 1111; otherwise, the spike is 00. Secondly, after processing an entire output feature map, the Activ unit writes the generated spike train to the Block RAM (BRAM), which serves as the intermediate storage between layers. Spike trains in BRAM follow a timestep-major order (see Figure 2) with the spike trains for consecutive timesteps stored in contiguous addresses. For example, if the layer consists of N\ud835\udc41Nitalic_N output channels and T\ud835\udc47Titalic_T timesteps, then N\u00d7T\ud835\udc41\ud835\udc47N\\times Titalic_N \u00d7 italic_T locations in total are required for the layer\u2019s spike train storage. This layout is consistent for both dense and sparse cores. \nIV-B Sparse Core\n We adapt conventional convolution (CONV) and fully connected (FC) layers for event-driven processing by dividing their operation into spike train compression and accumulation phases. The sparse core design, illustrated in Figure 3, implements these phases and consists of two primary components: an Event Control Unit (ECU) and Neural Cores (NC). Within the ECU, the control routine is implemented as a state machine and has several key functions. Compr. Routine: First, the ECU fetches a spike train from the input Spike RAM, and then tiles the whole spike train into n\ud835\udc5bnitalic_n-bit chunks, which are processed sequentially by the Compression (Compr.) routine. The core function of this routine is to eliminate the non-spiking (i.e., \u20190\u2019) bits from the bit array and generate a compact register array called S\u2062p\u2062i\u2062k\u2062e\u2062E\u2062v\u2062e\u2062n\u2062t\u2062s\ud835\udc46\ud835\udc5d\ud835\udc56\ud835\udc58\ud835\udc52\ud835\udc38\ud835\udc63\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc60SpikeEventsitalic_S italic_p italic_i italic_k italic_e italic_E italic_v italic_e italic_n italic_t italic_s (see Figure 3). This is achieved by processing n\ud835\udc5bnitalic_n bits per cycle and using a priority encoder to identify and send the address of the first set bit (\u20191\u2019) within each chunk to the S\u2062p\u2062i\u2062k\u2062e\u2062E\u2062v\u2062e\u2062n\u2062t\u2062s\ud835\udc46\ud835\udc5d\ud835\udc56\ud835\udc58\ud835\udc52\ud835\udc38\ud835\udc63\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc60SpikeEventsitalic_S italic_p italic_i italic_k italic_e italic_E italic_v italic_e italic_n italic_t italic_s array, a common application of priority encoders, as shown in Figure 3 (right). Finally, the ECU\u2019s bit reset component sets the identified \u20191\u2019 bit back to \u20190\u2019 in the previous cycle\u2019s spike train, allowing the Compr. routine to locate the next set bit in subsequent cycles. Address Generation: After compression, the accumulation phase begins, utilizing the S\u2062p\u2062i\u2062k\u2062e\u2062E\u2062v\u2062e\u2062n\u2062t\u2062s\ud835\udc46\ud835\udc5d\ud835\udc56\ud835\udc58\ud835\udc52\ud835\udc38\ud835\udc63\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc60SpikeEventsitalic_S italic_p italic_i italic_k italic_e italic_E italic_v italic_e italic_n italic_t italic_s register array. For each spike, there are 9 associated neurons in the feature map whose membrane potentials must be updated with corresponding filter coefficients. The Address Generation routine iterates through the filter coefficients and calculates the (r\u2062o\u2062w,c\u2062o\u2062l)\ud835\udc5f\ud835\udc5c\ud835\udc64\ud835\udc50\ud835\udc5c\ud835\udc59(row,col)( italic_r italic_o italic_w , italic_c italic_o italic_l ) addresses of neurons associated with that spike event and filter size (e.g., a spike at (r\u2062o\u2062w,c\u2062o\u2062l)\ud835\udc5f\ud835\udc5c\ud835\udc64\ud835\udc50\ud835\udc5c\ud835\udc59(row,col)( italic_r italic_o italic_w , italic_c italic_o italic_l ) affects the 9 neurons from (r\u2062o\u2062w\u22123,c\u2062o\u2062l\u22123)\ud835\udc5f\ud835\udc5c\ud835\udc643\ud835\udc50\ud835\udc5c\ud835\udc593(row-3,col-3)( italic_r italic_o italic_w - 3 , italic_c italic_o italic_l - 3 ) to (r\u2062o\u2062w,c\u2062o\u2062l)\ud835\udc5f\ud835\udc5c\ud835\udc64\ud835\udc50\ud835\udc5c\ud835\udc59(row,col)( italic_r italic_o italic_w , italic_c italic_o italic_l )). These (r\u2062o\u2062w,c\u2062o\u2062l)\ud835\udc5f\ud835\udc5c\ud835\udc64\ud835\udc50\ud835\udc5c\ud835\udc59(row,col)( italic_r italic_o italic_w , italic_c italic_o italic_l ) signals are then directed to the neural cores (NCs). Importantly, the Compr. and accumulation phases execute in parallel, enabling the compression of new spike trains from subsequent input feature maps while simultaneously processing earlier feature map spike events. With the (r\u2062o\u2062w,c\u2062o\u2062l)\ud835\udc5f\ud835\udc5c\ud835\udc64\ud835\udc50\ud835\udc5c\ud835\udc59(row,col)( italic_r italic_o italic_w , italic_c italic_o italic_l ) pairs provided, the Accum routine within each NC performs the accumulation phase of the LIF neuron. It reads the membrane potential value from the BRAM, updates them with corresponding coefficient weights, and writes back the result to the BRAM. Similar to the dense core, the sparse cores also support quantization. Also, note that both the Address Generation and Accum routines are fully pipelined and can update one neuron per cycle. Filter weights for CONV layers are stored in BRAM and LUTRAM, while larger fully-connected (FC) weights use Ultra RAMs (URAMs) for their higher density and energy efficiency. After processing all input feature maps, the control unit enables the NC\u2019s Activ. routine to initiate the LIF neuron\u2019s spiking phase. The architecture unrolls the output channels by a factor of N\ud835\udc41Nitalic_N, defined as a top-level parameter, to determine the number of NC instances. Each NC instance strides through the output channels by N\ud835\udc41Nitalic_N. For instance, an NC instance with index 2222 and N=8\ud835\udc418N=8italic_N = 8 will process OFMs with indices 1, 9, 17, etc. The architecture also features max-pooling on spikes, which aligns better with SNN temporal dynamics and minimizes information loss during downsampling, compared to other pooling methods [10]. Implementation on binary feature maps only requires sliding an OR gate over an N\u00d7N\ud835\udc41\ud835\udc41N\\times Nitalic_N \u00d7 italic_N input area, where N\ud835\udc41Nitalic_N is the downsampling ratio. \nIV-C Memory Optimization\n For efficient resource usage, we only use the FPGA\u2019s on-chip memory (BRAM, URAM, LUTRAM) for storing model parameters and spike trains, avoiding the external DDR memory. Early layers with smaller convolutions use LUTRAM, which is more efficient than flip-flops for small RAMs due to its flexibility, despite being less abundant than flip-flops in Xilinx UltraScale+ devices. *Instance-level dynamic power. Static power is 3.13\u2062W3.13\ud835\udc4a3.13W3.13 italic_W and 3.22\u2062W3.22\ud835\udc4a3.22W3.22 italic_W for int4 and fp32 respectively. Our strategy for aggressive on-chip storage, while beneficial, comes with a trade-off in power consumption. Since a large portion of this storage remains inactive during layer execution (until NC reaches the next output channel), the associated weight data is not used. To address this, we implement clock gating for our memory units (right side of Figure 3). The memory is partitioned into two regions, with the most significant bit (MSB) of the address line controlling the active region for both reads and writes. An AND gate controls the clock signal to the memory unit, ensuring that only the active region receives clock cycles. While this on-chip storage strategy works well for quantized models like int4, it poses limitations for fp32 models. Specifically, the high LUT usage for weight storage prevents us from scaling beyond the custom VGG9 architecture or deploying larger models. \nIV-D Quantization Support\n Due to the lack of native quantization support, the design incorporates floating-point computation capabilities in both dense and sparse cores to de-quantize weights and biases retrieved from memory. We utilize a resource-efficient shift-and-add approach instead of DSP blocks to perform multiplications by constants [11], improving overall efficiency. \nV Experimental Results\n In this section, we first detail our experimental setup and leverage our novel hybrid architecture to analyze the benefits of quantization (resource utilization, energy, power, and latency). We then compare direct and rate coding, demonstrating the superiority of direct coding. Finally, we compare our hardware against two recent and relevant works. \nV-A Experimental Setup\n We implemented our proposed hardware using SystemVerilog and synthesized on a Xilinx Virtex\u00ae UltraScale+\u2122 XCVU13P FPGA. For evaluation, we use the CIFAR-10, CIFAR-100, and Street View House Numbers (SVHN) datasets with a modified VGG9 network trained using snnTorch [12]. The network, trained using surrogate gradients [13], has a structure: 64C3-112C3-MP2-192C3-216C3-MP2-480C3-504C3-560C3-MP2-1064-P\ud835\udc43Pitalic_P. X\ud835\udc4bXitalic_XCY\ud835\udc4cYitalic_Y denotes X\ud835\udc4bXitalic_X filters of size Y\u00d7Y\ud835\udc4c\ud835\udc4cY\\times Yitalic_Y \u00d7 italic_Y and MPZ\ud835\udc4dZitalic_Z denotes Z\u00d7Z\ud835\udc4d\ud835\udc4dZ\\times Zitalic_Z \u00d7 italic_Z maxpooling. P\ud835\udc43Pitalic_P represents the number of neurons (i.e., population) in the network\u2019s output layer. Prior work [14] has shown that using a population of neurons in the output layer can lead to better accuracy, even with fewer time steps. That is, we can make the output layer larger instead of needing more time steps to achieve good results. Through experimentation, we found that the following P\ud835\udc43Pitalic_P values provided the best accuracy with the minimum possible spike train length: P=1000\ud835\udc431000P=1000italic_P = 1000 for CIFAR10 and SVHN datasets, and P=5000\ud835\udc435000P=5000italic_P = 5000 for CIFAR100. We selected the network architecture to ensure compatibility with the Virtex UltraScale+ platform, following prior work [6]. We tuned the leaky neuron hyperparameters (\u03b2=0.15\ud835\udefd0.15\\beta=0.15italic_\u03b2 = 0.15 and \u03b8=0.5\ud835\udf030.5\\theta=0.5italic_\u03b8 = 0.5) and used layer-wise batch normalization to prevent overfitting. Hardware configurations: For each dataset, we have one lightweight (LW) and two performance-optimized configurations (perf2 and perf4). The LW baseline prioritizes minimal resource usage while ensuring balanced layer-wise execution latency of the network. The two performance-optimized versions scale up resources by factors of 2\u00d72\\times2 \u00d7 and 4\u00d74\\times4 \u00d7, respectively. To determine the optimal lightweight configuration, we modeled the trained network\u2019s layer-wise workload distribution using Equation 3: where F is the number of filter coefficients (e.g., 9999 for 3\u00d73333\\times 33 \u00d7 3), C_out is the number of output channels, and S\u2062_\u2062i\ud835\udc46_\ud835\udc56S\\_iitalic_S _ italic_i is the number of spikes for input feature map i\ud835\udc56iitalic_i. The spike information was acquired empirically by running the network once on the hardware. Our goal was to partition resources to minimize the execution latency difference between the most and least workload-intensive layers. All three configurations were synthesized to operate at a 100 MHz clock frequency. \nV-B Resource Utilization\n Table I summarizes the implementation results of our proposed hardware for CIFAR100 (in perf2), which are representative of the trends observedfor CIFAR10 and SVHN. We compare the results of full-precision (fp32) with its 4-bit integer counterpart (int4). We empirically determined that a (1,28,12,54,16,72,70,19,4)1281254167270194(1,28,12,54,16,72,70,19,4)( 1 , 28 , 12 , 54 , 16 , 72 , 70 , 19 , 4 ) configuration (neural cores allocated per layer) yields the most balanced execution profile (layer overheads: 0.9%, 13.4%, 13.6%, 13.8%, 12.8%, 12.3%, 12.9%, 15.6%, 4.8%). The int4 implementation demonstrates significant resource savings compared to fp32, using 8\u00d78\\times8 \u00d7 fewer LUTs, primarily due to efficient LUTRAM usage for the CONV_1_2 weight data. Additionally, int4 uses 3.4\u00d73.4\\times3.4 \u00d7 fewer BRAMs/URAMs due to the minimum 8-bit width configuration of BRAM primitives for CONV layers. Overall, the fp32 and int4 designs occupy 24%percent2424\\%24 % and 34%percent3434\\%34 % of the FPGA\u2019s LUT resources, respectively. Importantly, the fp32 hardware consumes 2.82\u00d72.82\\times2.82 \u00d7 more power than the int4 configuration, highlighting the power efficiency benefits of quantization. 1\u2019\u2014\u2019 indicates no reported results in prior work. \nV-C Energy Analysis\n Our energy analysis comparing int4 and fp32 (Figure 4) reveals significant energy savings benefits of quantization across CIFAR10, CIFAR100, and SVHN datasets. We calculate the energy expenditure per image by summing the energy per layer. For CIFAR10 and CIFAR100, int4 reduces the average energy by 3.4\u00d73.4\\times3.4 \u00d7 and 1.7\u00d71.7\\times1.7 \u00d7, respectively, across all configurations. The majority of these savings (roughly 3\u00d73\\times3 \u00d7 for CIFAR10 and 1.5\u00d71.5\\times1.5 \u00d7 for CIFAR100) stem directly from int4\u2019s inherent power advantage. Additionally, the fp32 designs exhibit a higher spike count (1.1\u00d71.1\\times1.1 \u00d7 and 1.15\u00d71.15\\times1.15 \u00d7 for CIFAR10 and CIFAR100, respectively), contributing a further 10\u221215%10percent1510-15\\%10 - 15 % to the energy difference. For SVHN, the scaling trend differs slightly due to the resource allocation dominance of the CONV_1_2 layer (with more allocated neural cores), leading to a smaller energy gap between fp32 and int4. The perf4 configuration of the quantized hardware achieves a 28%percent2828\\%28 % energy reduction compared to its LW counterpart, contrasting with the 52%percent5252\\%52 % reduction in the full-precision design. Since the full-precision hardware utilizes LUTRAM for this layer, increasing the NC count has a less pronounced impact on power compared to the quantized hardware. Overall, quantization yields a significant 4\u00d74\\times4 \u00d7 reduction in latency and notable energy savings across all datasets. \nV-D Direct vs Rate coding comparison\n Table II depicts our comparison of direct coding and rate coding on CIFAR10 using the quantized LW configuration. Because the rate-coded network receives spikes as inputs, it only needs sparse cores, while the direct-coded network needs our hybrid architecture. As such, although both networks run on our hardware architecture, for a fair comparison, we turned off the dense core for the rate-coded network to prevent unnecessary power consumption. The Total Spikes represents the spikes across all timesteps. With only 2 timesteps, direct coding yielded much higher sparsity (2.6\u00d7\\times\u00d7 fewer spikes) and achieved 10%percent1010\\%10 % higher accuracy than rate coding at 25 timesteps. Further increasing the timesteps plateaued the accuracy for both schemes111To enable reproducibility of our accuracy and implementation, our training and hardware code are available at https://github.com/githubofaliyev/SNN-DSE/tree/DATE25. Importantly, the direct-coded network consumed 26.4\u00d726.4\\times26.4 \u00d7 less energy than its rate-coded counterpart, contrary to prior work [3]. \nV-E Comparison to previous work\n Table III compares our proposed hybrid architecture\u2019s dynamic power and throughput to two recent related works: an event-driven design with quantization support [15] and a resource-efficient approach [7]. We focus on dynamic power and throughput as these metrics were reported in prior works. Compared to [15], we achieve more than 2\u00d7\\times\u00d7 the throughput for SVHN and CIFAR10, with a power increase of 2.2\u00d72.2\\times2.2 \u00d7 and 1.8\u00d71.8\\times1.8 \u00d7, respectively (Table III). In addition to using a lower power board, [15] obtained lower power measurements by subtracting the board\u2019s baseline power consumption from the accelerator\u2019s total runtime power. Additionally, we outperform their work in inference accuracy (4%percent44\\%4 % and 8%percent88\\%8 % higher for SVHN and CIFAR10, respectively). Compared to [7], our perf4 configuration on CIFAR100 achieves 51\u00d751\\times51 \u00d7 throughput improvement and 2\u00d72\\times2 \u00d7 power savings, with only a 3.1%percent3.13.1\\%3.1 % accuracy decrease. This work is the most relevant comparison to ours due to the use of similar implementation platforms. \nVI Conclusion\n This paper presented a novel hybrid architecture that strategically combines dense and sparse cores, optimized for the unique workload characteristics of direct-coded SNNs. Our results demonstrate the significant advantages of direct coding, achieving both superior accuracy and hardware performance compared to traditional rate coding. We also highlight the benefits of quantization, showcasing its impact on sparsity and efficiency. The insights from this work lay the foundation for co-design approaches where SNN algorithms and hardware architectures are co-developed to maximize performance and enable more efficient SNN accelerators. Future research will further explore direct coding through expanded evaluations with diverse models and datasets, including larger-scale networks that may require external memory access. While our current implementation focuses on models that fit within on-chip BRAM, additional studies are needed to analyze performance impacts when incorporating off-chip memory access for broader model support. This includes investigating techniques to minimize external memory bandwidth requirements and optimize the memory hierarchy for larger networks. Acknowledgment This work was partially supported by National Science Foundation Grant 1844952. Any opinions, findings, conclusions or recommendations expressed are those of the authors and do not necessarily reflect the views of the NSF. References"}
{"text": "A Data-Driven Approach to Dataflow-Aware Online\u00a0Scheduling\u00a0for\u00a0Graph\u00a0Neural\u00a0Network\u00a0Inference Graph Neural Networks (GNNs) have shown significant promise in various domains, such as recommendation systems, bioinformatics, and network analysis. However, the irregularity of graph data poses unique challenges for efficient computation, leading to the development of specialized GNN accelerator architectures that surpass traditional CPU and GPU performance. Despite this, the structural diversity of input graphs results in varying performance across different GNN accelerators, depending on their dataflows. This variability in performance due to differing dataflows and graph properties remains largely unexplored, limiting the adaptability of GNN accelerators.\nTo address this, we propose a data-driven framework for dataflow-aware latency prediction in GNN inference. Our approach involves training regressors to predict the latency of executing specific graphs on particular dataflows, using simulations on synthetic graphs. Experimental results indicate that our regressors can predict the optimal dataflow for a given graph with up to 91.28% accuracy and a Mean Absolute Percentage Error (MAPE) of 3.78%. Additionally, we introduce an online scheduling algorithm that uses these regressors to enhance scheduling decisions. Our experiments demonstrate that this algorithm achieves up to 3.17\u00d73.17\\times3.17 \u00d7 speedup in mean completion time and 6.26\u00d76.26\\times6.26 \u00d7 speedup in mean execution time compared to the best feasible baseline across all datasets. \n1. Introduction GNNs have emerged as powerful tools for tackling a diverse range of tasks in fields of a relational nature, such as recommendation systems, bioinformatics, network analysis and quantum computing. Graph data is inherently irregular when compared to other sources, such as images or text. This irregularity introduces unique challenges related to the data movement and memory accesses involved in computing GNNs. These unique challenges have motivated the rise of specific GNN accelerator architectures which outperform conventional CPU and GPU acceleration, allowing for a much faster computation enabling low-latency GNN applications (Abadal et\u00a0al., 2021). Despite the advancements in accelerator design for GNNs, the structural diversity of the input graphs still poses a substantial challenge. Variations in the graph\u2019s structure can lead to significant performance fluctuations across different dataflow strategies employed by accelerators. In Figure\u00a01, we show the distribution of optimal dataflows in terms of latency for an example dataset among the considered possible configurations that will be introduced in the rest of this work. It can be seen that even for graphs belonging to the same domain, the selection of the best configuration is not trivial, and there is no one-size-fits-all solution. In response to this issue, we propose a novel framework for data-driven dataflow-aware latency prediction for GNN inference. Our framework leverages lightweight parametric regressors trained on a large dataset of synthetic graphs to predict the latency of GNN inference under various dataflow strategies. This predictive capability allows for the optimization of GNN execution by selecting the most suitable dataflow strategy for a given graph structure. Additionally, we introduce an online scheduling algorithm that utilizes our latency prediction framework to dynamically allocate GNN inference tasks across heterogeneous accelerators. Our contribution can be summarized in the following points: A new dataset generation method for GNN inference computation on different accelerator dataflows. A novel model for efficient latency estimation. A predictor-informed online scheduling algorithm for heterogeneous multi-accelerator settings. We conduct extensive experiments to evaluate the effectiveness of our proposed solutions. On the one hand, we test the predictors across multiple axes by measuring the regression accuracy, ranking accuracy and the latency of a single configurable accelerator computing the GNN with the best dataflow according to the model. Experiments show that our model achieves up to 91.28%percent91.2891.28\\%91.28 % accuracy and 3.783.783.783.78 regression MAPE in predicting the best configuration. This leads to a notable single accelerator improvement of up to 93.63%percent93.6393.63\\%93.63 %, a 58.42%percent58.4258.42\\%58.42 % improvement over the best fixed configuration for the dataset and 0.56%percent0.560.56\\%0.56 % degradation over optimal on both real graph and synthetic datasets. On the other hand, we evaluate the performance of our online scheduling algorithm within a heterogeneous multi-accelerator environment where GNNs that arrive in a queue for computation are assigned to an accelerator. The experimental results reveal significant gains in scheduling performance with a 3.17\u00d73.17\\times3.17 \u00d7 speedup in mean completion time, 6.26\u00d76.26\\times6.26 \u00d7 speedup in mean execution time and more than 1000\u00d71000\\times1000 \u00d7 speedup in turnaround time against the best feasible baseline on both synthetic and real-world datasets. \n2. Background and Motivation \n2.1. Graph Neural Networks GNNs are a family of algorithms designed to apply machine learning techniques to relational data, falling under the broader category of geometric deep learning algorithms. The GNN model is structured in layers, where each layer\u2019s input is the node representation produced by the preceding layer. For the first layer, the node representation is provided by a feature vector associated with each node. Some of the most commonly used GNN models include the Graph Isomorphism Network (GIN)\u00a0(Hamilton et\u00a0al., 2017), Graph Attention Network (GAT)\u00a0(Veli\u010dkovi\u0107 et\u00a0al., 2018), and Graph Convolutional Network (GCN)\u00a0(Kipf and Welling, 2016). The processing of each layer, as depicted in Figure\u00a02, is divided into two phases. The first phase, known as the aggregation phase, involves a Sparse-Dense Matrix Multiplication (SpMM) between the graph adjacency matrix A\ud835\udc34Aitalic_A, where ai,j=1subscript\ud835\udc4e\ud835\udc56\ud835\udc571a_{i,j}=1italic_a start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = 1 if nodes i\ud835\udc56iitalic_i and j\ud835\udc57jitalic_j are connected, and the set of node features from the previous iteration, H(k\u22121)superscript\ud835\udc3b\ud835\udc581H^{(k-1)}italic_H start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT. The graph adjacency matrix is typically stored in Compressed Row Storage (CRS) format to optimize space and computation efficiency (Garg et\u00a0al., 2022; Yan et\u00a0al., 2020; Geng et\u00a0al., 2020). The subsequent phase, referred to as the combination phase, consists of a Dense-Dense Matrix Multiplication (GEMM) between the intermediate matrix and a weight matrix W\ud835\udc4aWitalic_W. While matrix multiplication is one common method for processing GNNs, another approach is message passing, where nodes communicate with their neighbors to update their representations. The key dimensions in the computations are: the number of nodes V\ud835\udc49Vitalic_V, the number of node input features F\ud835\udc39Fitalic_F, the number of node output features G\ud835\udc3aGitalic_G, and the degree (number of neighbors) of each node Nvsubscript\ud835\udc41\ud835\udc63N_{v}italic_N start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT. \n2.2. Spatial Accelerators and GNN Dataflows Graph data is irregular when compared to image, audio and tabular sources. This motivated the rise of specific GNN accelerators (Yan et\u00a0al., 2020, 2020; Sarkar et\u00a0al., 2023) featuring either fixed dataflows or adaptive capabilities, which outperform conventional GPU and CPU execution enabling high performance thanks to parallelization and energy efficiency due to high specialization. Differently from traditional DNN workloads such as fully connected and convolutional layers (Sze et\u00a0al., 2017) , a layer of a GNN is organized in two phases, and an intra-phase dataflow must be selected for each of them. Figure\u00a03a shows the loop nest representation of the main computation of a GNN layer. Aggregation and combination are implemented as two loop nests executed sequentially. The matrices involved are the same as in Figure\u00a02. In each phase, the intra-phase dataflow is specified by the loop ordering and the spatial loop tiling and unrolling. For the latter, the parallel-fors describe the parallel execution of a loop on several Processing Elements (PEs) by unrolling a specific dimension of the involved matrices. In the aggregation phase, the tile sizes are T_Va, T_Fa and T_N, representing the tile sizes for the V\ud835\udc49Vitalic_V, F\ud835\udc39Fitalic_F and N\ud835\udc41Nitalic_N dimensions, respectively. The number of maximum utilized PEs is T_Va\u00d7T_Fa\u00d7T_NT_VaT_FaT_N\\texttt{T\\_Va}\\times\\texttt{T\\_Fa}\\times\\texttt{T\\_N}T_Va \u00d7 T_Fa \u00d7 T_N. However, notice that the Nvsubscript\ud835\udc41\ud835\udc63N_{v}italic_N start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT dimension is the number of neighbors and depends on v\u2208\ud835\udcb1\ud835\udc63\ud835\udcb1v\\in\\mathcal{V}italic_v \u2208 caligraphic_V, hence while iterating on low-degree nodes, the spatial accelerator may be underutilized with high T_Ns. In the combination phase, the loop nest represents a dense matrix multiplication, and the tiling factors are T_Vc, T_Gc and T_Fc, representing the tile sizes for the V\ud835\udc49Vitalic_V, G\ud835\udc3aGitalic_G and F\ud835\udc39Fitalic_F dimensions respectively. In this phase, the static number of utilized PEs is equal to T_Vc\u00d7T_Gc\u00d7T_FcT_VcT_GcT_Fc\\texttt{T\\_Vc}\\times\\texttt{T\\_Gc}\\times\\texttt{T\\_Fc}T_Vc \u00d7 T_Gc \u00d7 T_Fc. The global buffer and the PEs are interconnected through a Network-on-Chip (NoC) for data movement. According to the unrolling dimensions, data movement between PEs may be needed for spatial reduction, i.e. accumulation of partial sums, or multicast capabilities could be useful to reduce memory accesses. Usually, each dataflow requires specific microarchitectural implementation. Furthermore, the dataflows of the two phases are interdependent. The interaction between the aggregation and combination phases is described by the inter-phase dataflow and determines the number of memory accesses needed to move data from one phase to the other. In this work, we adopt the sparse-dense workloads dataflow taxonomy proposed by Garg et al. (Garg et\u00a0al., 2022). Figure\u00a03 shows the space-time diagram of three different inter-phase dataflows:\n(i) Sequential\u00a0(seq). With this configuration, the two phases are run sequentially similarly to two DNN layers. The output of the first phase (intermediate matrix) is written to memory and then loaded back to PEs for the next phase. The size of the intermediate matrix is V\u00d7F\ud835\udc49\ud835\udc39V\\times Fitalic_V \u00d7 italic_F; hence, as shown in Figure\u00a03b, for large graphs, the intermediate matrix cannot be stored entirely in the global buffer, requiring additional accesses to the off-chip DRAM causing higher energy consumption.\n(ii) Sequential Pipeline\u00a0(sp). Applying loop fusion and temporal tiling techniques to the Sequential loop nest, the execution of the Aggregation and Combination can be split in smaller steps temporally interleaved. As shown in Figure\u00a03c, at each step, a small tile of the intermediate matrix is calculated and stored in the global buffer or even in PE local buffers\nto be used in the next phase step without accessing higher level memory.\n(iii) Parellel Pipeline\u00a0(pp). As shown in Figure\u00a03d, the two phases can also be executed in parallel. This requires partitioning PEs between two phases, the global buffer working as a ping-pong buffer and the NoC supporting the dual data movement. Balancing the two pipeline stages is crucial to avoid stall cycles. In particular, the intermediate matrix tile production rate of the aggregation phase should be equal to the combination consumption rate. Achieving this can be challenging, especially with heterogeneous node degree distributions. Similarly to the traditional DNN acceleration domain\u00a0(Garg et\u00a0al., 2022), multiple accelerators, each featuring a diverse set of supported dataflows and flexibility degrees, can be combined together to compose a heterogeneous multi-accelerator system such as the one shown in Figure\u00a04. Such systems can be very effective in highly parallel use cases, allowing multi-dataflow execution without the hardware overhead needed in highly flexible single-accelerator systems. \n2.3. Motivation Despite advancements in accelerator design for GNNs, the main challenge in both flexible dataflow single-accelerator and multi-accelerator heterogeneous dataflow systems for GNNs remains the following: differently from DNN workloads, featuring fixed tensor sizes independent of the input, in GNNs, the amount of computation involved and consequently the size and structure of the matrices involved varies on the specific input graph (especially V\ud835\udc49Vitalic_V and Nvsubscript\ud835\udc41\ud835\udc63N_{v}italic_N start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPTs). Variations in graph structures can cause substantial performance differences across dataflow strategies used by accelerators. Thus, while for DNN workloads, an optimal dataflow choice for each layer can be determined offline and adopted in deployment for all the situational inputs (Mei et\u00a0al., 2021; Jung et\u00a0al., 2023) , for GNNs there is no one-fits-all solution. The optimal configuration of intra-phase dataflow and inter-phase dataflow has to be determined dynamically for each input dataset as it depends on the dataset characteristics. As shown in Figure\u00a05, even for graphs of the same dataset, the optimal configuration in terms of latency varies, and different datasets exhibit different overall optimal configuration distributions. The problem we aim to address in this work is predicting the latency of computing a GNN inference on a specific accelerator configuration. Having this knowledge prior to execution enables better computational decisions around GNN workloads. We are especially interested in using those predictions to rank configurations to select which is the best configuration for a given GNN. For a more specific application, we consider the scenario of scheduling a set of jobs, namely GNN layers, each having a release time rjsubscript\ud835\udc5f\ud835\udc57r_{j}italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT (arrival time) not known in advance, in order to minimize the average completion times \u00a0(Phillips et\u00a0al., 1998). The objective is to address the real-time scheduling problem for heterogeneous multi-accelerator setups. This real-time scheduling scenario has been extensively investigated for traditional DNN workloads, with particular emphasis on Inference-as-a-Service and Multi-Tenant Data Center applications\u00a0(Kim et\u00a0al., 2023; Blanco et\u00a0al., 2024).\nThe problem can be considered as unrelated parallel machine scheduling with job release times, which is NP-hard even in an offline setting and with arrival times known a priori\u00a0(Schulz and Skutella, 2002; Lenstra et\u00a0al., 1977). Leveraging the proposed latency predictor, machine-learning based speed-oblivious online scheduling techniques can be adopted\u00a0(Lindermayr et\u00a0al., 2023). \n3. Data-driven GNN Dataflow Selection We address the challenge of predicting the latency of executing a GNN with a specific accelerator configuration (Section 2.3) by modeling dataflow selection as a learning problem. For an intra-dataflow \u03c9\u2208\u03a9\ud835\udf14\u03a9\\omega\\in\\Omegaitalic_\u03c9 \u2208 roman_\u03a9 and an inter-phase dataflow \u03b4\u2208\u0394\ud835\udeff\u0394\\delta\\in\\Deltaitalic_\u03b4 \u2208 roman_\u0394 our objective is to select the set of parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8 attaining the best approximation for the latency l\ud835\udc59litalic_l of execution Knowing the latency before execution enables making dataflow-aware decisions for a collection of graphs. In this section, we first explore how to successfully find \u03b8\ud835\udf03\\thetaitalic_\u03b8 using supervised learning, and we then provide experimental results supporting our claims. \n3.1. Learning to Predict Latency In order to learn the GNN and configuration to latency mapping \u03c0\u03b8:\ud835\udca2\u00d7\u03a9\u00d7\u0394\u2192\u211d:subscript\ud835\udf0b\ud835\udf03\u2192\ud835\udca2\u03a9\u0394\u211d\\pi_{\\theta}:\\mathcal{G}\\times\\Omega\\times\\Delta\\rightarrow\\mathbb{R}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT : caligraphic_G \u00d7 roman_\u03a9 \u00d7 roman_\u0394 \u2192 blackboard_R effectively for any dataset, we require: (i) a significant amount of data with variability in the feature space of the graphs \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G topology, and (ii) the latency values for such GNN data across all accelerator configurations in \u03a9\u00d7\u0394\u03a9\u0394\\Omega\\times\\Deltaroman_\u03a9 \u00d7 roman_\u0394. We satisfy the requirements by combining two tools. For the graph level variability, we use Graphlaxy (Wassington and Abadal, 2022), a graph dataset generator where the resulting datasets are spread in the feature space of the graphs. We obtain the latencies of executing a graph across different accelerator configurations with STONNE-Omega (Garg et\u00a0al., 2022), a framework for accurate simulation of the latency l^\u2062(\ud835\udca2,\u03c9,\u03b4)^\ud835\udc59\ud835\udca2\ud835\udf14\ud835\udeff\\hat{l}(\\mathcal{G},\\omega,\\delta)over^ start_ARG italic_l end_ARG ( caligraphic_G , italic_\u03c9 , italic_\u03b4 ) of executing a graph \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G with an intra-dataflow \u03c9\ud835\udf14\\omegaitalic_\u03c9 and an inter-phase dataflow \u03b4\ud835\udeff\\deltaitalic_\u03b4. By simulating the Graphlaxy graphs on STONNE-Omega we can generate a comprehensive dataset with variability in both \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G and \u03c9,\u03b4\ud835\udf14\ud835\udeff\\omega,\\deltaitalic_\u03c9 , italic_\u03b4 axis. Across the dataset, we select the parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8 that satisfy Our model \u03c0\u03b8subscript\ud835\udf0b\ud835\udf03\\pi_{\\theta}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT begins with a graph encoding step where metrics of interest are extracted from the graph \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G. These metrics include conventional graph measures such as the number of nodes, density, and clustering coefficient, along with custom metrics engineered with the learning goal in mind (see Table\u00a01 for a complete list of definitions). The encoding step is followed by a learnable non-linear transformation. We find gradient boosting methods to be the best-suited models as they provide accurate results with very fast training and inference. These methods proved particularly effective in handling the large variability of latency magnitude, outperforming other approaches, such as MultiLayer Perceptrons (MLPs), which required accurate data normalization to achieve comparable results. Additionally, we find that using separate models for each configuration results in more accurate predictions. By using this approach, we can effectively predict the latency of GNN executions across various accelerator configurations, allowing for optimized scheduling and resource allocation in heterogeneous computing environments. \n3.2. Experiments on Latency Prediction In the following paragraphs, we present a comprehensive evaluation of our approach to predicting GNN execution latency using datasets generated by Graphlaxy for training. We experiment on 24 different configurations of STONNE-Omega given by the combination of the 3 inter-phase dataflows introduced in Sec.\u00a02.2 and 8 tiling schemes. These schemes are reported in Table\u00a02 and were selected based on insights from\u00a0(Garg et\u00a0al., 2022), considering different granularities and both spatial and temporal aggregation of neighbours. For all the tilings, the \u2217*\u2217 symbol indicates that PE utilization is maximized using T_Va and T_Vc. In our STONNE-Omega simulations, we employ the same accelerator architecture as in\u00a0(Garg et\u00a0al., 2022), comprising 512 PEs, each equipped with a 64B local register file. We consider GCN\u00a0(Kipf and Welling, 2016) as the GNN model for our experiments. We use lightGBM\u00a0(Ke et\u00a0al., 2017) for the gradient boosting method implementation. We propose two objectives for this area of experimentation. The first objective is to evaluate how well models trained on labels from executions of Graphlaxy graphs predict the latencies of unseen graphs coming from both Graphlaxy and other distributions, including graphs from real-world datasets. We evaluate the quality of the regressions by measuring the MAPE. The lower the MAPE, the tighter the predictions obtained by the model. The second objective is to assess the impact of using the trained models to predict the best configuration for a single accelerator. In order to measure how good are the best configurations suggested by the model we measure the top-k accuracy of the predicted best configuration being on the top-k of true best accelerators. For deeper insight into what improvement would this selections achieve we compare the latencies of a flexible accelerator guided by our model against three benchmarks for alternative dataflow selection strategies: (i) random, selecting a random configuration for each graph; (ii) best fixed, selecting only one configuration which works best across the specific dataset; (iii)\u00a0optimal, selecting the true best configuration for each graph. We use three Graphlaxy datasets simulated on 24 STONNE-Omega configurations: small (243243243243 graphs, avg. 10.4910.4910.4910.49 nodes), medium (9,17191719,1719 , 171 graphs, avg. 241.95241.95241.95241.95 nodes), and large (6,38163816,3816 , 381 graphs, avg. 782.36782.36782.36782.36 nodes). We also include datasets derived from various real-world distributions, including 43 graphs representing real-world networks sourced from the SuiteSparse Matrix Collection\u00a0(Kolodziej et\u00a0al., 2019), and 40 graphs from the QOPTLib\u00a0(Osaba and Villar, 2023) dataset, which encompasses quantum combinatorial optimization problem representations. Additionally, we consider 600 enzyme representations, 1113 protein representations, and a collection of 2000 graphs representing Reddit discussions from\u00a0(Morris et\u00a0al., 2020). Furthermore, we include the Pattern and Travelling Salesman Problem (TSP) semi-synthetic datasets as described in\u00a0(Dwivedi et\u00a0al., 2022). The training and validation are done exclusively on the Graphlaxy graphs with the objective of obtaining models with low graph structure bias. We utilize a (70, 15, 15) split for training, validation and testing on the Graphlaxy datasets. All other non-Graphlaxy datasets are considered exclusively for testing. Table 3 shows the experimental results in latency prediction metrics for the test sets. For regression quality, we observe a very small MAPE under 5%percent55\\%5 % in medium and large in-distribution graphs, the error increases for out-of-distribution graphs and also for smaller graphs independently. The models are able to predict the best configuration out of the 24 possible configurations at different rates across different datasets. There are significant improvements from predicting the best configuration to predicting a Top-3 configuration, the magnitude of this improvements varies across datasets. We find that a smaller MAPE does not always lead to better Top-k accuracy; for instance, the TSP dataset has a larger MAPE than Enzimes but more than double the Top-k accuracy. The improvement of a model-guided flexible accelerator against random choices is large and consistent, over 80%percent8080\\%80 % across all datasets. Against the best fixed configuration, the model-guided flexible accelerator yields improvements for 8 out of 10 datasets. Naturally, fixed configurations show good performance for datasets with more homogeneous graphs, such as Enzymes and Pattern. Improvements over the best fixed configuration are also smaller for datasets with smaller graphs. Our method performs 5%percent55\\%5 % or less from optimal in half of the datasets and under 15%percent1515\\%15 % from optimal in all datasets. The consistency in improvement over random and degradation over best is an indicator that, even when the models do not predict a Top-1 or Top-3 configuration all the time, there is a small performance difference between the predicted configuration and the best selection and a significant difference between the predicted configuration and a random choice. Table 4 shows ablation results in justification of our modeling decisions. We observe that the base model requires some iteration in order to achieve the reported performance. Both regression and selection capabilities improve when adding synthetic features and when applying a logarithm transformation. \n4. Online Scheduling As introduced in the problem statement (Section 2.3), we present a specific real-world application for our methodology of learning to predict latency. Consider an online scheduling scenario where we want to schedule a set of jobs with unknown arrival times under a Pareto distribution into heterogeneous accelerators to minimize the average completion times \u00a0(Phillips et\u00a0al., 1998). In this section, we incrementally introduce an online scheduling strategy to leverage the latency prediction models from Section 3.1 along with a detailed experimental comparison against a selection of benchmarks. \n4.1. Latency Prediction Guided Online Scheduling In the context of online scheduling, the Shortest Job First (SJF) heuristic is particularly effective because it minimizes the average waiting time for jobs. By prioritizing shorter jobs, SJF ensures that more tasks are completed in a given time frame, thereby reducing the overall time jobs spend in the queue. This approach is beneficial in environments with heterogeneous accelerators where job duration can vary significantly. The rationale behind SJF\u2019s effectiveness lies in its ability to keep the system\u2019s load balanced and prevent long jobs from delaying the completion of shorter ones. SJF can significantly outperform other scheduling algorithms in terms of minimizing average completion time, especially under high system loads and diverse job sizes (Schrage, 1968). In most real scenarios we do not know the length of the job before execution. In practise, we use estimates of the job length to construct approximate SJF variants. A simple approach in a graph context is to use the number of edges or nodes to determine the length of a job. Such information on the graph size is useful for load balancing between accelerators. Nevertheless, the previous approach has no information about the specific graph-accelerator performance. We can overcome this limitation by using the latency prediction models as estimators for the job length. This approach provides a more accurate dataflow-aware estimation of the job length. \n4.2. Experiments on Online Scheduling We assess the performance of online scheduling algorithms by measuring three metrics on jobs:\n(1) Mean Completion Time, when a job finishes;\n(2) Mean Turnaround Time, difference between finishing time and arrival time;\n(3) Mean Execution Time, difference between finishing time and starting time.\nWe compare the following strategies to make scheduling decisions:\n(i) Random;\n(ii) FCFS, first come first serve;\n(iii) LIFO, last come first serve;\n(iv) SJF-Nodes, least nodes next;\n(v) SJF-Edges, least edges next;\n(vi) SJF-Truth, ground truth shortest job next;\n(vii) Ours or SJF-Predicted, the shortest job next based on latency predicted by the proposed model.\nNote that SJF-Truth is the equivalent of the SJF heuristic introduced in Section 4.1 but is not feasible in practise since it requires knowledge about the true cost of executing a job in a processor. Our method serves as an approximation of SJF truth, which is feasible in practise. We compose the online scheduling dataset for the experimentation. This dataset contains all the non-Graphlaxy graphs from the datasets from Section 3.2. The inter-arrival times are randomly drawn from a Pareto distribution, yielding a mean utilization rate of 85%percent8585\\%85 % across the included accelerator settings, and each graph corresponds to a job. We consider a first scenario where we have a set of 3 accelerators (one for each inter-phase dataflow) and a flexible tiling configuration, similar to the one depicted in Figure\u00a04. Thus, in this scenario, we need to decide which graph to schedule in which accelerator and also which tiling configuration that accelerator will act upon. We schedule the graphs from the online scheduling dataset to the accelerators using all the previous algorithmic strategies separately. The accelerator tiling decision for each graph is decided using the prediction for our SJF-Predicted method, the optimal tiling for the SJF-Truth and a random tiling for other baselines. Note that the knowledge of the optimal tiling makes the algorithm theoretical; it is not feasible in practice; we mark such algorithms with \u2726. Figure 6 contains the online scheduling results for this first scenario over five runs with a fixed seed for all algorithms. Metrics are normalized separately by the largest value in any algorithm. We observe that the combined effects of both having better scheduling decisions and a dataflow-aware tiling selection yield a large improvement over baselines in all metrics while matching the performance of SJF-Truth first with optimal tiling selection. Next, we are interested in understanding what magnitude of the improvements is obtained by the tiling selections versus having better estimates for the graph cost. We consider the same heterogeneous multi-accelerator setting with the same algorithmic baselines with the difference that now all baseline algorithms select the optimal tiling after assigning a graph to an accelerator. For our method the tiling is still selected with the prediction. Figure 7 shows the results for the second setting. With optimal tiling decisions the mean execution times and mean completion times of all baselines are very close to the true shortest job first performance. We observe that algorithms with better estimates for the graph cost significantly improve the turnaround time, indicating a superior ability to handle job queues and, consequently, better overall scheduling capabilities. Note that all methods using the true optimal tiling are not feasible in practise and thus marked with \u2726. The computational cost of inference for predictors is an important consideration for the practical feasibility of our approach. In our implementation, we utilize a lightGBM boosting trees compiler called lleaves (Boehm, [n.\u2009d.]) to compile the trained models. To assess the inference time, we conducted measurements on a workstation equipped with a Ryzen 9 5900x processor. The average inference time across all graphs in our datasets was 2,311.88032311.88032,311.88032 , 311.8803 nanoseconds. Assuming an accelerator operating at 1111\u00a0GHz, and all the 24 models running in parallel on multiple cores, this translates to approximately 2,31223122,3122 , 312 accelerator cycles per inference.\nTo contextualize this performance, we compare it to the average waiting time for job scheduling in our previous SJF-Predicted runs, which was 18,756.4918756.4918,756.4918 , 756.49 cycles. The inference time for our predictive models is thus only about 12.3%percent12.312.3\\%12.3 % of the average job waiting time. This comparison demonstrates the feasibility of implementing SJF-Predicted scheduling, even without dedicated GPU acceleration for the predictive models.\nIt is worth noting that the computational cost of model inference remains constant regardless of graph size. Consequently, as graph sizes increase, the relative cost of model inference becomes increasingly negligible in the overall scheduling process. \n5. Conclusion GNNs present state of the art performance across domains with relational nature. Recent efforts have shown that different accelerators with GNN-specific dataflow architectures significantly outperform conventional GPU and CPU acceleration. In this work, we introduce a novel approach to tackling the unexplored question of how graphs with different properties perform on different dataflows. We train models to predict the latency of executing a certain graph on a certain dataflow on an extensive dataset of simulations. Our experimental evaluation shows that we can efficiently predict the best dataflow for a given graph. This expands the potential of adaptability of GNN accelerators to graph inputs. Moreover, we introduce an online scheduling algorithm leveraging the predictors. Experimental results show that our method outperforms all feasible baselines and matches the performance of the shortest job first, strong but unfeasible in practise, heuristic. Our method achieves up to 3.17\u00d73.17\\times3.17 \u00d7 speedup in mean completion time, 6.26\u00d76.26\\times6.26 \u00d7 speedup in mean execution time and more than 1000\u00d71000\\times1000 \u00d7 speedup in turnaround time against the best feasible baseline on the online scheduling dataset. Several intriguing research directions emerge for future\nwork. The first is to expand on a larger design space exploration of the architecture. The second is to consider more complex models for latency prediction and online scheduling although this direction would require additional time cost concerns. References"}
{"text": "marginparsep has been altered.\ntopmargin has been altered.\nmarginparwidth has been altered.\nmarginparpush has been altered.\nThe page layout violates the ICML style.\nPlease do not change the page layout, or include packages like geometry,\nsavetrees, or fullpage, which change it for you.\nWe\u2019re not able to reliably undo arbitrary changes to the style. Please remove\nthe offending package(s), or layout-changing commands and try again. \u00a0 From CISC to RISC: \nLanguage-Model Guided Assembly Transpilation \u00a0 Anonymous Authors1\u2009 Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE The transition from x86 to ARM architecture is becoming increasingly common across various domains, primarily driven by ARM\u2019s energy efficiency and improved performance across traditional sectors. However, this ISA shift poses significant challenges, mainly due to the extensive legacy ecosystem of x86 software, and lack of portability across proprietary ecosystems and software stacks. This paper introduces CRT, a lightweight LLM-based transpiler that automatically converts x86 assembly to ARM assembly. Our approach bridges the fundamental architectural gap between x86\u2019s CISC-based and ARM\u2019s RISC-based computing paradigms while preserving program semantics and optimizing performance.\nWe evaluate CRT on diverse real-world applications, achieving 79.25% translation accuracy from x86 to ARMv5 on our comprehensive test suite, and a 88.68% accuracy from x86 to RISC-V. In practical deployments on Apple M2 hardware (ARMv8), our transpiled code achieves 1.73x speedup compared to Apple\u2019s Rosetta 2 virtualization engine, while delivering 2.41x memory efficiency and 1.47x better energy consumption. Through testing and analysis, we show that CRT successfully navigates the CISC/RISC divide, and generates correctly executable RISC code despite machine \u201dlanguage\u201d barriers. We release our code, models, training datasets, and benchmarks at: https://ahmedheakl.github.io/asm2asm/ \n1 Introduction The ending of Moore\u2019s Law and Dennard scaling has led to a paradigm shift in the way modern processors are designed and architected. No longer benefiting from generational improvement in power, performance, and area efficiency (PPA), many academic and industry players are rethinking their architectural designs. This includes both introducing more specialized hardware components (such as tensor cores for ML processing)\u00a0Markidis et\u00a0al. (2018); Jouppi et\u00a0al. (2017), alternatives to Von Neumann architectures (e.g., processing-in-memory\u00a0Han et\u00a0al. (2020); Kang et\u00a0al. (2013), and revisting the fundemental computing paradigm of CISC versus RISC designs. Complex instruction set computers (or CISC) such as Intel and AMD\u2019s x86 instruction set architecture (ISA) have maintained a stronghold in the datacenter and server space (holding more than 79.9% of the market\u00a0Grand View Research (2024) as well as personal computing devices (with more than 82% market share in 2024\u00a0Rahman et\u00a0al. (2024)). Reduced instruction set computers (RISC), on the other hand, are predominantly deployed in energy-constrained environments, such as IoT devices, mobile phones, and edge devices\u00a0Grand View Research (2024). In this space, ARM corporation is the predominant player, licensing its architecture to many companies such as Apple and Qualcomm mobile phone chips, and \u201csmart\u201d devices like household appliances. However, this has all begun changing very fast in recent years\u00a0Woo (2020). For example, Apple recently switched over from Intel x86 chips to power their Macbooks and are now exclusively using ARM chips rebranded as M1, M2, and M3 cores\u00a0Shilov (2023). Amazon has begun designing in-house chips called Graviton\u00a0Morgan (2022) to power their datacenter and compute services. Furthermore, Microsoft has also been adopting their operating system design to support ARM, both for in-house hardware (such as Surface tablets) as well as general purpose compute (for Windows OS). While many business decisions go into such a drastic change in infrastructure, the common thread is that ARM-based devices provide excellent energy-efficiency compared to their x86 counterparts, and have dramatically bridged the gap in performance as well\u00a0CloudPanel (2023). Despite lots of potential advantages of low-powered ARM processors, adopting or moving to a new ISA in the hardware space renders prior code in the software space incompatible, as the machine code must be re-targeted for the new ISA. The ISA defines the hardware-software \u201ccontract\u201d, which has enabled substantial improvements over the years independently in the software space and hardware space. Nevertheless,\nan alternative \u201cmachine language\u201d presents a huge technological barrier to overcome, as code portability now becomes a problem. Additionally, for competitive reasons, many companies do not ship their source code around, and instead provide executable binaries which are challenging to de-compile and break intellectual property (IP). Alternatively, to avoid having to recompile a huge code base and to address code portability challenges, hardware virtualization has been used to dynamically \u201ctranslate\u201d between ISAs.\nSeveral tools currently facilitate x86 to ARM virtualization. QEMU\u00a0Bellard (2005b), an open-source, general-purpose emulator supporting multiple architectures and, while versatile, introduces significant performance overhead compared to native execution\u00a0Wei et\u00a0al. (2019). Rosetta 2\u00a0Apple Inc. (2020), Apple\u2019s proprietary translation layer, specifically designed for x86 to ARM (Apple Silicon) translation, is more efficient than QEMU, but its closed-source nature and platform-specificity limit its broader application. Given these existing solutions\u2019 limitations, a crucial question emerges: Can we develop a direct translation medium between x86 and ARM architectures that ensures correctness without the performance hit of a virtualization layer? Addressing the correctness of assembly to assembly translations is arguably the more challenging research aspect. In addition to legacy software compatibility reasons, CISC instructions have fundamental architectural differences to RISC instructions, such as how an instruction handles registers versus memory for different opcodes, the number of available general purpose registers between the ISAs, and the access of sub-register bits (e.g., modifying only the lower 8-bits of a register value). Similarly, compilers need to work harder to generate ARM-based code, while that complexity is lowered onto the micro-architecture in an x86-based system. Binary sizes are also different, where programs assembled into x86 are typically much shorter (due to the complex nature of instructions which are later converted to micro-code inside the processor), compared to the longer binaries in ARM (composed of many, simpler instructions). Furthermore, operating systems need to be aware of the underlying \u201clanguage\u201d of the hardware in order to properly manage it; we observe this phenomenon when seeing that Windows today has two separate OS binaries for x86 versus ARM\u00a0Anderson & Smith (2023); Microsoft (2024). Existing approaches demonstrate an apparent dichotomy: open-source emulators prioritize flexibility over performance, while solutions like Rosetta achieve efficiency through hardware-specific optimizations. This creates a gap in the ecosystem for a solution that combines both attributes. This paper proposes an approach based on language models (LMs) as machine translation engines. Building upon LMs\u2019 demonstrated success in various translation tasks, we apply them to learn mappings between x86 and ARM assembly code through paired examples. Intuitively, we use the LM to \u201ctranslate\u201d between CISC and RISC machine code, just as it has performed admirably in recent years in enabling translation between human languages. A key difference, however, is that the translation must be precise in machine languages, as any incorrect syntax, mis-used registers, bad jumps, or other architecturally important constructs can render the program incorrect semantically and/or functionally.\nOur methodology presents an opportunity to merge open development practices with high-performance translation capabilities, as the models can identify and optimize instruction patterns without depending on proprietary optimizations or predetermined translation rules while avoiding complete system emulation overhead or code overhauls. This work presents the following key contributions: The first CISC to RISC transpiler, coined CRT, built via a custom-trained LLM achieving a test accuracy of 79.25% on ARM and 88.69% on RISC-V64. An in-depth analysis into the inner workings of our transpiler, including hardware-informed design decisions to best train an accurate LLM model for assembly transpilation (\u00a73, \u00a76). We perform a case-study using our transpiler in a real-world setting, and compare it to Apple Rosetta\u2019s x86 to ARM virtualization engine. Results show that CRT\u2019s generated assembly achieves 1.73x speedup compared to Rosetta while delivering 1.47x better energy efficiency and 2.41x memory efficiency (\u00a77). In the remainder of this paper, we provide additional background in the space of assembly languages and LLM-for-hardware (\u00a72), describe our approach in designing the transpiler (\u00a73), evaluation of its efficacy (\u00a74), and discuss the challenges and benefits of our approach (\u00a75, \u00a76). Finally, we conclude with a case study of transpiling for an Apple M2 processor, and the advantages over prior approaches for this task (\u00a77). \n2 Background & Related Work Virtualization and Emulation: Emulation and assembly-level virtualization enable the execution of one ISA\u2019s binary on a host machine for which it was not compiled for originally. QEMU Bellard (2005a), an open-source emulator, uses dynamic binary translation Sites et\u00a0al. (1993), offering flexibility but with performance overhead, enabling x86 to ARM emulation, amongst other ISAs. Rosetta 2, Apple\u2019s virtualization layer for macOS, combines ahead-of-time (AOT) and just-in-time (JIT) translation, providing better performance within the Apple ecosystem. These approaches face challenges in achieving native-level performance and ensuring broad compatibility, due to the dynamic nature of execution. A transpiler approach, directly converting x86 to ARM assembly, could supplant these solutions by eliminating runtime translation overhead with a one-time translation into the host ISA. This method could address the limitations of current emulation and virtualization techniques, particularly in performance-critical scenarios, or where pre-processing is feasible, or when source code is not available (due to proprietary IP). Neural Code Translation: Machine learning approaches for code translation have primarily focused on high-level programming. Initial research explored neural machine translation architectures, with TransCoder Lachaux et\u00a0al. (2020) demonstrating unsupervised translation between C++, Java, and Python. Pre-trained transformer models such as CodeBERT Feng et\u00a0al. (2020) and CodeT5 Wang et\u00a0al. (2021) have advanced code understanding and generation capabilities across multiple programming languages. The emergence of LLMs specialized for code, including Code Llama Roziere et\u00a0al. (2023) and DeepSeek Coder Liu et\u00a0al. (2024), has demonstrated increasingly sophisticated code manipulation capabilities through self-supervised learning on vast code repositories. These methods often rely on structural representations like abstract syntax trees for high-level language translation Chen et\u00a0al. (2018). However, these methods face distinct challenges when applied to assembly-level translation, where the structural representations and semantic preservation requirements differ significantly from high-level language translation. Language Models for Low-Level Programming: Recent research has increasingly demonstrated the potential of language models in various tasks related to low-level code analysis and transformation. A language model is a statistical model that learns to predict the probability distribution of tokens within a language, enabling it to generate coherent and contextually relevant text.\nThese models have been successfully applied in areas such as decompilation, binary similarity analysis, and compiler optimization, demonstrating their ability to tackle intricate, instruction-level challenges. In decompilation, LLM4Decompile Tan et\u00a0al. (2024) introduced specialized language models for direct binary-to-source translation and decompiler output refinement. DeGPT Hu et\u00a0al. (2024) further explored decompiler enhancement through semantic-preserving transformations. SLaDe Armengol-Estap\u00e9 et\u00a0al. (2024) combines a 200M-parameter sequence-to-sequence Transformer with type inference techniques to create a hybrid decompiler capable of translating both x86 and ARM assembly code into readable and accurate C code, effectively handling various optimization levels (-O0 and -O3). -O0 -O3 Language models have also been adapted to optimization tasks, with LLM Compiler Cummins et\u00a0al. (2024) introducing a foundation model that supports zero-shot optimization flag prediction, bidirectional assembly-IR translation, and compiler behavior emulation. This approach demonstrates the potential for language models to enhance compiler optimization workflows by automating complex tasks. Binary similarity analysis has similarly benefited from language model adaptations. DiEmph Xu et\u00a0al. (2023) addressed compiler-induced biases in transformer models, while jTrans Wang et\u00a0al. (2022) incorporated control flow information into the transformer architecture. Yu et al. Yu et\u00a0al. (2020) combined BERT-based semantic analysis with graph neural networks to capture both semantic and structural properties of binary code. While these applications have shown promising results, the use of LLMs for assembly code transpilation remains relatively underexplored. Assembly languages present unique challenges due to the fundamental differences in instruction sets and execution models across architectures. GUESS & SKETCH Lee et\u00a0al. (2024) introduced a neurosymbolic approach combining language models with symbolic reasoning for translating assembly code between ARMv8 and RISC-V architectures. Our work, CRT, extends this direction by addressing the more challenging task of transpiling between CISC (x86) and RISC (ARM, RISC-V) architectures, bridging fundamental architectural differences in ISA complexity and execution models. \n3 Approach This sections describe our approach for CISC to RISC assembly transpilation (CRT), covering the problem setup, model choices, training, and tokenizer adaptations. \n3.1 Problem Definition We aim to translate x86 assembly code to ARM assembly code by leveraging Language Models (LMs) to automatically handle the fundamental differences between these ISAs. Let X={x1,x2,\u2026,xn}\ud835\udc4bsubscript\ud835\udc651subscript\ud835\udc652\u2026subscript\ud835\udc65\ud835\udc5bX=\\{x_{1},x_{2},\\dots,x_{n}\\}italic_X = { italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } denote the set of x86 vocab, and Y={y1,y2,\u2026,yn}\ud835\udc4csubscript\ud835\udc661subscript\ud835\udc662\u2026subscript\ud835\udc66\ud835\udc5bY=\\{y_{1},y_{2},\\dots,y_{n}\\}italic_Y = { italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } denote the set of corresponding ARM vocab. Our goal is to learn a mapping function f:X\u2192Y:\ud835\udc53\u2192\ud835\udc4b\ud835\udc4cf:X\\rightarrow Yitalic_f : italic_X \u2192 italic_Y that translates any x86 code xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT into its ARM equivalent yisubscript\ud835\udc66\ud835\udc56y_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Our approach is to utilize LLMs and have the model learn the conditional distribution P\u2062(Y\u2223X;\u03b8)\ud835\udc43conditional\ud835\udc4c\ud835\udc4b\ud835\udf03P(Y\\mid X;\\theta)italic_P ( italic_Y \u2223 italic_X ; italic_\u03b8 ), where \u03b8\ud835\udf03\\thetaitalic_\u03b8 represents the model parameters. The model generates ARM code in an autoregressive manner, producing each token ytsubscript\ud835\udc66\ud835\udc61y_{t}italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT based on the input x86 code X\ud835\udc4bXitalic_X and previously generated tokens y<tsubscript\ud835\udc66absent\ud835\udc61y_{<t}italic_y start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT: P\u2062(Y\u2223X;\u03b8)=\u220ft=1TP\u2062(yt\u2223y<t,X;\u03b8)\ud835\udc43conditional\ud835\udc4c\ud835\udc4b\ud835\udf03superscriptsubscriptproduct\ud835\udc611\ud835\udc47\ud835\udc43conditionalsubscript\ud835\udc66\ud835\udc61subscript\ud835\udc66absent\ud835\udc61\ud835\udc4b\ud835\udf03P(Y\\mid X;\\theta)=\\prod_{t=1}^{T}P(y_{t}\\mid y_{<t},X;\\theta)italic_P ( italic_Y \u2223 italic_X ; italic_\u03b8 ) = \u220f start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_P ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2223 italic_y start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT , italic_X ; italic_\u03b8 ). This approach aims to provide contextually accurate translations, avoiding prior rule-based methods of the past\u00a0Ebcio\u011flu & Altman (1997). \n3.2 Training Stages and Model Selection Our framework consists of three main stages: Data collection, Experimentation (of hyperparameters), and Optimization & Deployment, as illustrated in Figure\u00a02. Data Stage:\nWe construct our dataset by collecting a diverse corpus of C source files and generating corresponding x86 (CISC) and ARM (RISC) assembly pairs. These paired compilations constitute our training data, from which the language model learns the cross-architectural mapping between CISC and RISC instruction sets. For evaluation, we employ two metrics: functional correctness assessed through unit tests accompanying the C files, and assembly-level similarity measured via the more strict metric of edit distance (\u00a74.4) between the generated outputs and ground truth assembly pairs. Experimentation Stage:\nFor model selection, we fine-tune various small-scale, established open-source language models, leveraging their efficiency in processing high-level code. During this phase, we experiment with hyperparameters including batch size, gradient accumulation\u00a0Lamy-Poirier (2021), warmup steps\u00a0Goyal (2017), optimizers (e.g., AdamW\u00a0Loshchilov (2017) and Adafactor\u00a0Shazeer & Stern (2018)), learning rates, and epochs to find the best settings for our application of assembly transpilation. Each model undergoes a thorough evaluation on our curated benchmark, and accuracy is verified by executing test cases using Qemu\u00a0Bellard (2005b) to ensure reliability. We also explored the alignment of code pairs in this phase. While this showed initial promise, it did not help significantly and so we leave our experimentation details for the interested reader in appendix\u00a0\u00a7A.3. Optimization & Deployment:\nIn the stage, the best-performing model is selected and trained on the entire dataset. We also explore post-training quantization, to study the efficacy of compressing our asm-to-asm transpiler for a low-resource deployment setting. We run a case study to evaluate and compare the transpiled code against Rosetta\u00a0Apple Inc. (2020), to assess performance in a production-like environment (\u00a77). \n3.3 Tokenizer Extension To enhance our LLMs\u2019 understanding and generation of assembly code, we extended the tokenizer to include the most frequently used opcodes and register names from both x86 and ARM architectures (see Table\u00a01). Tokenization segments raw text into tokens for the model to process\u00a0Vaswani (2017), allowing accurate recognition of assembly language components. This customization efficiently represents the distinct semantics of each instruction set, improving the model\u2019s ability to parse and generate correct translations by aligning with the low-level details of the input assembly code. \n4 Experiments and Evaluation We present a comprehensive evaluation of CRT\u2019s effectiveness in x86-to-ARM binary transpilation across three dimensions: training data preparation (\u00a74.1), hyperparameter tuning (\u00a74.2), and model architecture selection. Using standard benchmarks (\u00a74.3) and metrics (\u00a74.4), we assess the generated assembly code\u2019s semantic preservation and functional correctness. \n4.1 Training Data The training dataset was derived from AnghaBench\u00a0Da\u00a0Silva et\u00a0al. (2021), a comprehensive benchmark suite containing 1 million compilable C programs mined from major public C repositories on GitHub. From this benchmark, we randomly sampled 500k programs to form our training set, equivalent to 8 billion tokens. These programs were then compiled to x86 using gcc\u00a0Compiler (2009) and cross-compiled to ARMv5 using ARM-gnueabi-gcc\u00a0Radcolor (n.d.); both sets were generated on an AMD Ryzen 7 processor. \n4.2 Experimental Setup All of our hyperparameter optimization experiments were conducted on a small 100k portion of AnghaBench. We tested various hyperparameter settings on this subset of our benchmark. After identifying the optimal configuration, we scaled up the training data to 500k samples. We trained three models: DeepSeek-Coder1.3B\u00a0Guo et\u00a0al. (2024), Yi-Coder2B\u00a001.AI (2024), and BART-Large (300M)\u00a0Lewis (2019) on the AnghaBench dataset. Given the dataset size of 1 million samples, with an average of 13k tokens per sample, we opted for smaller models and worked with reduced dataset size of 500k samples. All models were trained using four A100 GPUs (40 GB each). Training with 500k samples, a batch size of 4, and 2 epochs required three days. To conserve memory, mixed precision training with bfloat16 was employed. Given limited capacity for large batch sizes, we applied gradient accumulation\u00a0Lamy-Poirier (2021) with an effective batch size of 4. Additionally, all models were trained with optimization level -O0, and we used paged AdamW\u00a0Loshchilov (2017) to avoid memory spikes, with a weight decay of 0.001. We chose a small learning rate of 1\u00d710\u221241superscript1041\\times 10^{-4}1 \u00d7 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT with a linear schedule, as experiments indicated this schedule performed best. All models were trained with a context window of 16k. -O0 For inference, we used caching to enhance inference speed and disabled sampling to ensure deterministic outputs. Our evaluation set was sourced from the LLM4Decompile\u00a0Tan et\u00a0al. (2024) set, compiled to x86. We employed QEMU\u00a0Bellard (2005b) to simulate the evaluation environment. Following training, we apply quantization techniques (e.g., bfloat16, int8, and int4) using llama.cpp\u00a0Ggerganov  to optimize for efficient inference on CPU-based devices. This step is crucial to maintain high performance while reducing the computational load, making the solution feasible for local deployment. \n4.3 Evaluation Benchmark CRT\u2019s performance accuracy is evaluated using the HumanEval benchmark, originally introduced by Chen et\u00a0al. (2021) for Python code generation. The benchmark consists of 164 programming problems that assess language comprehension, reasoning, and algorithmic thinking. For our evaluation, we utilize the C-translated version from LLM4Decompile Tan et\u00a0al. (2024), which maintains the same problems while converting both function implementations and test cases to C code. To ensure thorough testing, we measure code line coverage using gcov, GNU\u2019s source code coverage analysis tool. As emphasized by Myers et\u00a0al. (2011), line coverage is a fundamental metric in software testing that indicates which lines of code were executed at least once during testing, helping identify untested code paths and potential blind spots in test suites. The higher the line coverage percentage, the more comprehensive the testing. HumanEval resulted in an average line coverage of 98.81%, indicating that nearly all lines of code were executed during testing. For the evaluation process, we generate the corresponding assembly code pairs following the training data preparation process detailed in Section \u00a74.1. \n4.4 Evaluation Metrics We evaluate our approach using two primary metrics: Edit Distance: Following prior work\u00a0Lee et\u00a0al. (2024), we employ the Levenshtein edit distance Lcvenshtcin (1966) between the ground truth and transpiled ARM assembly as an initial measure of syntactic similarity. However, this metric is limited because semantically equivalent assembly sequences can differ syntactically due to variations like register allocation or instruction ordering (\u00a76). Functional Correctness:\nTo overcome the limitations of edit distance, we assess the functional equivalence by running the ground truth and transpiled code against comprehensive test suites, employing software testing principles via unit test coverage. A transpilation is considered correct if it passes all test cases of the corresponding program. The experimental results presented in\u00a0\u00a75 show that a significant portion of the transpilation results differ syntactically (having non-zero edit distance) from the ground truth yet preserve program semantics and execute correctly. \n5 Results We evaluate the efficacy of our transpiler for CISC-to-RISC assembly translation, focusing on the correctness of the output ARM assembly. Utilizing the metrics defined above (\u00a74), we compare our approach with state-of-the-art coding LLMs and evaluate our approach for x86 to ARM transpilation (Table2). \n5.1 Transpiler Validation Of the evaluated LLMs, DeepSeekCoder-1.3B-xTokenizer (our model utilizing the tokenizer and trained as described in \u00a74.2) achieves a test accuracy of 79.25% on x86 to ARM transpilation, substantially outperforming larger models such as GPT4o (8.18%), DeepSeekCoder2-16B (7.36%), and Yi-Coder-9B (6.33%) (Table\u00a02). Despite being 9\u00d7\\times\u00d7 to 20\u00d7\\times\u00d7 smaller in size, our model exhibits at up to 9.8\u00d7\\times\u00d7 the accuracy, highlighting the effectiveness of our approach. Compared to prior work on ARM to RISC-V translation \u00a0Lee et\u00a0al. (2024) (which achieves 68% accuracy - 80% when using symbolic analysis - with a similarly sized model), our model attains higher accuracy on the more complex task of x86 to ARM translation without the use of external, symbolic testing or tools. We attribute this approximate 7.8% difference to our better tokenization scheme (which is needed for the CISC/RISC disambiguation) as well as the 8x larger context window. An interesting comparison arises between DeepSeekCoder-1.3B and Yi-Coder-1.5B; DeepSeekCoder outperforms Yi-Coder by 28%. We attribute this to DeepSeekCoder being trained from scratch on 2 trillion code tokens, whereas Yi-Coder is based on the Yi-chat model, potentially limiting its efficacy for this specific task. Additionally, the results from our aligned BART shows a 12% jump in accuracy from baseline BART inspired by\u00a0Lee et\u00a0al. (2024). This shows that our alignment technique is effective even with a very low context window of 1024 but yet still lacks behind large context window of DeepSeekCoder by 62.8%. Our model exhibits strong syntactic robustness, as evidenced by the high number of transpiled programs with an edit distance of zero (Table\u00a02, column\u00a03). This indicates that the generated assembly code is often syntactically identical to the reference code, demonstrating proficiency in producing correct assembly language syntax for both RISC architectures (ARM & RISC-V). Moreover, the absence of syntax errors allows us to focus our evaluation on functional correctness, measured by test accuracy metrics. This aligns with our observation that LMs rarely produce typos or grammatical errors, even in assembly instructions, based on our tests. \n5.2 Impact of Tokenizer and Quantization Enhancements in our tokenizer contribute to the model\u2019s accuracy. DeepSeekCoder-1.3B-xTokenizer increases accuracy by 2% compared to DeepSeekCoder-1.3B, indicating that our extended tokenizer enables more efficient learning. Additionally, the optimized tokenizer reduces the average number of tokens by 7.27%, improving inference speed. Our models quantized to int8 and int4 precision achieve comparable results to the float32 models. For ARM transpilation, accuracy decreases by only 3.8% when moving from float32 to int8, and by nearly 2.5% when moving to int4 (Table\u00a02 column\u00a02). \n5.3 Training Performance Selecting appropriate training hyperparameters has a significant impact on model accuracy and edit distance. In our experiments, implementing warmup steps\u00a0Goyal (2017) notably improved the model\u2019s edit distance, decreasing it by 8%. This improvement is attributed to enhanced convergence and training stability, which are crucial for complex tasks like x86 to ARM transpilation. Since we are fine-tuning models that were initially trained to follow specific instructions to produce code, the use of warmup steps with a higher initial learning rate helps our model transition from this instruction-based generation setting to our transpilation setting, thereby avoiding early overfitting and increasing generalization\u00a0Kalra & Barkeshli (2024). \n5.4 Inference Performance As shown in Figure\u00a03, we analyze the model\u2019s accuracy across different inference configurations. Our beam search experiments (Figure\u00a03(a)) demonstrate that increasing the number of beams improves accuracy. Specifically, as the beam size increases from 1 to 8, the model explores multiple decoding paths, akin to human-like consideration of different options. This approach is similar to the symbolic solving post-processing technique used in\u00a0Lee et\u00a0al. (2024), but with a computational complexity of O\u2062(b\u2062N2)\ud835\udc42\ud835\udc4fsuperscript\ud835\udc412O(bN^{2})italic_O ( italic_b italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) compared to their O\u2062(2N)\ud835\udc42superscript2\ud835\udc41O(2^{N})italic_O ( 2 start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ) complexity. This increased flexibility allows the model to produce more robust solutions. However, larger beam sizes increase computational overhead, presenting a trade-off between performance and inference speed. From a runtime performance, our models exhibit real-world inference efficiency. On an NVIDIA A100 GPU with 15.6\u00a0TFLOPS, our model generates a sample with a 16k context length in 18.3 seconds on average, achieving a rate of 437.2 tokens per second. On a Ryzen 7 CPU, the model achieves 18.51 tokens per second with int8 quantization and 87.23 tokens per second with int4 quantization using Ollama, demonstrating applicability for real-world use cases requiring efficient inference. Figure\u00a03(b) shows that our model\u2019s accuracy increases logarithmically with the size of the training dataset. As more data is provided, the model better learns to reason about transpilation with diverse training examples following LLMs scaling laws\u00a0Kaplan et\u00a0al. (2020). We also experimented with different compilers (GCC and Clang) during training and found that both produced similar results, with an edit distance of 133 and exact match accuracy of 50.31%. All results are thus reported with the GCC compiler, due to its compatibility with QEMU for running tests as mentioned in section\u00a04.2. \n5.5 Analysis of Transpiled Assembly Code To better understand the transpilations generated by CRT, we examined the evaluation benchmark\u2019s ground truth and compared it with the transpiled code. An interesting case arises when the transpiled code is correct, but the edit distance from the ground truth deviates from zero. Our investigation revealed various patterns of implementation that maintain functional correctness despite differing from the ground truth. Commutative operations, like the add opcode, sometimes show order differences between the ground truth and predicted code. Register allocation variations occur when different registers are chosen for the same operations, as long as data flow and register dependencies are preserved. Memory location swapping is another pattern where variables are stored in different memory locations, yet their relationships remain consistent. Additionally, stack frame sizes sometimes differ between the ground truth and transpiled code. We also identified cases of instruction-level semantic equivalence, where different instruction sequences achieve the same logical result. For example, multiplication in the ground truth might use a direct mul opcode, while the transpiled code achieves the same computation using shifts and additions. Variations in constant handling were also noted, with the transpiled code sometimes using immediate values directly in instructions while the ground truth loads constants from memory. Instruction consolidation is common, with the transpiled code combining multiple instructions (mov r1, r2; add r1, r1, #1) into single, streamlined versions (add r2, r2, #1) that maintains functionality. In many instances, these patterns appear together in what we refer to as \u201dcomposite variations\u201d. The error patterns become particularly evident in assembly code, where even slight variations can lead to significant functional issues. For instance, non-commutative operations are prone to critical errors when operand order is reversed, as seen in subtraction, where using sub r1, r3, r2 instead of sub r1, r2, r3 completely changes the computation. Incorrect register management, where registers are overwritten prematurely, can lead to data loss. Immediate value errors also appear, particularly in shift operations; for instance, asr r2, r2, #1 is mistakenly used for division by 2 when division by 4 (asr r2, r2, #2) is needed, causing computational discrepancies. Memory addressing errors, such as misaligned access, occur when values are stored and retrieved from incorrect offsets, resulting in data corruption. Additional examples are provided in the Appendix\u00a7A.1. We also observe for incorrect cases with a high edit distance, the transpiled code diverges significantly from the ground truth, often reflecting substantial deviations from the intended logic\u2014sometimes even resulting in unintended behaviors like infinite loops. Such drastic variations make it difficult to interpret or troubleshoot the transpiled output by simply comparing it to the ground truth, as the underlying logic no longer aligns. \n5.6 Extension to RISC-V ISA To demonstrate the generality of our method, we also trained our model on the task of transpiling from x86 to RISC-V64, achieving a test accuracy of 88.68% (Table\u00a03). Notably, our model significantly outperforms existing models like GPT4o and DeepSeekCoder2-16B, which achieved much lower test accuracies of 7.55% and 6.29%, respectively. However, this result indicates that transpiling to RISC-V64 is easier than to ARM, possibly due to RISC-V\u2019s simpler and more consistent instruction set compared to the more complex instructions in recent ARM architectures. For RISC-V efficient transpilation (Table\u00a03 column\u00a02), the int8 quantized model\u2019s accuracy is nearly identical to the float32 model, with a minor drop of 0.63% and the same exact match accuracy. However, the int4 quantized model experiences a significant accuracy drop of 19.5%. The results not only reinforce that we can deploy our model on consumer-grade machines with high-efficiency, but also from an information theory perspective that the number bits necessary for CISC-to-RISC transpilation can still accurately encode functional semantics for machine code. \n6 Discussion Our proposed model effectively tackles the challenges of CISC-to-RISC assembly code transpilation, achieving high performance without extensive model scaling. In this section, we provide insights into our model\u2019s behavior and the factors influencing its performance. One key observation is the performance difference between x86-to-ARM and x86-to-RISC-V64 transpilation. Our model performs better when translating to RISC-V, likely due to its simpler instruction set (47 instructions) compared to ARM\u2019s more complex set (approximately 100 instructions), as shown in Figure\u00a06. This suggests that the complexity of the target ISA significantly affects the difficulty of the transpilation task. Consequently, we focused most of our experiments on x86-to-ARM transpilation to address challenges associated with more complex RISC ISAs. Another important aspect is the model\u2019s syntactic flexibility. While CRT achieves high functional correctness, it often produces syntactically diverse outputs, as indicated by the high edit distance metric. Instead of memorizing patterns, the model reasons through register mappings and instruction sequences, generating functionally correct but syntactically varied code. This flexibility is beneficial in real-world code generation, where functional equivalence is more valuable than syntactic similarity. For example, the model may choose different sets of registers for each instance, as long as it maintains consistency in their usage. Acceptable variations include using different operand orders in commutative instructions (e.g., add r0, r1, r2 vs.\u00a0add r0, r2, r1, assigning temporary variables to different registers, combining multiple instructions into a more efficient single instruction, varying allocated memory size or resource usage without affecting functionality, employing different methods for loading constants, and consistently swapping memory locations for variables (e.g., using [fp, #-8] instead of [fp, #-12] for a loop counter). These variations showcase the model\u2019s ability to generate functionally equivalent but syntactically diverse code, which is often desirable in assembly code generation. Despite these successes, CRT is not without limitations. We observed that certain errors persist, which can be categorized into three main types (1) Register Allocation Errors (14.11%): Allocating registers that have already been allocated, leading to memory issues (2) Addressing Errors (62.03%): Jumping to prohibited memory addresses or copying incorrect addresses from x86 code (3) Other Errors (23.86%): Including invalid constants and floating-point exceptions. We noticed that additional training reduced the number of Type 1 errors by 21.45%, Type 2 errors by 7.67%, and Type 3 errors by 1.39%. This suggests that while increased training data can mitigate some errors, particularly those related to register allocation, other errors may require different strategies. One of the challenges we identified is related to the model\u2019s handling of numerical tokens. Current tokenizers often treat each digit of a number as a separate token, which can hinder the model\u2019s ability to correctly process long numerical values, such as memory addresses or constants that need to be accurately copied or slightly modified from the x86 code. We believe that developing a better tokenizer that handles numbers as single tokens could significantly improve the model\u2019s performance, particularly in reducing addressing and constant-related errors. Notably, our model, despite being at least seven times smaller than those used in similar studies\u00a0Lee et\u00a0al. (2024); Tan et\u00a0al. (2024), was effective in this task. This suggests that for machine language translation, model quality and data curation are more critical than sheer model size. As shown in Figure\u00a03(b), the quality and size of the training data significantly impact performance. Additionally, using an extended tokenizer improved the understanding of instructions and registers, and a longer context window enabled the model to track register usage effectively. Achieving these results with a small, quantized model indicates that efficiency and practicality need not be sacrificed for performance. This has significant implications for deploying such models on resource-constrained hardware common in embedded systems and other RISC applications. Looking ahead, there are several avenues for improving our model. In addition to developing a better tokenizer, as previously mentioned, incorporating more diverse and comprehensive training data could further reduce errors, particularly those related to addressing and constants. Exploring techniques such as incorporating domain-specific knowledge or constraints into the model training process might also enhance performance. Overall, our findings highlight that high-quality assembly code transpilation across diverse ISAs requires a holistic approach that goes beyond merely increasing model size. Thoughtful design of tokenization and training processes, attention to ISA-specific challenges, and efficient quantization collectively enable high-performing, deployment-ready models. These insights contribute to a deeper understanding of how models can be designed to facilitate the industry\u2019s ongoing transition towards scalable, energy-efficient processor architectures, supporting software compatibility and performance across platforms. \n7 Case Study To evaluate the efficiency of our transpiler, we performed a real world study on an Apple M2 Pro machine, which features a more recent version of ARM, the ARM64v8-A architecture. This transition offered two key advantages: first, it enabled us to use the native ARM compiler toolchain to generate the target instruction sequences, rather than relying on cross-compilation; second, through Apple\u2019s Rosetta 2 translation layer, we established a unified testing environment enabling comparative analysis across different execution modes within the same hardware platform. \n7.1 Experimental Setup The evaluations were conducted using Apple Clang 14.0.3 (clang-1403.0.22.14.1) on an Apple M2 Pro processor with 16GB of RAM running macOS 13.7. All programs were compiled targeting the arm64-apple-darwin22.6.0 architecture with -O0 optimization level. Additional details are the same as \u00a74.2 for our transpilation correction evaluation. -O0 We examine the execution characteristics across three distinct execution environments on the M2 Pro machine. First, we establish the baseline performance by executing natively compiled ARM64 binaries. Second, we measure the performance of x86 binaries executed through Apple\u2019s Rosetta 2 dynamic binary translation layer. Third, we evaluate our CRT transpiled code that directly transforms x86 assembly to ARM64. For each environment, we analyze three performance dimensions: execution time, CPU energy consumption measured using powermetrics instrumentation, and memory utilization patterns. To ensure statistical validity, we execute each program 100 times and compute its geometric mean for reporting\u00a0Fleming & Wallace (1986). All performance evaluations are conducted under controlled conditions. The functional correctness of the proposed approach, when trained on ARMv8, yielded the results shown in Table 4. A key finding is that compared to ARMv5, we observed a performance drop of 4.25% with ARMv8 achieving 75.0% on the evaluation-set. This decline can be attributed to ARMv8\u2019s increased architectural complexity (refer to Appendix\u00a0A.2). The increased complexity is evident in several areas, including register usage patterns, addressing modes, comparison operations, and floating-point handling. In each of these areas, ARMv8 adopts more sophisticated approaches compared to ARMv5\u2019s straightforward implementations. These architectural features make instruction patterns more challenging for the LLM to learn effectively. Examining the confusion matrix (Figure\u00a05) for the proposed approach\u2019s performance across ARMv5 and ARMv8 architectures reveals an agreement of 76.8%. However, we observe distinct failure patterns (15 versus 23 cases failing uniquely on ARMv5 and ARMv8 respectively), suggesting architecture-specific performance variations that induce divergent error modes in the LLM\u2019s execution. \n7.2 Performance Analysis The performance evaluation shows that CRT achieves near-native efficiency across multiple metrics. With execution times nearly matching native code, CRT delivers a 1.73x speedup over Rosetta, along with 1.47x better energy efficiency and 2.41x better memory efficiency. Notably, CRT\u2019s memory footprint remains close to native execution (1.034 MB vs. 1.03 MB), whereas Rosetta requires 2.49 MB. \n7.3 ARMv5 versus ARMv8 Analysis The proposed transpiler enables efficient cross-architecture code execution with performance nearing native compilation, proving the feasibility of LLM-based binary translation. These results indicate that machine learning approaches can effectively learn complex mappings between instruction set architectures, maintaining high performance with significant improvements over Rosetta. Looking forward, this opens the door for efficient CISC to RISC transitions, particularly to ARM architectures, while maintaining seamless control over legacy x86 software with reduced overhead and improved performance. This could potentially accelerate the adoption of ARM architectures across enterprise environments. \n8 Conclusion This paper presents CRT, a language model-guided transpiler for converting CISC (x86) assembly code to RISC (ARM and RISC-V) architectures. Our work explores a shift in systems software design - moving away from hand-crafted rules and predetermined translation patterns toward more intelligent and adaptable solutions. As hardware architectures continue to diversify and evolve, particularly with the rise of domain-specific accelerators, learning-based approaches may become increasingly relevant for maintaining software portability while maximizing hardware performance. The future of systems software lies in leveraging machine learning to create solutions that can adapt to architectural changes without requiring separate engineering efforts for each new target platform. References \nAppendix A Appendix \nA.1 Analysis of Assembly Code Variations In this section, we present a detailed analysis of the variations between ground truth and predicted assembly code from our evaluation benchmark. Our analysis focuses on two key aspects: (1) functionally equivalent code with non-zero edit distance, and (2) incorrect implementations that fail the test cases. For functionally equivalent code, we identified several common patterns that maintain correctness despite syntactic differences. These patterns range from simple variations(Table\u00a05) like register allocation and commutative operations, to complex patterns that combine multiple types of differences (Table\u00a06). For incorrect implementations, we focused particularly on cases with small edit distances to understand how subtle differences can lead to functional failures (Table\u00a07). These cases often involve critical errors in immediate values, register management, or memory access patterns that fundamentally alter the program\u2019s behavior. \nA.2 Assembly Code Comparison Across Architectures Figure\u00a06 provides sample assembly code segments across different ISAs, all for the same program. We showcase some interesting cases, including how ARM has major differences between v5 and v8, despite both being RISC architectures. As the ISA evolves, the binary compatibility changes\u2014a program compiled for ARMv5 might not be compatible with ARMv8. Further, we highlight distinct challenges for transpilers in understanding certain constructs, such as mixed-width register operands, comparison-and-jump definitions, and the varying number of instructions needed to achieve the same functional objective. These samples showcase how ISAs are truly unique \u201dlanguages,\u201d and why we believe (and evaluate) the efficacy of an LLM in transpiling between them. \nA.3 Alignment Building upon the Guess & Sketch method proposed by Lee et al.\u00a0Lee et\u00a0al. (2024) for cross-architecture transpilation using BART, we investigated the application of this neural machine translation model to convert x86 assembly code into ARM assembly code. However, BART\u2019s limitation of a 1024-token context window poses significant challenges when dealing with large assembly functions. This difficulty is further exacerbated by the substantial differences between x86 and ARM instruction sets, which necessitate the preservation of complex semantic relationships that often extend beyond the fixed window size. To address these challenges, we attempted a semantic-aware code segmentation pipeline that facilitates effective translation while maintaining both local instruction patterns and the overall program structure across different architectures. Intuitively, we aimed to \u201delevate\u201d the code up to the IR level to implement alignment. The goal targeted precise code alignment within the limitations of BART\u2019s 1024-token context window. Our initial experiments with fixed-window segmentation resulted in a modest accuracy rate of 4.40%, highlighting the inadequacy of simple segmentation methods in preserving essential program semantics. By implementing our semantic-aware pipeline, we significantly improved the translation accuracy to 16.46%, effectively maintaining both local and global semantics during cross-architecture assembly translation. However, more work needs to be done, and we found that the larger context window size, as described in the main paper, effectively side-stepped this issue of alignment."}
{"text": "Dataflow Optimized Reconfigurable Acceleration for FEM-based CFD Simulations Computational Fluid Dynamics (CFD) simulations are essential for analyzing and optimizing fluid flows in a wide range of real-world applications.\nThese simulations involve approximating the solutions of the Navier-Stokes differential equations using numerical methods, which are highly compute- and memory-intensive due to their need for high-precision iterations.\nIn this work, we introduce a high-performance FPGA accelerator specifically designed for numerically solving the Navier-Stokes equations.\nWe focus on the Finite Element Method (FEM) due to its ability to accurately model complex geometries and intricate setups typical of real-world applications.\nOur accelerator is implemented using High-Level Synthesis (HLS) on an AMD Alveo U200 FPGA, leveraging the reconfigurability of FPGAs to offer a flexible and adaptable solution.\nThe proposed solution achieves 7.9\u00d7\\times\u00d7 higher performance than optimized Vitis-HLS implementations and 45% lower latency with 3.64\u00d7\\times\u00d7 less power compared to a software implementation on a high-end server CPU.\nThis highlights the potential of our approach to solve Navier-Stokes equations more effectively, paving the way for tackling even more challenging CFD simulations in the future. \nI Introduction\n Computational Fluid Dynamics (CFD) play a vital role in analyzing and optimizing fluid flow in complex systems, enhancing design, performance, and safety across industries such as aerospace\u00a0[1], automotive\u00a0[2], and environmental engineering\u00a0[3].\nConstructing physical prototypes for studying these problems is costly and time-consuming, particularly for intricate fluid dynamics scenarios that require elaborate setups and significant resources.\nCFD simulations offer a cost-effective and efficient alternative by providing detailed insights and flexibility for design optimization without physical models.\nThey facilitate virtual testing across diverse conditions and use cases\u00a0[4, 5], helping to identify potential issues early, mitigate risks, and improve overall design performance\u00a0[6]. CFD simulations involve solving the Navier-Stokes Partial Differential Equations (PDEs), which are fundamental in describing fluid flow behavior\u00a0[7].\nSolving these PDEs analytically is challenging, so numerical methods are used to approximate their solutions.\nThe most common methods are the Finite Difference Method (FDM) and the Finite Element Method (FEM).\nFDM\u00a0[8] approximates the derivatives in the Navier-Stokes equations using differences between function values at discrete points on a structured grid, facilitating implementation.\nHowever, FDM\u2019s dependence on structured grids restricts its effectiveness for complex geometries and irregular boundaries, making it challenging to handle intricate boundary conditions and maintain stability. FEM\u00a0[9] discretizes the domain into small elements and employs interpolation functions to approximate solutions, utilizing unstructured meshes that can adapt to intricate geometries and irregular boundaries.\nThis flexibility makes FEM especially effective for modeling complex real-world applications, such as flows around irregularly shaped aircraft wing\u00a0[10].\nHowever, it comes with increased implementation complexity and higher computational demands. Numerically solving the Navier-Stokes equations is time-consuming because of the need for fine grid spacing, numerous time-steps to reach statistical steady-state solutions, and the complexity of the algorithms.\nTo achieve a practical time-to-solution, it is crucial not only to use efficient numerical modeling but also to leverage the advanced capabilities of modern computational architectures.\nGeneral-purpose processors, such as Central Processing Units (CPUs) and Graphics Processing Units (GPUs), are commonly used for numerically solving PDEs.\nCurrent CPU implementations\u00a0[11] primarily rely on Matrix-Vector multiplication, utilizing matrix processing libraries to enhance speed.\nHowever, this approach has two major drawbacks that hinder performance and efficiency: i) it requires substantial buffer memory to store the large, sparse matrices, and ii) CPUs find it challenging to leverage data and computation reuse in these matrices\u00a0[12].\nRecently, GPUs have also been utilized to accelerate CFD simulations\u00a0[13, 14] due to their ability to manage large-scale data and computations more efficiently than CPUs.\nDespite these advantages, GPUs continue to struggle with low energy efficiency, even when processing simpler PDEs on smaller grids\u00a0[15]. Several domain-specific accelerators have been developed to numerically solve PDEs using the FDM method.\nChen et al.\u00a0[15] proposed an FDM accelerator for 2D Laplace and Poisson equations on grids up to 128x128, utilizing processing-in-memory (PIM), though it suffers from limited computing precision.\nMu et al.\u00a0[16] developed an accelerator for the 2D Laplace equation on a 21x21 grid with dynamic computing precision.\nIn their subsequent work\u00a0[17], they expanded support to 3D Laplace equations on a 16x16x16 grid without external memory accesses, but both designs are limited to specific grid sizes.\nLi et al.\u00a0[18] recently introduced FDMAX, an elastic accelerator architecture designed to overcome some of these limitations.\nFDMAX can handle 2D Laplace, Poisson, Heat, and Wave equations with arbitrary grid sizes using 32-bit floating-point precision, offering notable improvements in performance and energy efficiency.\nHowever, existing solutions target the FDM method and do not address the Navier-Stokes equations.\nThis implies that adapting these architectures would still encounter major limitations due to the inherent constraints of FDM, particularly in handling complex geometries.\nAdditionally, these accelerators are based on ASIC designs, which are inflexible, costly, and time-consuming to fabricate\u00a0[19]. Once built, ASICs cannot be modified, making it challenging to adjust key parameters like boundary conditions or grid sizes that are essential for realistic CFD simulations.\nRecently, Friebel et al.\u00a0[20] introduced a FEM-based reconfigurable accelerator.\nHowever, their emphasis on resource-constrained FPGAs restricts performance as they do not take advantage of the capabilities provided by modern cloud FPGA devices. In this work, we present the first FEM-based reconfigurable accelerator specifically tailored for solving the Navier-Stokes equations.\nThe proposed accelerator employs FEM for spatial discretization, enabling it to adjust to the intricate geometries and setups typical in practical complex CFD applications. We developed our accelerator architecture using High-Level Synthesis (HLS), a user-friendly approach for programming FPGAs.\nRather than opting for an ASIC-based design, we utilize the reconfigurability of FPGAs, enabling flexible and dynamic hardware adaptation.\nThis allows for adjustments to different grid sizes or boundary conditions, for instance.\nFrom an architectural standpoint, we introduced tailored source code restructurings that enables HLS to exploit Task Level Parallelism (TLP). To further boost TLP efficiency, memory-aware optimizations are proposed to parallelize off-chip memory transfers to the FPGA\u2019s reconfigurable fabric, alongside directive-based HLS micro-architectural optimizations to enhance the accelerator\u2019s performance through Initiation Interval minimization.\nWe thoroughly evaluate the performance of our accelerator by deploying it on an AMD Alveo U200 FPGA, showing that the proposed solution achieves 7.9\u00d7\\times\u00d7 higher performance with respect to optimized Vitis-HLS implementations. In comparison with its software implementation counterpart mapped on a high-end server CPU, we show that the proposed solution delivers 45% latency gains in end-to-end CFD simulations, while dissipating 3.64\u00d7\\times\u00d7 less power. \nII Theoretical Background\n \nII-A Navier-Stokes Equations\n The Navier-Stokes PDEs describe the evolution of a fluid\u2019s velocity field over time, influenced by forces like pressure, viscous stresses, and external factors such as gravity\u00a0[7].\nWe focus on the 3D compressible Navier-Stokes equations, as detailed in\u00a0[14], which are described by the following equations: Equations\u00a01,\u00a02, and\u00a03 correspond to the conservation of mass, conservation of momentum, and the conservation of energy, respectively.\nThe spatiotemporarily dependent variables to be solved are the fluid density (\u03c1\ud835\udf0c\\rhoitalic_\u03c1), velocity (\ud835\udc96\ud835\udc96\\boldsymbol{u}bold_italic_u), and temperature (T\ud835\udc47Titalic_T).\nThe total energy (E\ud835\udc38Eitalic_E) and pressure (p\ud835\udc5dpitalic_p) are related to these variables through constitutive equations, following the ideal gas law.\nThe viscous stress tensor (\ud835\udf49\ud835\udf49\\boldsymbol{\\tau}bold_italic_\u03c4) represents the internal frictional forces within the fluid caused by its viscosity.\nThe terms \ud835\udc87\ud835\udc87\\boldsymbol{f}bold_italic_f and S\ud835\udc46Sitalic_S are the source terms of Equations\u00a02, and\u00a03, accounting for external energy inputs/losses within the system.\nThe parameter \u03ba\ud835\udf05\\kappaitalic_\u03ba is a constant that represents the fluid\u2019s thermal conductivity.\nWe solve the equations using the initial and boundary conditions defined by the Taylor-Green Vortex (TGV) problem\u00a0[21, 14]. \nII-B Numerical Methods\n Since the Navier-Stokes equations cannot be solved analytically, numerical methods are used to approximate the solution.\nGiven that \u03c1\ud835\udf0c\\rhoitalic_\u03c1, \ud835\udc96\ud835\udc96\\boldsymbol{u}bold_italic_u, and T\ud835\udc47Titalic_T are spatiotemporally dependent, we employ the Finite Element Method (FEM) for spatial discretization and the Runge-Kutta Method (RK) to compute the system\u2019s time evolution.\nIn the following paragraphs, we outline the fundamentals of these two methods. Finite Elements Method. Consider a convection-diffusion model of the form A\u2062(x)=0\ud835\udc34\ud835\udc650A(x)=0italic_A ( italic_x ) = 0, where A\u2062(x)=M\u2062(x)+C\u2062(x)+D\u2062(x)\ud835\udc34\ud835\udc65\ud835\udc40\ud835\udc65\ud835\udc36\ud835\udc65\ud835\udc37\ud835\udc65A(x)=M(x)+C(x)+D(x)italic_A ( italic_x ) = italic_M ( italic_x ) + italic_C ( italic_x ) + italic_D ( italic_x ).\nThe operators M\u2062(x)=\u2202x\u2202t\ud835\udc40\ud835\udc65\ud835\udc65\ud835\udc61M(x)=\\frac{\\partial x}{\\partial t}italic_M ( italic_x ) = divide start_ARG \u2202 italic_x end_ARG start_ARG \u2202 italic_t end_ARG, C\u2062(x)=\u2207\u22c5\ud835\udc87\u2062(x)\ud835\udc36\ud835\udc65\u22c5\u2207\ud835\udc87\ud835\udc65C(x)=\\nabla\\cdot\\boldsymbol{f}(x)italic_C ( italic_x ) = \u2207 \u22c5 bold_italic_f ( italic_x ), and D\u2062(x)=\u2212\u2207\u22c5(\u03bb\u2062\u2207x)\ud835\udc37\ud835\udc65\u22c5\u2207\ud835\udf06\u2207\ud835\udc65D(x)=-\\nabla\\cdot(\\lambda\\nabla x)italic_D ( italic_x ) = - \u2207 \u22c5 ( italic_\u03bb \u2207 italic_x ), correspond to the Mass, Convection, and Diffusion terms, respectively111Equations\u00a01,\u00a02, and\u00a03 can be mathematically expressed as Convection-Diffusion models\u00a0[22].\nFor the geometry to be simulated, we assume it is represented by a discretized mesh.\nThe mesh consists of volume elements defined by vertices and edges, allowing for the representation of complex geometries beyond simple cubes.\nThe unknown function x\ud835\udc65xitalic_x at each node of element e\ud835\udc52eitalic_e is represented by the vector \ud835\udc99e=[x1e\u2062\u2026\u2062xne]Tsuperscript\ud835\udc99\ud835\udc52superscriptdelimited-[]subscriptsuperscript\ud835\udc65\ud835\udc521\u2026subscriptsuperscript\ud835\udc65\ud835\udc52\ud835\udc5b\ud835\udc47\\boldsymbol{x}^{e}=[x^{e}_{1}\\,\\dots\\,x^{e}_{n}]^{T}bold_italic_x start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT = [ italic_x start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2026 italic_x start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT.\nTo approximate xesuperscript\ud835\udc65\ud835\udc52x^{e}italic_x start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT, a linear combination of n\ud835\udc5bnitalic_n shape functions Nisubscript\ud835\udc41\ud835\udc56N_{i}italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is used, each of which is equal to 1 at its respective node and 0 at all other nodes within the same element.\nThis leads to the trial function xe=\u2211ixie\u22c5Nisuperscript\ud835\udc65\ud835\udc52subscript\ud835\udc56\u22c5subscriptsuperscript\ud835\udc65\ud835\udc52\ud835\udc56subscript\ud835\udc41\ud835\udc56x^{e}=\\sum_{i}x^{e}_{i}\\cdot N_{i}italic_x start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT = \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_x start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u22c5 italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.\nThe trial function is then substituted into the original differential equation to compute the residual of the differential equation for that element.\nThe goal of FEM is to find the coefficients xiesubscriptsuperscript\ud835\udc65\ud835\udc52\ud835\udc56x^{e}_{i}italic_x start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT for all elements such that: Since these integrals are typically not solvable in closed form, the Gauss-Lobatto-Legendre (GLL) numerical integration technique is employed.\nThis reformulates Equation 4 as: where Wgsubscript\ud835\udc4a\ud835\udc54W_{g}italic_W start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT and \ud835\udf43\ud835\udc88subscript\ud835\udf43\ud835\udc88\\boldsymbol{\\xi_{g}}bold_italic_\u03be start_POSTSUBSCRIPT bold_italic_g end_POSTSUBSCRIPT denote the quadrature weights and points.\nThe computation of the Mass, Diffusion, and Convection terms at the quadrature points \ud835\udf43\ud835\udc88subscript\ud835\udf43\ud835\udc88\\boldsymbol{\\xi_{g}}bold_italic_\u03be start_POSTSUBSCRIPT bold_italic_g end_POSTSUBSCRIPT yields a linear system of equations of the form \ud835\udc72\u2062\ud835\udc99~=\ud835\udc83\ud835\udc72bold-~\ud835\udc99\ud835\udc83\\boldsymbol{K}\\boldsymbol{\\tilde{x}}=\\boldsymbol{b}bold_italic_K overbold_~ start_ARG bold_italic_x end_ARG = bold_italic_b where \ud835\udc99~=[(\ud835\udc99e1)T\u2062\u2026\u2062(\ud835\udc99em)T]Tbold-~\ud835\udc99superscriptdelimited-[]superscriptsuperscript\ud835\udc99subscript\ud835\udc521\ud835\udc47\u2026superscriptsuperscript\ud835\udc99subscript\ud835\udc52\ud835\udc5a\ud835\udc47\ud835\udc47\\boldsymbol{\\tilde{x}}=[(\\boldsymbol{x}^{e_{1}})^{T}\\dots(\\boldsymbol{x}^{e_{m%\n}})^{T}]^{T}overbold_~ start_ARG bold_italic_x end_ARG = [ ( bold_italic_x start_POSTSUPERSCRIPT italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT \u2026 ( bold_italic_x start_POSTSUPERSCRIPT italic_e start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT combines the unknown vectors of all elements. In this linear system, \ud835\udc83\ud835\udc83\\boldsymbol{b}bold_italic_b represents a constant term, while \ud835\udc72\ud835\udc72\\boldsymbol{K}bold_italic_K is a diagonal matrix that encapsulates the information from Equation\u00a05. Runge-Kutta Method. Consider an initial value problem for an Ordinary Differential Equation (ODE) of the form d\u2062yd\u2062t=f\u2062(t,y),y\u2062(t0)=y0formulae-sequence\ud835\udc51\ud835\udc66\ud835\udc51\ud835\udc61\ud835\udc53\ud835\udc61\ud835\udc66\ud835\udc66subscript\ud835\udc610subscript\ud835\udc660\\frac{dy}{dt}=f(t,y),\\quad y(t_{0})=y_{0}divide start_ARG italic_d italic_y end_ARG start_ARG italic_d italic_t end_ARG = italic_f ( italic_t , italic_y ) , italic_y ( italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT.\nThe Runge-Kutta method\u00a0[23] is a numerical approach used to solve ODEs when analytical solutions are difficult or impossible to obtain.\nIt enhances simpler methods by offering more accurate approximations at each time step, typically by evaluating the slope at multiple points within the interval.\nFollowing the approach outlined in\u00a0[14], we employed the fourth-order Runge-Kutta method (RK4), known for its effective balance between accuracy and computational efficiency. \nII-C Source Code Description & Characterization\n In this section, we provide a brief overview of the source code developed using the numerical methods discussed in\u00a0II-B to solve the problem described in\u00a0II-A.\nThe process begins with loading and preprocessing the discretized mesh.The main computation takes place within a time-stepping loop, where the algorithm proceeds through four stages of the Runge-Kutta (RK) method at each time step.\nDuring each stage, the algorithm computes the Diffusion and Convection terms based on FEM.\nAs illustrated in Figure\u00a01, the algorithm calculates the contribution of each grid element e\ud835\udc52eitalic_e to the diffusion and convection terms by loading the element\u2019s data, independently computing both terms, and storing the results for the next iteration.\nThis process requires computations across the nodes of each element.\nFirst, the node data are retrieved, followed by the computation of the gradient, \ud835\udf49\ud835\udf49\\boldsymbol{\\tau}bold_italic_\u03c4, and residuals.\nFinally, the node\u2019s contribution is stored.\nAfter each RK step, the algorithm updates the values of \u03c1,\ud835\udc96,T,E\ud835\udf0c\ud835\udc96\ud835\udc47\ud835\udc38\\rho,\\boldsymbol{u},T,Eitalic_\u03c1 , bold_italic_u , italic_T , italic_E, and p\ud835\udc5dpitalic_p.\nThis procedure continues until the solution is computed for all time steps. To pinpoint the most time-consuming parts, we performed a detailed profiling analysis across different input sizes, .i.e. number of mesh nodes, ranging from 1M to 4M.\nFigure\u00a02 shows the average breakdown of execution time.\nAs shown, the RK method was the most time-intensive, accounting for an average of 76.5% of the total execution time.\nWithin the RK method, the diffusion and convection functions emerged as the primary hotspots, consuming 39.2% and 21.04% of the total execution time, respectively.\nTherefore, the entire RK method is amenable for acceleration, with particular emphasis on optimizing convection and diffusion. Similar profiling data have been also recently reported\u00a0[4] targeting FEM-based multi-GPU CFD, further strengthening the validity of our results. \nIII FEM Reconfigurable Accelerator Design\n The following sections offer a detailed overview of the proposed accelerator architecture and the inter-task, memory and intra-task micro-architectural optimizations introduced for the RK method, i.e. the most computationally demanding component Section\u00a0II-C. The remaining computations are handled by the host CPU. We utilize HLS for introducing the proposed accelerator architecture and optimizing its FPGA mapping.\n \nIII-A Accelerator Architecture\n An overview of the proposed accelerator is illustrated in Figure\u00a03.\nThe CPU is responsible for handling the initialization and overseeing the iteration process across the time steps.\nIt is also responsible for transferring the necessary data via PCIe to the off-chip memory of the target FPGA.\nOn the other hand, the accelerator is divided into two separate kernels, with each assigned to a different Super Logic Region (SLR).\nThe Runge-Kutta Loop (RKL) kernel performs the core computations, while the Runge-Kutta Update (RKU) kernel evaluates \u03c1\ud835\udf0c\\rhoitalic_\u03c1, \ud835\udc96\ud835\udc96\\boldsymbol{u}bold_italic_u, T\ud835\udc47Titalic_T, E\ud835\udc38Eitalic_E, and p\ud835\udc5dpitalic_p at every time step.\nRKL is assigned to the SLR with direct access to off-chip memory, while RKU connects to the same memory through the Super Long Lines (SLL) interconnect.\nAlthough SLL connections have higher latency compared to intra-SLR fabric connections\u00a0[24], RKU is less time-consuming, and its data access patterns are far more regular than those in RKL. This RKL-RKU partitioning enables more effective utilization of FPGA\u2019s resources, by de-stressing the Place&Route phase from trading performance optimality for high resource utilization and routing congestion. RKL is optimized to effectively carry out the core computations of the FEM-based CFD simulation.\nSpecifically, for the computation of the Diffusion and Convection terms, the original source code was reorganized into a Load-Compute-Store form to exploit Task Level Pipelining (TLP).\nInitially, the data required for each element is transferred in batches from off-chip memory to the BRAMs and URAMs within the Programmable Logic (PL)\u00a0 1.\nThe next step involves executing the computations for the Diffusion and Convection terms\u00a0 2.\nSince these computations share significant functionality, as shown in Figure\u00a01, and no data dependencies are present, we combined these operations into a single module to improve hardware reuse during each element\u2019s computation.\nThis stage uses the node data already stored in the PL\u00a0 2a, calculates the gradients of both terms, as well as the \ud835\udf49\ud835\udf49\\boldsymbol{\\tau}bold_italic_\u03c4 and residuals\u00a0 2b, and finally stores the node\u2019s contribution to the total diffusion and convection calculations\u00a0 2c.\nOnce the diffusion and convection computations for the current element are completed, the data is written back to off-chip memory\u00a0 3.\nSince the Load, Compute, and Store tasks are sequential, where each task produces data that the next one consumes, they can be pipelined across iterations to enhance performance.\nAfter completing the RKL computations for the current time step, the RKU evaluates \u03c1\ud835\udf0c\\rhoitalic_\u03c1, \ud835\udc96\ud835\udc96\\boldsymbol{u}bold_italic_u, T\ud835\udc47Titalic_T, E\ud835\udc38Eitalic_E, and p\ud835\udc5dpitalic_p.\nOnce completed, the iteration concludes, and the CPU begins the next one. \nIII-B Task Level Pipelining\n Utilizing instruction-level optimizations, such as loop pipelining and unrolling, is a standard approach when accelerating applications on FPGAs with High-Level Synthesis.\nHowever, relying solely on these optimizations often results in limited overall performance, resource over-utilization, and memory bandwidth bottlenecks, as they focus primarily on fine-grained loop-level improvements.\nFor example, applying loop pipelining to the outer loop of a nested structure, which is common in most scientific computations, often requires fully unrolling the inner loops for the pipeline to function effectively, leading to kernels that may exceed the available resources of the FPGA.\nMoreover, these approaches do not exploit coarse-grained parallelism, which limits scalability and leaves substantial performance potential unutilized. Task Level Pipelining (TLP) forms a key factor for effective Dataflow Optimization, and it is an effective approach for addressing these limitations, providing enhanced performance and improved resource efficiency.\nTLP involves partitioning the core computation into N\ud835\udc41Nitalic_N sequential tasks, T\u2062a\u2062s\u2062k1,T\u2062a\u2062s\u2062k2,\u2026,T\u2062a\u2062s\u2062kN\ud835\udc47\ud835\udc4e\ud835\udc60subscript\ud835\udc581\ud835\udc47\ud835\udc4e\ud835\udc60subscript\ud835\udc582\u2026\ud835\udc47\ud835\udc4e\ud835\udc60subscript\ud835\udc58\ud835\udc41Task_{1},Task_{2},\\dots,Task_{N}italic_T italic_a italic_s italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_T italic_a italic_s italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , italic_T italic_a italic_s italic_k start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT, where each task passes data to the next through inter-task buffers, which can be either First-In-First-Out (FIFO) or Ping-Pong (PIPO) buffers.\nThe N\ud835\udc41Nitalic_N tasks form the TLP stages, with the most time-consuming task determining the Initiation Interval (II), which represents the number of clock cycles needed before the next iteration of the entire pipeline can begin.\nIn a specific time step of the pipeline, the inter-task buffers temporarily store the data produced by T\u2062a\u2062s\u2062kk\ud835\udc47\ud835\udc4e\ud835\udc60subscript\ud835\udc58\ud835\udc58Task_{k}italic_T italic_a italic_s italic_k start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT. The data will then be processed by T\u2062a\u2062s\u2062kk+1\ud835\udc47\ud835\udc4e\ud835\udc60subscript\ud835\udc58\ud835\udc581Task_{k+1}italic_T italic_a italic_s italic_k start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT, while T\u2062a\u2062s\u2062kk\ud835\udc47\ud835\udc4e\ud835\udc60subscript\ud835\udc58\ud835\udc58Task_{k}italic_T italic_a italic_s italic_k start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT runs concurrently the next iteration. As illustrated in Figure\u00a03, we identified two areas where TLP can be applied: (a) the element-wise computations (i.e., tasks  1,  2, and  3), and (b) the node-wise computations within each element (i.e., tasks  2a,  2b, and  2c).\nNoting that the Diffusion and Convection terms share considerable functionality and perform nearly identical computations, and with no data dependencies detected, we code-merged these similar operations into a single function/module to enhance hardware reuse.\nTo apply TLP optimization effectively and prevent deadlocks, two key conditions were considered\u00a0[25].\nFirst, the Single-Producer-Single-Consumer rule was established to ensure that each task has a single producer providing data and a single consumer receiving it, facilitating smooth data flow and preventing conflicts.\nSecond, it was ensured that inter-task buffers do not bypass any tasks and transfer data sequentially, thereby maintaining the integrity of the pipeline process.\nMeeting these conditions can be particularly challenging for complex applications like our CFD simulation and often requires extensive manual rewriting.\nSections\u00a0III-C and\u00a0III-D provide a detailed examination of how we optimized off-chip memory reads within our tasks, followed by the fine-grained HLS directive optimizations for higher Instruction-Level-Parallelism (ILP). \nIII-C Off-chip Memory Transfer Parallelization\n This section discusses the two primary optimizations applied to the tasks in our pipeline that interact with off-chip memory.\nThese optimizations aim to enhance memory throughput and prevent contention, which can hinder the performance of TLP. Arrays to Memory Channel Assignment.\nThe tasks depicted in Figure\u00a03 contain loops that access off-chip memory through one or more AXI interfaces.\nThese interfaces are connected to an AXI-Interconnect, which in turn connects to the off-chip memory.\nEach memory access, typically corresponding to an array element, must be explicitly mapped to a specific AXI interface.\nTo minimize iteration latency in these loops, we schedule memory accesses concurrently by assigning them to separate AXI interfaces, as depicted in Figure\u00a04.\nFigure\u00a04 shows a code snippet of Load-Element task  1, demonstrating how the respective AXI interface directives are applied to assign these memory accesses to different AXI interfaces.\nThis approach eliminates interface contention, which would otherwise force the memory accesses to occur sequentially.\nHowever, at certain stages of the algorithm, the arrays to be transferred exceed the available AXI interfaces, making it impossible to assign each array individually.\nTo overcome this limitation, we implement interface reuse for arrays accessed by different tasks during successive steps of the algorithm, such as the LOAD-Element and STORE-Element-Contribution tasks.\nSince these loops are not executed in parallel, this method ensures that arrays sharing the same interface do not compete for memory bandwidth, thereby optimizing data transfer efficiency. Decoupling Memory Load and Store Interfaces: In the RK method, we often encounter loops iterating over arrays stored in off-chip memory, executing operations like x\u2062[i]\u2190f\u2062(x\u2062[i],y\u2062[i])\u2190\ud835\udc65delimited-[]\ud835\udc56\ud835\udc53\ud835\udc65delimited-[]\ud835\udc56\ud835\udc66delimited-[]\ud835\udc56x[i]\\leftarrow f(x[i],y[i])italic_x [ italic_i ] \u2190 italic_f ( italic_x [ italic_i ] , italic_y [ italic_i ] ), where f\ud835\udc53fitalic_f is the function applied to arrays x\ud835\udc65xitalic_x and y\ud835\udc66yitalic_y.\nThese arrays retrieve data from off-chip memory through an AXI interface.\nConsequently, the same AXI interface is responsible for reading the values of x\ud835\udc65xitalic_x and writing back the updated results. This inter-iteration dependency hinders loop pipelining, ultimately slowing down the overall execution.\nTo enable pipelined updates, we introduce an additional interface dedicated to x\ud835\udc65xitalic_x, where one interface handles reading and the other manages writing.\nThis approach resolves the inter-iteration dependency, allowing for pipelined memory updates. \nIII-D HLS Directives for TLP Initiation Interval Optimization\n As outlined in Section\u00a0III-B, the Initiation Interval (II) of TLP is determined by the most time-consuming task.\nIn this section, we focus on reducing the TLP\u2019s II, and thus consequently, the overall execution time of the simulation.\nSince our goal is to reduce the TLP II, we focus on optimizing the task with the highest latency in an iterative manner, i.e. the HLS optimization directives are applied each time to the task exposing the highest latency criticality.\nOptimizing all available tasks could result in resource violations due to the limited capacity of the FPGA.\nMoreover, focusing on low-latency tasks would offer minimal performance improvements.\n We prioritize tasks  2a,  2b, and  2c for optimization since they are the most latency-critical.\nFurthermore, these tasks gain from processing data stored directly in the PL, where small matrices are housed in the 32KB BRAMs and larger matrices that surpass BRAM capacity are stored in the 288KB URAMs.\nThe data is fetched through task\u00a0 1. More in detail, we primarily concentrate on optimizing intra-task micro-architecture, by applying three specific HLS directives: a) loop unrolling, b) loop pipelining, and c) array partitioning.\nAfter identifying in each step the most latency critical task, we examined HLS directive placement in an intra-task manner. Specifically, for each critical task identified in the current step,\n1. we extract the for-loops with a high trip count and multiple operations in the loop body, 2. we examine potential inter-iteration dependencies and 3. we apply the loop pipelining directive.\nFor these large loops, we did not perform unrolling, as this would duplicate the loop body by the factor used, resulting in high resource utilization.\nFor the for-loops with small trip counts, we completely unrolled them based on the factors allowed by our available resources.\nTo enable the parallel data accesses required by our directives, we also apply array partitioning with the appropriate factors.\nThis procedure is repeated until no further optimization could be achieved, either due to unresolved dependencies or resource over-utilization, which would result in lower clock frequencies.\n \nIV Experimental Evaluation\n We implement and evaluate the proposed accelerator architecture regarding performance, resources utilization, and energy efficiency.\nTo convert the developed C++ simulation source and its optimizations into HDL, we utilized Xilinx Vitis HLS 2021.1 and the Xilinx Vitis Unified Software Platform 2021.1.We chose the AMD Alveo U200 as the target FPGA. The Alveo U200 card includes 3 Super Logic Regions (SLRs) and 4 DDR memories, each with a capacity of 16GB.\nCommunication with the host is enabled via PCIe and the Xilinx Runtime (XRT).  \nIV-A Comparison with Vitis-HLS Optimized Design\n As an initial step, we compare the proposed accelerator with the Vitis-HLS optimized design. Recent Vitis-HLS release applies the following HLS directive as general optimization strategy: i) automatic loop pipelining using the flag config_compile -pipeline_loops, ii) unrolling of small tripcount loops through config_unroll -tripcount_threshold, and iii)\ncomplete partitioning of small arrays using config_array_partition -complete_threshold.\nTo assess the effect of input data size, we measure the total execution time of the computationally intensive components of our application, specifically RKU and RKL, for varying numbers of mesh nodes.\nAs illustrated in Figure\u00a05, increasing the number of nodes in the examined mesh results in longer execution times for the RK method in both baselines.\nSpecifically, increasing the number of nodes from 1.4M to 4.2M results in a 3.4\u00d73.4\\times3.4 \u00d7 increase in execution time for both the proposed design and the Vitis-optimized version.\nThe proposed approach consistently surpasses the Vitis optimization across all tested node counts, achieving an average improvement of 7.9\u00d77.9\\times7.9 \u00d7.\nThe lower performance can be partially attributed to the Vitis-optimized kernel being restricted to a 100 MHz clock frequency, whereas the proposed design operates at 150 MHz.\nThis limitation of the Vitis-HLS optimized design arises from both the RKL and RKU modules being mapped onto the same SLR, which caused significant routing congestion and restricted the maximum clock speed. Regarding resource utilization, as shown in Table\u00a0I, our optimized design leads to 1.5\u00d71.5\\times1.5 \u00d7 higher FF% and LUT%, a 1.9\u00d71.9\\times1.9 \u00d7 higher BRAM% and DSP%, and a 16.8\u00d716.8\\times16.8 \u00d7 higher URAM%, compared to Vitis-HLS optimized design.\nAlthough the increase in URAM usage is significant, i.e. Vitis-HLS treats URAM as scarce resource, the utilization of other resources shows no more than a two-fold increase compared to Vitis-HLS optimizations.\nThis indicates that we achieve substantial performance gains with only minimal increases in resource utilization. \nIV-B Comparison with Server CPU\n We compared the proposed FPGA accelerated solution with its software implementation counterpart, i.e. the exact same C++ implementation running in single-threaded mode on a high-performance Linux server, specifically equipped with an Intel Xeon Silver 4210 CPU @ 2.20GHz with 32K L1D/I, 1M L2 and 14M L3 cache.\nThis choice offers a more balanced comparison than alternatives often cited in the literature, such as the ARM Cortex-A53\u00a0[20], due to the Xeon\u2019s superior processing capabilities.\nFor this comparison, we used a 4.2M node mesh, which closely represents a real-world scenario. Our design achieved a 45% reduction in execution time, demonstrating the effectiveness of the proposed solution.\nAnother key metric showcasing the potential of our implementation is power consumption.\nThe CPU implementation consumed an average of 120.42W across all test cases with varying mesh sizes.\nIn contrast, the FPGA averaged 32.4W for the core application, with an additional 30.7W for peripherals and 1.7W for the rest of the system, resulting in an average power consumption that is 3.64\u00d73.64\\times3.64 \u00d7 lower than the CPU.\nThese initial results highlight the potential of reconfigurable accelerators for efficiently accelerating FEM-based simulations, a field that remains unexplored. \nV Conclusion\n In this work, we presented a high-performance FPGA accelerator tailored for numerically solving the Navier-Stokes equations, with a focus on FEM due to its capability to accurately represent complex geometries and intricate real-world scenarios.\nThe proposed accelerator, implemented with HLS on AMD Alveo U200 FPGA, delivers 7.9\u00d7\\times\u00d7 better performance than the Vitis-HLS optimized version, while compared with its software implementation counterpart running on a high-end server it reduces latency by 45% while consuming 3.64\u00d7\\times\u00d7 less power.\nThis underscores the potential of our approach for solving the Navier-Stokes equations more efficiently, paving the way for addressing even more challenging CFD simulations. References"}
{"text": "tcb@breakable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n MixPE: Quantization and Hardware Co-design for Efficient LLM Inference Transformer-based large language models (LLMs) have achieved remarkable success as model sizes continue to grow, yet their deployment remains challenging due to significant computational and memory demands. Quantization has emerged as a promising solution, and state-of-the-art quantization algorithms for LLMs introduce the need for mixed-precision matrix multiplication (mpGEMM), where lower-precision weights are multiplied with higher-precision activations. Despite its benefits, current hardware accelerators such as GPUs and TPUs lack native support for efficient mpGEMM, leading to inefficient dequantization operations in the main sequential loop. To address this limitation, we introduce MixPE, a specialized mixed-precision processing element designed for efficient low-bit quantization in LLM inference. MixPE leverages two key innovations to minimize dequantization overhead and unlock the full potential of low-bit quantization. First, recognizing that scale and zero point are shared within each quantization group, we propose performing dequantization after per-group mpGEMM, significantly reducing dequantization overhead. Second, instead of relying on conventional multipliers, MixPE utilizes efficient shift&add operations for multiplication, optimizing both computation and energy efficiency. Our experimental results demonstrate that MixPE surpasses the state-of-the-art quantization accelerators by 2.6\u00d72.6\\times2.6 \u00d7 speedup and 1.4\u00d71.4\\times1.4 \u00d7 energy reduction. \n1. Introduction Large language models have sparked a new revolution across a broad spectrum of tasks, exerting a profound influence on our daily lives.\nHowever, the colossal size of LLMs results in high computation and energy costs to train and serve these models. The AI industry is applying many techniques to reduce the cost of models. Quantization is one critical instance of these techniques, in which individual tensor values are cast from full-precision FP32 to a cheaper numeric standard. The most popular quantization formats include INT4, INT8, and FP16, given their vast hardware support. Based on these data formats, state-of-the-art integer quantization algorithms can be divided into three categories: 8-bit weight and 8-bit activation (W8A8), 4-bit weight and 16-bit activation (W4A16), 4-bit weight and 8-bit activation (W4A8). The later two mixed-precision methods, i.e., quantizing LLMs with low-precision weights and high-precision activations, have become particularly attractive as they save memory and computation costs while maintaining model accuracy\u00a0(lin2024qserve, ; TRT, ). This is because activations are harder to quantize than weights. More specifically, the weight distribution is quite uniform and flat, which is relatively easy to quantize. Previous work has shown that quantizing the weights of LLMs with 8-bit, 4-bit or even with 1-bit does not significantly degrade accuracy\u00a0(dettmers2022int8, ; yao2022zeroquant, ; ma20241bit, ). Conversely, activations are generated on-the-fly with a high variance, noticeably presented as dynamic outliers\u00a0(xiao2023smoothquant, ; guo2023olive, ; lin2024awq, ). These outliers can lead to significant accuracy degradation. Mixed-precision quantization shifts the key computation pattern of LLM inference from conventional General Matrix Multiplication (GEMM) to mixed-precision GEMM (mpGEMM), where the weight matrix is in lower precision (e.g., INT4) and the activation matrix remains in higher precision (e.g., INT8/FP16).\nHowever, current hardware, such as GPUs and TPUs, does not natively support mpGEMM. Consequently, low-bit LLM inference systems have to implement inefficient dequantization-based approaches for mpGEMM\u00a0(lin2024qserve, ; TRT, ). Specifically, low-bit weights are dequantized to high precision before performing GEMM. Such extra operations are performed in the main loop and can become a performance bottleneck. When batch size is relatively small (\u22644absent4\\leq 4\u2264 4), dequantization overhead can account for over 20% of the overall computation, as shown in the left figure of Figure\u00a01. Meanwhile, as the GEMM is still computed in high precision, dequantization-based mpGEMM cannot take full advantage of low-precision computation. Therefore, reducing dequantization overhead is crucial for achieving optimal throughput in LLM inference. Motivated by this, we introduce a Mixed-precision Processing Element, MixPE, to accelerate low-bit LLM inference with a dedicated mpGEMM solution.\nUnlike conventional dequantization-based approaches, MixPE performs direct mixed-precision GEMM and postpones dequantization to after the per-group GEMM computation. This approach leverages the fact that the scale factor and zero point are shared within each quantization group, allowing for more efficient handling of low-precision weights. Additionally, to fully harness the benefits of low-precision computation, MixPE replaces traditional multipliers with efficient, low-power shift&add operations for multiplying low-bit weights with high-bit activations.\nThrough holistically co-designing software (dequantization after mpGEMM) and hardware, i.e., shift&add-based processing element (PE), MixPE not only reduces dequantization overhead but also exploits low-precision arithmetic for achieving high-throughput, energy-efficient LLM inference. Compared to conventional INT8-based TPUs, MixPE can achieve over 4\u00d74\\times4 \u00d7 speedup (as shown in the right figure in Figure\u00a01) on LLM inference. Moreover, we present a parameterized design space exploration (DSE) framework that enables the evaluation of various GEMM accelerators. This framework allows us to map out the Pareto frontier, highlighting the optimal trade-offs between numerical fidelity and hardware efficiency. Our key contributions are as follows: We introduce MixPE, a novel hardware accelerator that efficiently handles mixed-precision GEMM with minimal dequantization overhead by co-optimizing the quantization scheme and PE design. We present a DSE framework that formalizes the design space as a trade-off between numerical fidelity and hardware cost. Through an exhaustive sweep of design variables, we identify MixPE as the Pareto frontier of numerical accuracy and performance. Experimental results demonstrate that, with W4A8 quantization, MixPE achieves a 2.6\u00d72.6\\times2.6 \u00d7 speedup and 1.4\u00d71.4\\times1.4 \u00d7 energy savings compared to state-of-the-art quantization accelerators. Additionally, for W4A16, MixPE delivers a 2.44\u00d72.44\\times2.44 \u00d7 speedup while reducing energy consumption by 68% compared to traditional FP16 multiplication PEs. \n2. Related Work \n2.1. Integer Quantization in LLMs Nowadays, there is a rapid growth in the scale of large language models, which in turn requires significant hardware resources. For example, Llama-3-70B\u00a0(dubey2024llama3, ) consumes approximately 148GB of memory just to hold its model weights (in FP16), far exceeding the capacity of a modern GPU like NVIDIA A100 or H100. This imposes a considerable challenge for LLM deployment. To reduce inference costs in LLM deployment, low-bit integer quantization, which maps floating point tensors to discrete level, has become a popular approach\u00a0(guo2023olive, ; zhao2024atom, ; lin2024qserve, ). Given n bits to represent the integer, the quantization process can be formulated as: where x\ud835\udc65xitalic_x is the floating-point value, qxsubscript\ud835\udc5e\ud835\udc65q_{x}italic_q start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT is the n-bit quantized counterpart, s\ud835\udc60sitalic_s is the scaling factor and z\ud835\udc67zitalic_z is the zero point. Therefore, the dequantization process can be represented as: For integer quantization, INT4 and INT8 formats are prevalently utilized in low-bit LLM inference. In this paper, we denote x-bit weight and y-bit activation quantization in LLMs as WxAy for abbreviation. Apart from bit precision, quantization can be achieved with different granularity, resulting in different trade-offs between accuracy and efficiency. For per-tensor quantization, the entire tensor shares one set of scale and zero-point\u00a0(nagel2021white, ). Denoting channel as the last dimension of the input matrix, per-channel quantization for weights or per-token quantization for activations means that s\ud835\udc60sitalic_s and z\ud835\udc67zitalic_z are shared within each row of tensor\u00a0(xiao2023smoothquant, ). Dividing each channel into several sub-groups (g columns), per-group quantization further reduces the degree of parameter sharing by using different s\ud835\udc60sitalic_s and z\ud835\udc67zitalic_z for every group within each row\u00a0(lin2024awq, ). The finer the granularity, the more precise the quantization, but the higher the overhead. Additionally, group quantization is often employed to improve the accuracy of 4-bit quantization. A typical group size is 128\u00a0(zhao2024atom, ; lin2024awq, ; lin2024qserve, ), where each contiguous block of 128 elements is quantized as a single group, allowing for more accurate representation of the data within each group. \n2.2. mpGEMM in Low-bit LLM inference In decoder-only LLMs, using different bit-widths for weights and activations creates a unique requirement for mixed-precision General Matrix Multiplication, where the weight matrix uses lower precision while the activation matrix remains in higher precision. This mixed-precision requirement applies to primary GEMM operations in both multi-head attention and feed-forward blocks when weight and activation quantization bits differ (as illustrated in Figure\u00a02).\nHowever, current commercial hardware like GPUs and TPUs primarily support canonical GEMM, where both inputs are in the same format and bit-width, and lack native support for mpGEMM. Existing mpGEMM methods generally fall into two main categories: indirect mpGEMM, primarily dequantization-based mpGEMM, and direct mpGEMM, which includes lookup table (LUT)-based mpGEMM and processing element (PE)-based mpGEMM approaches. Indirect mpGEMM\nDequantization-based mpGEMM first upscale the low-precision weights to match the high-precision activations and then perform conventional GEMM\u00a0(TRT, ; lin2024awq, ; lin2024qserve, ). Fig illustrates a dequantization-based mpGEMM example, where INT4 weights are multiplied by FP16 activations. While this approach supports various precision combinations, dequantization introduces additional operations that may become a performance bottleneck. Moreover, since the GEMM is executed in high precision, dequantization-based mpGEMM cannot fully exploit the benefits of low-precision computation. Direct mpGEMM\nLUT-based mpGEMM is one direct mpGEMM approach that uses lookup tables (LUTs) to implement mpGEMM\u00a0(park2022lut, ; maleki2023look, ; mo2024lut, ). Specifically, it precomputes dot products of high-precision activations with a limited set of low-precision weights and replaces the computation with simple lookups in the resulting table.\nHowever, due to limited LUT support on existing LLM inference hardwares like GPUs, LUT-based mpGEMM approaches are often less effective than dequantizaiton-based approaches. Moreover, storing a lookup table for each potential weight-activation combination can require significant memory, especially as the range of data format increases. Another direct mpGEMM approach involves designing specialized processing elements (PEs) that can directly execute mixed-precision GEMM computations. BitFusion\u00a0(sharma2018bit, ) proposes to leverage tensor bit-width adaptivity to reuse low-precision components (e.g., 4-bit integers) for higher-precision calculations (e.g., 8-bit integers) without additional overhead.\nHowever, BitFusion is restricted to integer types, which limits its quantization efficiency and often results in increased memory and computation requirements. OLAccel\u00a0(park2018ola, ) performs sparse high-precision computation for outliers in parallel with dense low-precision computation for regular tensors.\nHowever, it stores tensors in off-chip memory, which results in longer memory access latencies and thus lower throughput. Both ANT\u00a0(guo2022ant, ) and Olive\u00a0(guo2023olive, ) propose novel data formats for efficiently storing large values within tensors.\nHowever, these approaches require specialized data decoders, introducing additional area and computation overhead.\nIn contrast, our work, MixPE, introduces a flexible mpGEMM computation design that supports both integer and floating-point formats without introducing hardware overhead. \n3. Method \n3.1. Motivation Quantizing weights and activations can significantly reduce memory usage and improve computational throughput. Although model weights are known in advance and typically exhibit uniform distributions, achieving accurate representation remains challenging due to the limited precision of 4-bit integer formats. To further improve precision, group quantization is widely used. This method divides the weight matrix into subgroups and performs quantization within each group. To incorporate group quantization into the conventional GEMM pipeline, state-of-the-art quantization algorithms, including TensorRT-LLM-W4A16\u00a0(TRT, ) and QServe-W4A8\u00a0(lin2024qserve, ), require dequantization operations in the main loop, as shown in the left figure of Figure\u00a03. For an m\u00d7n\u00d7k\ud835\udc5a\ud835\udc5b\ud835\udc58m\\times n\\times kitalic_m \u00d7 italic_n \u00d7 italic_k quantized GEMM problem, m\ud835\udc5amitalic_m represents the number of sequences (or batch size), while n\ud835\udc5bnitalic_n and k\ud835\udc58kitalic_k correspond to channel dimensions. Both m\ud835\udc5amitalic_m and n\ud835\udc5bnitalic_n are parallelizable dimensions, whereas k\ud835\udc58kitalic_k serves as the reduction dimension, requiring a sequential main loop. The main loop includes over 100 iterations, which significantly dominate the runtime of the GEMM operation. The dequantization process in the main loop introduces two significant efficiency bottlenecks. First, dequantization requires additional multiplication and subtraction operations (see Equation\u00a02) on the large n\u00d7k\ud835\udc5b\ud835\udc58n\\times kitalic_n \u00d7 italic_k weight matrix. In LLM inference, the batch size m\ud835\udc5amitalic_m is typically small (e.g., 8, 16, 32), whereas n\ud835\udc5bnitalic_n and k\ud835\udc58kitalic_k are much larger (e.g., 2048 or 4096). Upscaling such a large matrix during each iteration adds substantial overhead, quickly becoming a performance bottleneck. Second, the GEMM operations in the left figure of Figure\u00a03 are still performed in high precision. This high-precision computation not only reduces the potential performance gains from low-bit quantization but also leads to increased power consumption and memory bandwidth usage. We preview our group-quantized mpGEMM design in the right figure of Figure\u00a03. Observing each group shares a common set of scale and zero-point values, we propose performing mixed-precision GEMM first (Step \u2460), followed by per-group dequantization (Step \u2461). To enable this, we design a mixed-precision processing element optimized for efficient low-bit computation, unlocking the full potential of low-precision arithmetic. The dot product result for each group is then dequantized and accumulated to produce the final output (Step \u2462), thereby significantly reducing dequantization overhead in the main loop. \n3.2. Dequantize After GEMM Consider k\ud835\udc58kitalic_k-dimensional weight and activation vectors: where \ud835\udc30\ud835\udc30\\mathbf{w}bold_w is a row vector of the weight matrix \ud835\udc16\ud835\udc27\u00d7\ud835\udc24subscript\ud835\udc16\ud835\udc27\ud835\udc24\\mathbf{W_{n\\times k}}bold_W start_POSTSUBSCRIPT bold_n \u00d7 bold_k end_POSTSUBSCRIPT and \ud835\udc31\ud835\udc31\\mathbf{x}bold_x is a row vector of the activation matrix \ud835\udc17\ud835\udc26\u00d7\ud835\udc24subscript\ud835\udc17\ud835\udc26\ud835\udc24\\mathbf{X_{m\\times k}}bold_X start_POSTSUBSCRIPT bold_m \u00d7 bold_k end_POSTSUBSCRIPT. Based on Equation\u00a02, their inner product can be expressed as: where swisubscript\ud835\udc60subscript\ud835\udc64\ud835\udc56s_{w_{i}}italic_s start_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT and zwisubscript\ud835\udc67subscript\ud835\udc64\ud835\udc56z_{w_{i}}italic_z start_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT are the scale and zero point of wisubscript\ud835\udc64\ud835\udc56w_{i}italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, respectively. Here xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and Qwisubscript\ud835\udc44subscript\ud835\udc64\ud835\udc56Q_{w_{i}}italic_Q start_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT are in different precisions, with Qwisubscript\ud835\udc44subscript\ud835\udc64\ud835\udc56Q_{w_{i}}italic_Q start_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT in INT4 and xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT in either INT8 or FP16. To perform GEMM on GPUs, Qwisubscript\ud835\udc44subscript\ud835\udc64\ud835\udc56Q_{w_{i}}italic_Q start_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT must be dequantized before being multiplied with xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, corresponding to the left figure in Figure\u00a03.\nIn particular, we observe that, when group quantization is employed, the scale factor and zero-point are shared within each group. Let the number of groups be Ng\u2062r\u2062o\u2062u\u2062p=k/gsubscript\ud835\udc41\ud835\udc54\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc5d\ud835\udc58\ud835\udc54N_{group}=k/gitalic_N start_POSTSUBSCRIPT italic_g italic_r italic_o italic_u italic_p end_POSTSUBSCRIPT = italic_k / italic_g, where g\ud835\udc54gitalic_g is the group size, and denote the set of element indices in the n\ud835\udc5bnitalic_n-th group as Gnsubscript\ud835\udc3a\ud835\udc5bG_{n}italic_G start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. As a result, we can express the dot product in Equation\u00a04 as follows: Here sGnsubscript\ud835\udc60subscript\ud835\udc3a\ud835\udc5bs_{G_{n}}italic_s start_POSTSUBSCRIPT italic_G start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT and zGnsubscript\ud835\udc67subscript\ud835\udc3a\ud835\udc5bz_{G_{n}}italic_z start_POSTSUBSCRIPT italic_G start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT represent the scale factor and zero point of Gnsubscript\ud835\udc3a\ud835\udc5bG_{n}italic_G start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT, respectively.\nFrom Equation\u00a05, we observe two key points. First, if multiplication between Qwjsubscript\ud835\udc44subscript\ud835\udc64\ud835\udc57Q_{w_{j}}italic_Q start_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT and xjsubscript\ud835\udc65\ud835\udc57x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT can be performed directly, dequantization can be applied after the inner-group dot product without sacrificing accuracy. This approach reduces the frequency of dequantization operations to 1/g1\ud835\udc541/g1 / italic_g of the original rate, significantly decreasing dequantization overhead in the main loop. Second, the computational complexity of the first term in Equation\u00a05 is O\u2062(k)\ud835\udc42\ud835\udc58O(k)italic_O ( italic_k ), while the second term is O\u2062(1)\ud835\udc421O(1)italic_O ( 1 ). As a result, most existing accelerators for Transformers\u00a0(wang2021spatten, ; zou2024bie, ) prioritize accelerating the inner product term, with the remaining summation term delegated to software or other specialized computation units. Thus, designing an efficient computation engine for the mixed-precision dot product is essential to achieving optimal mpGEMM performance in LLM inference. \n3.3. Mixed-Precision Processing Elements State-of-the-art quantization algorithms for LLM inference often quantize model weights to low-bit integer formats, achieving high theoretical throughput and a reduced memory footprint. In computer systems, integers are represented in binary with a fixed bit width, where each bit can be either 0 or 1. For a 4-bit integer (INT4), this allows for 24=16superscript24162^{4}=162 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT = 16 possible values, enabling INT4 to represent up to 16 distinct numbers. For W4A8 quantization, we employ an unsigned 4-bit (UINT4) quantization on weights as QServe\u00a0(lin2024qserve, ), and introduce a specialized mixed-precision processing element design tailored for quantized GEMM: where w\ud835\udc64witalic_w and x\ud835\udc65xitalic_x represent the values in UINT4 weights and INT8 activations, respectively, and <<much-less-than<<< < denotes the left shift operator for integer data formats. Due to the structure of integer representation, original multiplications can be efficiently replaced by bit shifts. Specifically, shifting left by n bits is equivalent to multiplying by 2nsuperscript2\ud835\udc5b2^{n}2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT. Based on Equation\u00a06, we implement MixPE using shift and add operations, enabling highly efficient mixed-precision GEMM with weights in INT4 and activations in INT8. Theoretically, MixPE for W4A8 can achieve 2\u00d72\\times2 \u00d7 speed-up, 25% - 50% memory cost savings, and 25% - 50% communication savings, compared with the current INT8 multiplication module in tensor core, which is very promising for scaling-up next-generation hardware accelerators. Then, for W4A16 operations, even though bitwise shifts cannot replace multipliers as they are not applicable to floating-point values, efficient techniques still exist to achieve fast scaling by powers of two without the need for a traditional power-intensive multiplier. The MixPE design for W4A16 is detailed below: where \u2297tensor-product\\otimes\u2297 represents a dedicated RTL module designed for efficient multiplication of floating-point values and powers of two. FP16 follows the IEEE 754\u00a0(kahan1996ieee, ) standard, with a 1-5-10 bit format: 1 sign bit, 5 exponent bits, and 10 mantissa (fraction) bits, yielding an actual value of: value(FP16) = (\u22121)sign\u00d72E-bias\u00d7(1.mantissa)(-1)^{\\text{sign}}\\times 2^{\\text{E-bias}}\\times(1.\\text{mantissa})( - 1 ) start_POSTSUPERSCRIPT sign end_POSTSUPERSCRIPT \u00d7 2 start_POSTSUPERSCRIPT E-bias end_POSTSUPERSCRIPT \u00d7 ( 1 . mantissa ). With this format, scaling an FP16 value by a power of two involves adjusting only the exponent bits, while the sign and mantissa remain unchanged. Specifically, we first perform a shift operation on the exponent bits, followed by bit-wise addition, enabling efficient power-of-two scaling. This process is similar to the INT4\u00d7\\times\u00d7INT8 procedure, leveraging the same shift&add approach for efficient computation. By directly modifying the exponent in FP16, we can scale activations by powers of two without traditional power-intensive multipliers. This design choice in MixPE for W4A16 significantly reduces computational overhead, minimizing operations and avoiding the latency and energy costs of conventional multipliers. In the Figure\u00a04, we provide a detailed comparison of the INT4\u00d7\\times\u00d7INT8 weight-activation processing element between traditional multiplier-based designs and MixPE. The same topology applies to W4A16, where we also leverage shift&add operations on the exponent bits of FP16 activations to achieve power-of-two scaling. In traditional multiplier-based PE, the INT4 activation must first be converted to INT8 before performing multiply-accumulate operations using standard multipliers (see the left figure in Figure\u00a04), which fails to capitalize on the computational advantages of INT4. In contrast, MixPE achieves efficient computation between INT4 and INT8 by directly leveraging hardware-friendly shift operations and an optimized adder tree (see the right figure in Figure\u00a04), significantly improving both power efficiency and throughput. We then describe the architectural design for integrating MixPE into a systolic array, a structure also used in commercial deep learning accelerators like Google\u2019s TPU\u00a0(jouppi2017tpu, ). Our design aligns with practical settings, where the weight tensor is quantized to low bit-width while the activation and output tensors retain higher precision. As such, we find that our design achieves the best benefits on the systolic array with the output-stationary dataflow\u00a0(sharma2018bit, ). The integration of MixPE into the output stationary systolic array architecture is illustrated in Figure\u00a05. This architecture, which communicates with memory via a global buffer, includes an input/weight buffer, multiple MixPE units, and an output buffer. The key innovation here is the MixPE design, which replaces traditional multipliers with efficient shifting and scaling modules. By incorporating MixPE, the systolic array achieves a hardware-efficient and energy-saving solution for mixed-precision GEMM computations, advancing quantization techniques and accelerating LLM inference tasks. \n3.4. Multi-Objective Design Space Exploration The primary goal of LLM quantization is to reduce memory usage and computational costs during inference. Quantization methods generally seek to balance two often competing factors: (1) quantization accuracy, which reflects the quality loss incurred by quantization relative to FP32 full precision, and (2) hardware efficiency, which combines computational efficiency (energy and silicon area for matrix multiplication) and memory efficiency (the number of bits needed to store or transfer a tensor). Popular quantization formats include INT4, INT8, and FP16, which form the basis of state-of-the-art quantization schemes such as W4A8, W8A8, and W4A16. This variety of data formats and configurations creates a complex design space, requiring careful trade-offs between accuracy, computational efficiency, and memory efficiency. Therefore, we first formalize the design space as a function of quantization accuracy and hardware cost. We then conduct an exhaustive exploration of design variables to identify Pareto-optimal design configurations. First, to assess quantization accuracy, we calculate the quantization signal-to-noise ratio (SNR), a metric shown to reliably predict the end-to-end language model loss induced by quantization\u00a0(darvish2023mx, ). This approach enables extensive design space exploration without the need for costly experiments on each LLM quantization configuration. Specifically, SNR is defined as the ratio of the power of the original (non-quantized) signal to the power of the quantization noise: where \ud835\udc31=[x1,x2,\u2026,xk]\u2208\u211dk\ud835\udc31subscript\ud835\udc651subscript\ud835\udc652\u2026subscript\ud835\udc65\ud835\udc58superscript\u211d\ud835\udc58\\mathbf{x}=[x_{1},x_{2},\\dots,x_{k}]\\in\\mathbb{R}^{k}bold_x = [ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ] \u2208 blackboard_R start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT and \u2016\ud835\udc31\u20162=\u2211i=1kxi2superscriptnorm\ud835\udc312superscriptsubscript\ud835\udc561\ud835\udc58superscriptsubscript\ud835\udc65\ud835\udc562\\|\\mathbf{x}\\|^{2}=\\sum_{i=1}^{k}x_{i}^{2}\u2225 bold_x \u2225 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. In quantization, the objective is to ensure that the quantized vector \ud835\udc10\ud835\udc31subscript\ud835\udc10\ud835\udc31\\mathbf{Q_{x}}bold_Q start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT closely approximates the original vector \ud835\udc31\ud835\udc31\\mathbf{x}bold_x, minimizing any discrepancies introduced by quantization. This goal corresponds to reducing the denominator term in Equation\u00a08, which represents the quantization noise, thereby preserving the numerical fidelity of the original data. Generally, a higher SNR reflects a quantized vector that more accurately retains the direction and magnitude of the original non-quantized vector, denoting a high-quality quantization process. As shown in Figure\u00a05, we focus on systolic array architectures, a class of accelerators widely adopted in low-power TPUs\u00a0(jouppi2017tpu, ). Systolic array-based accelerators are highly configurable, encompassing a broad design space with trade-offs across performance, power consumption, circuit area, and memory cost. The accelerator design illustrated in Figure\u00a05 features a 2D grid of parallel processing elements, arranged in a tiled structure. The count of PEs in each dimension (columns and rows) determines the aspect ratio of the chip, while the type of PEs is determined by the input data formats.\nSpecifically, for conventional GEMM with the same input data format, we employ standard INT8\u00d7\\times\u00d7INT8 PE and FP16\u00d7\\times\u00d7FP16 PEs, whereas for mixed-precision GEMM, we leverage the proposed MixPE to handle INT4\u00d7\\times\u00d7INT8 and INT4\u00d7\\times\u00d7FP16 mpGEMM operations. In the process of Field Programmable Gate Array (FPGA) synthesis, various parameters can affect the circuit area and power consumption. For instance, synthesis tools can map adders and other blocks into circuits that reduce critical path delay at the cost of increased area\u00a0(sklyarov2014synthesis, ). To obtain the core area for a given configuration, we synthesize each configuration with easily achievable timing constraints and with only inputs and outputs being registered. This ensures that synthesis implementation selection targets the minimum area in all designs. To consider both the area and energy cost, we utilize normalized area-power product as the hardware cost. In general, to evaluate quantization accuracy, we define a statistical methodology that computes quantization SNR ratio. To measure hardware efficiency, we employ a 2D systolic array that enables the computation of synthesized power and area for different quantization configurations. Leveraging these two parameterized models, we can explore different configurations in the design space and construct a Pareto frontier that captures the optimal balance of quantization accuracy and hardware efficiency. \n4. Experiments \n4.1. Experiment Setup Accelerator Implementation\nWe implement the MixPEs described in Section\u00a03.3 along with traditional GEMM PEs using Verilog RTL. For synthesis, we utilized Xilinx Vivado 2023.1\u00a0(Vivado, ) on a Xilinx Zynq UltraScale+ ZCU104 Evaluation Board, allowing us to assess resource utilization and perform static and dynamic power estimations on FPGA. For simplicity, we refer to the developed MixPE designs as \u201cMixPE-A8\u201d and \u201cMixPE-A16\u201d for W4A8 and W4A16, respectively. Additionally, we use \u201cINT8\u201d and \u201cFP16\u201d to denote the conventional INT8 and FP16 multiplication PEs. Baselines\nTo evaluate the end-to-end model performance of MixPEs, we compare MixPEs against four PE baselines, including conventional INT8 PE and FP16 PE, BitFusion\u00a0(sharma2018bit, ), and OLAccel\u00a0(park2018ola, ). Here we exclude ANT\u00a0(guo2022ant, ) and Olive\u00a0(guo2023olive, ) from comparison as both introduce additional decoders into the systolic array, incurring extra overhead. Specifically, BitFusion\u00a0(sharma2018bit, ) supports the mixed-precision of 4-bit and 8-bit integer types. OLAccel\u00a0(park2018ola, ) computes large values on dedicated, high-precision MAC units. We employ CACTI\u00a0(muralimanohar2009cacti, ) to estimate the area, latency, and power consumption of memory structures. Additionally, we develop a cycle-accurate simulator inspired by DnnWeaver\u00a0(sharma2016dnnweaver, ) and BitFusion\u00a0(sharma2018bit, ). We evaluate MixPE on four open-source representative LLM models, including ViT-base\u00a0(alexey2020vit, ), ViT-Huge\u00a0(alexey2020vit, ), OPT-6.7B\u00a0(zhang2022opt, ) and LLaMA-2-13B\u00a0(touvron2023llama, ). These models are selected to comprehensively assess MixPE\u2019s performance across different model architectures and scales. Design Space Exploration\nTo calculate the SNR for different quantization formats, we use the Pile dataset\u00a0(gao2020pile, ), sampling 128 random data points to obtain representative activation distributions. For the weight distribution, we adopt a standard normal distribution X\u223c\ud835\udca9\u2062(0,1)similar-to\ud835\udc4b\ud835\udca901X\\sim\\mathcal{N}(0,1)italic_X \u223c caligraphic_N ( 0 , 1 ), which is commonly observed in LLM workloads\u00a0(darvish2023mx, ). While some layers may exhibit heavy-tailed weight distributions due to non-linear transformations (e.g., SwiGLU), addressing these variations is outside the scope of this paper. \n4.2. Accelerator Hardware Evaluation As explained in Section\u00a03.3, we integrate MixPE to the systolic array-based hardware accelerator and compare its performance and energy against INT8 PE and FP16 PE adopted in commercial GPUs, such as Nvidia Tensor Core\u00a0(choquette2021tensorcore, ) and Google TPU\u00a0(jouppi2017tpu, ). For a fair comparison, we set the frequency as 250MHz for all the hardware designs.\nThe number of columns and number of rows are both 4444, leading to a 4\u00d74444\\times 44 \u00d7 4 systolic array structure. In FPGA design, area typically refers to the amount of utilized hardware resources, including LUTs, FFs, DSPs, and IOs. The normalized area utilization across various accelerator components is illustrated in Figure\u00a06.\nNotably, the MixPE-A8 achieves a 54% area reduction compared to the standard INT8 PE and the MixPE-A16 reduces the area to 77% of the FP16 counterpart, underscoring its resource efficiency. In terms of power consumption, circuit power can be divided into two primary components: dynamic and static power. The power comparison in Figure\u00a06 reveals that the MixPE design demonstrates marked improvements in power efficiency, achieving a 64% reduction in total power consumption compared to the FP16 PE and a 21% reduction relative to the INT8 PE. Specifically, this power reduction mainly comes from the reduction in dynamic power, denoting that MixPE effectively minimizes switching activity by leveraging efficient shift&add operations instead of traditional multipliers. In summary, MixPE significantly outperforms traditional multiplier-based PEs in both area and power efficiency, making it an ideal choice for mixed-precision GEMM applications on resource-constrained LLM deployments. \n4.3. Accelerator Design Space Exploration The synthesized accelerator performance, including power and area, can be affected by many aspects, such as aspect ratio, global buffer, and high-performance DSP usage. Therefore, we conducted design space exploration to gain insight in the impact of the different parameters on the performance of hardware accelerators.\nFigure\u00a07 shows the trade-off between numerical fidelity and the hardware cost for different configurations, revealing that MixPE establishes a new Pareto frontier compared to traditional systolic array designs. In particular, for the same hardware cost, the MixPE design consistently demonstrates superior numerical fidelity, as indicated by its higher signal-to-noise ratio. This advantage stems from MixPE\u2019s highly efficient shift&add module to minimize resource usage and energy consumption, making it a compelling choice for efficient LLM deployment in real-world scenarios. \n4.4. Model Performance and Energy We evaluate the performance of LLMs on different accelerators, with a batch size of 8. In our experiments, we compare MixPE against four baseline designs: BitFusion\u00a0(sharma2018bit, ), OLAccel\u00a0(park2018ola, ), and traditional INT8 and FP16 PE. BitFusion\u00a0(sharma2018bit, ) utilizes a mixed-precision scheme with 4-bit and 8-bit INT types, and we extend OLAccel\u00a0(park2018ola, ) to support Transformer-based models with 4-bit weight and 8-bit activation quantization. Note that according to the original paper, both BitFusion and OLAccel require 8-bit GEMM for certain layers. For clarity, all accelerators use the same per-group quantization scheme with a group size of 128: W4A8 for MixPE-A8, OLAccel, BitFusion, and INT8 PEs, and W4A16 for MixPE-A16 and FP16 PEs. Thus, the major difference lies in mpGEMM strategy, so the performance comparison focuses solely on accelerator performance, with metrics based on model throughput and energy efficiency. The normalized speedup and energy results are presented in Figure\u00a08. The top plot of Figure\u00a08 demonstrates that MixPE-A8 has the most significant advantage in latency speedup. Specifically, MixPE-A8 delivers speedups of 2.62\u00d7\\times\u00d7, 3.58\u00d7\\times\u00d7, and 4.42\u00d7\\times\u00d7 over OLAccel, BitFusion and INT8, respectively. MixPE-A16, on the other hand, achieves a 2.44\u00d7\\times\u00d7 speedup over its FP16 counterpart and even outperforms the other W4A8 baselines, i.e., BitFusion, OLAccel, and INT8, due to the substantial cycle reduction enabled by the dequantization-after-GEMM strategy. Meanwhile, the speedup values for MixPE are consistent across different models, owing to the similarity in the decoder-only architectures of the language models, with differences mainly in the number of layers and hidden state parameters. The bottom plot of Figure\u00a08 compares the normalized energy consumption of different designs, which includes the static energy and dynamic power (DRAM, on-chip buffer, and core). MixPE-A8 exhibits the lowest energy consumption, achieving an average reduction of 2.21\u00d7\\times\u00d7 compared to the other designs. Specifically, MixPE-A8 achieves a power reduction of 1.39\u00d7\\times\u00d7, 2.44\u00d7\\times\u00d7, and 2.78\u00d7\\times\u00d7 over OLAccel, BitFusion, and INT8, respectively. Additionally, MixPE-A16 decreases the energy consumption of FP16 by 68% on average. Notably, the energy reduction becomes more pronounced as the model size increases. For instance, MixPE-A16\u2019s energy consumption is even lower than that of lower-bit BitFusion and INT8 on the three larger language models, highlighting its scalability. This enhanced energy efficiency is attributed to the optimized shift&add modules in MixPE-A16, which leverage low-bit precision data to maximize computational efficiency. Overall, by co-optimizing both the quantization algorithm\u2014specifically the dequantization-after-GEMM strategy\u2014and the hardware accelerator, i.e., the shift&add PE design, MixPE fully exploits the benefits of low-bit precision quantization, leading to substantial reductions in both energy and computational overhead. \n4.5. Ablation Study of Dequantization Overhead We measure the dequantization overhead of the mixed-precision quantization algorithm on both MixPE and INT8 PE. Our analysis focuses on the W4A8 quantization scheme applied to OPT-6.7B, with batch sizes ranging from 2 to 32. The dequantization overhead for both MixPE and INT8 PE is presented in Figure\u00a09. The results indicate that MixPE exhibits significantly lower dequantization overhead compared to the INT8 PE. This reduction becomes more pronounced at lower batch sizes due to the relatively smaller computational load. In practical LLM deployment settings, where batch sizes are often small but the sequence length is long, MixPE emerges as the more efficient choice, offering substantial improvements in both energy and computational efficiency during real-world inference tasks. \n5. Conclusion In this work, we present MixPE, a novel hardware accelerator designed to efficiently handle mixed-precision GEMM with minimal dequantization overhead. The key insight lies in exploiting per-group parameter sharing to enable dequantization after mpGEMM, thereby reducing overhead in the main loop. To fully capitalize on the benefits of low-precision weights, we propose a dedicated shift&add-based processing element to achieve mpGEMM without energy-intensive multipliers. MixPE pushes the efficiency of LLM quantization to a new level by co-designing the quantization scheme and hardware accelerator. Moreover, we conduct an exhaustive exploration of design variables, demonstrating that MixPE establishes a Pareto frontier that optimally balances numerical fidelity and hardware efficiency. Finally, MixPE outperforms existing accelerators, achieving a 2.6\u00d72.6\\times2.6 \u00d7 speedup and 1.4\u00d71.4\\times1.4 \u00d7 reduction in energy consumption. References"}
{"text": "S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o \u2013 An LLM for Netlist-to-Schematic Conversion Machine learning models are advancing circuit design, particularly in analog circuits. They typically generate netlists that lack human interpretability. This is a problem as human designers heavily rely on the interpretability of circuit diagrams or schematics to intuitively understand, troubleshoot, and develop designs. Hence, to integrate domain knowledge effectively, it is crucial to translate ML-generated netlists into interpretable schematics quickly and accurately. We propose Schemato, a large language model (LLM) for netlist-to-schematic conversion. In particular, we consider our approach in the two settings of converting netlists to .asc files for LTSpice and LaTeX files for CircuiTikz schematics. Experiments on our circuit dataset show that S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o achieves up to 93% compilation success rate for the netlist-to-LaTeX conversion task, surpassing the 26% rate scored by the state-of-the-art LLMs. Furthermore, our experiments show that Schemato generates schematics with a mean structural similarity index measure that is 3\u00d7\\times\u00d7 higher than the best performing LLMs, therefore closer to the reference human design. \nI Introduction\n Recent years have seen a rapid increase in the number of machine learning (ML)-based solutions that address different aspects of analog design, specially towards both topology and sizing [1, 2, 3, 4, 5]. The netlist representation of the analog circuit forms a central part of all these approaches, since they all operate by either encoding the information in the netlist or generating a netlist as a part of an end-to-end ML model. However, often such circuits, while they may maximize a certain objective function and, hence, are feasible in the eyes of an ML practician, might not result in a practical design in the eyes of an analog designer. An analog designer usually views and assesses a circuit through the circuit diagram or schematics and not a netlist. This means that having a schematic or circuit diagram corresponding to a netlist is crucial for the ML models engineer to benefit from the domain knowledge of the designer\u2019s feedback. This is specially the case when the circuits become large in size and when one considers problems such as synthesizing circuits [6, 7], where the designer must spend considerable resource in drawing a schematic by hand each time to be able to give feedback the ML models more effectively. Thus, it is highly desirable to be able to efficiently and automatically translate netlists into circuit diagrams or schematics. Circuit schematics are manually generated using conventional electronic design automation (EDA) tools by analog designers and visualized with associated viewing tools, such as LTSpice which processes text-based structured schematic files, .asc to produce circuit diagrams. An alternative approach is to use the TikZ library in LaTeX, where circuits are represented by code[8]. Both methods rely on structured programs with specific semantics or \u201clanguages\u201d.\nSimilarly, netlists adhere to well-defined semantics for circuit representation, making the task of conversion to a schematic or diagram analogous to a language translation task. This analogy highlights the potential of leveraging large language models (LLMs), which excel at understanding and predicting semantic structures [9, 10]. LLMs have also recently shown great potential in different aspects of EDA, particularly, in digital and logic synthesis [11, 12, 13, 14, 4, 15] wherein the models learn to understand, abstract, and build upon the design process of human designers. This motivates us to propose an LLM-based solution for translating netlists to circuit schematics/diagrams. The earliest works on automated netlist to schematic generation have mostly been in the domain of digital ICs [16, 17]. Some of the early works for analog circuit include [18] that introduced a symmetry-based placement algorithm that identifies symmetrical structures within a netlist, and the work of [19] that concentrated on schematic routing aimed at minimizing net crossings.\nThe more recent work of [20] presents a ML-based approach that combines identification of subblocks with an RL block that generates the schematic by optimizing for a reward based on the building block compliance rate that measures aesthetic of the generated schematic based on topological and heuristic constraints. While these approaches have been shown to perform well on different circuit topologies, they rely on either the explicit use of heuristic rules or an understanding of the underlying topology/hierarchy of its subblocks or subcircuits or on the use of specific aesthetic reward functions. S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o overcomes these limitations by developing an automated netlist to schematic conversion system that is independent of the circuit type or functionality, enabling broad applicability across diverse designs without requiring prior knowledge of specific circuit characteristics, while maintaining as faithfully as possible the aesthetics of a human design. This is because our approach learns from human-designed circuit examples and therefore does not require an explicit aesthetics measure. We also note that due to the universal nature of the framework, we are not limited to specific schematic templates or the choice of the SPICE solver; although we have used LTSpice with .asc format in our experiments, our approach is equally valid with the use of other EDA tools with their schematic formats. In particular, our contributions are: We\npropose Schemato, an LLM for automatic translation of netlists to text-based schematic representations, fine-tuned on human-created design examples. We consider two types of netlist to schematic conversion, namely, netlist-to-asc files for use with LTSpice, and netlist-to-LaTeX for use with the CircuiTikz library for schematic generation, respectively. We perform exhaustive experiments on publicly available circuit and evaluate the performance of our approach using various metrics such as bilingual evaluation understudy (BLEU), compilation success rate (CSR), and mean structural similarity index measure (MSSIM). Experiments show that Schemato outperforms state-of-the-art LLMs with a CSR of 93.75% in the netlist-to-LaTeX conversion task and a 3\u00d7\\times\u00d7 higher average MSSIM over the test set. To the best of our knowledge, no prior work exists that addresses this specific and important problem using LLMs, and given the increasing influence of LLMs, we believe our contribution is both timely and valuable for advancing ML-based approaches for analog design. \nII Preliminaries\n In this section, we present a short review of LLMs followed by a short review of the LTSpice .asc and LaTeX .tex schematic script that forms the basis of our current work. \nII-A Review of LLMs\n Large Language Models (LLMs) are sophisticated AI systems designed to process and generate human-like text by leveraging vast datasets and advanced neural architectures [9, 10]. They excel in a range of linguistic and cognitive tasks, including translation, summarization, and complex reasoning. A key factor for effectiveness is prompt engineering \u2014 the crafting of precise and contextualized instructions to maximize accuracy and relevance of LLM responses. By utilizing advanced techniques such as Chain-of-Thought (CoT) prompting for multi-step reasoning and few-shot prompting for task-specific adaptation, LLMs can be made to handle nuanced tasks with remarkable efficiency [21]. Despite their versatility, state-of-the-art LLMs are typically trained on broad, general-purpose datasets. As a result, their performance in specialized domains, such as EDA, may fall short due to the lack of task-specific optimization [11, 12, 13, 15]. Addressing these limitations often requires fine-tuning or targeted prompt strategies to align LLM capabilities with the unique demands of EDA applications. In our work, we consider different combinations of prompt design, few-shot examples, and fine-tuning of LLMs to translate netlists into circuit diagrams/schematics. Our experiments demonstrate that while both prompt engineering and the incorporation of few-shot examples are crucial for improving the output of the original LLM, fine-tuning yields a substantially enhanced LLM, called S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o. \nII-B Review of LTSpice-specific .asc format\n We next shortly review the key terms in the LTSpice .asc file111More details may be found in the documentation of LTSpice [22]. As we discuss later, this will also motivate some of the prompts used in our experiments. In the .asc file, each line starts with one of the following keywords accompanied by the corresponding input arguments \u201cVersion index\u201d defines the LTSpice version. \u201cSHEET index height width\u201d: defines the sheet index and size on the LTSpice application. \u201cWIRE xs\u2062t\u2062a\u2062r\u2062tsubscript\ud835\udc65\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61x_{start}italic_x start_POSTSUBSCRIPT italic_s italic_t italic_a italic_r italic_t end_POSTSUBSCRIPT ys\u2062t\u2062a\u2062r\u2062tsubscript\ud835\udc66\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61y_{start}italic_y start_POSTSUBSCRIPT italic_s italic_t italic_a italic_r italic_t end_POSTSUBSCRIPT xe\u2062n\u2062dsubscript\ud835\udc65\ud835\udc52\ud835\udc5b\ud835\udc51x_{end}italic_x start_POSTSUBSCRIPT italic_e italic_n italic_d end_POSTSUBSCRIPT ye\u2062n\u2062dsubscript\ud835\udc66\ud835\udc52\ud835\udc5b\ud835\udc51y_{end}italic_y start_POSTSUBSCRIPT italic_e italic_n italic_d end_POSTSUBSCRIPT\u201d: creates a wire that extends from position (xs\u2062t\u2062a\u2062r\u2062tsubscript\ud835\udc65\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61x_{start}italic_x start_POSTSUBSCRIPT italic_s italic_t italic_a italic_r italic_t end_POSTSUBSCRIPT, ys\u2062t\u2062a\u2062r\u2062tsubscript\ud835\udc66\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61y_{start}italic_y start_POSTSUBSCRIPT italic_s italic_t italic_a italic_r italic_t end_POSTSUBSCRIPT) to (xe\u2062n\u2062dsubscript\ud835\udc65\ud835\udc52\ud835\udc5b\ud835\udc51x_{end}italic_x start_POSTSUBSCRIPT italic_e italic_n italic_d end_POSTSUBSCRIPT, ye\u2062n\u2062dsubscript\ud835\udc66\ud835\udc52\ud835\udc5b\ud835\udc51y_{end}italic_y start_POSTSUBSCRIPT italic_e italic_n italic_d end_POSTSUBSCRIPT). \u201cSYMBOL component xc\u2062o\u2062m\u2062psubscript\ud835\udc65\ud835\udc50\ud835\udc5c\ud835\udc5a\ud835\udc5dx_{comp}italic_x start_POSTSUBSCRIPT italic_c italic_o italic_m italic_p end_POSTSUBSCRIPT yc\u2062o\u2062m\u2062psubscript\ud835\udc66\ud835\udc50\ud835\udc5c\ud835\udc5a\ud835\udc5dy_{comp}italic_y start_POSTSUBSCRIPT italic_c italic_o italic_m italic_p end_POSTSUBSCRIPT \u03b8\ud835\udf03\\thetaitalic_\u03b8\u201d: instantiates a circuit component (e.g. nmos, pmos, resistor, opamp, inverter) at the specified position (xc\u2062o\u2062m\u2062psubscript\ud835\udc65\ud835\udc50\ud835\udc5c\ud835\udc5a\ud835\udc5dx_{comp}italic_x start_POSTSUBSCRIPT italic_c italic_o italic_m italic_p end_POSTSUBSCRIPT, yc\u2062o\u2062m\u2062psubscript\ud835\udc66\ud835\udc50\ud835\udc5c\ud835\udc5a\ud835\udc5dy_{comp}italic_y start_POSTSUBSCRIPT italic_c italic_o italic_m italic_p end_POSTSUBSCRIPT) and orientation \u03b8\ud835\udf03\\thetaitalic_\u03b8, where \u03b8=R\u206290\ud835\udf03\ud835\udc4590\\theta=R90italic_\u03b8 = italic_R 90 corresponds to 90\u2218superscript9090^{\\circ}90 start_POSTSUPERSCRIPT \u2218 end_POSTSUPERSCRIPT anti-clockwise rotation and \u03b8=M\u2062270\ud835\udf03\ud835\udc40270\\theta=M270italic_\u03b8 = italic_M 270 for 270\u2218superscript270270^{\\circ}270 start_POSTSUPERSCRIPT \u2218 end_POSTSUPERSCRIPT anti-clockwise rotation combined with mirroring. This line is followed by one of these lines that sets a symbol attribute: \u201cSYMATTR InstName component_name\u201d: assigns the component\u2019s name. \u201cSYMATTR SpiceModel model_name\u201d: assigns the component\u2019s spice model. \u201cSYMATTR ModelFile model_file\u201d: assigns the component\u2019s spice model path. \nII-C Review of LaTeX format\n We next shortly review the key terms in the LaTeX .tex file, tailored to the use of CircuitTikz library. As with the .asc files, we shall see how this information is valuable in improving the generated schematics through prompts shown in Fig.\u00a02. The following are the keywords that characterize a .tex file. \u201c\\documentclass{}\u201d: Defines the type of document (e.g., article, report, book) and sets the document\u2019s overall format. \u201c\\usepackage{}\u201d: Imports additional packages to extend LaTeX\u2019s functionality, such as graphics, fonts, or TikZ for diagrams. \u201c\\usetikzlibrary{}\u201d: Includes specific TikZ libraries to enable advanced features like shapes, positioning, and circuit diagrams. \u201c\\begin{}\u201d and \u201c\\end{}\u201d: Define environments, such as \u201cdocument\u201d for the main content or \u201ctikzpicture\u201d for TikZ diagrams. \u201c\\draw{}\u201d: Specifies drawing commands within a \u201ctikzpicture\u201d environment to create shapes, lines, and other visual elements. \u201c\\node{}\u201d: Places text or shapes at specific locations in a diagram, often used within TikZ. \u201c\\section{}\u201d and \u201c\\subsection{}\u201d: Create structured headings for organizing content hierarchically. \nIII Proposed approach: S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o\n In Fig.\u00a01, we present an illustration of the S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o-based circuit design workflow. The left half illustrates the fully data-driven model, while the right half integrates domain knowledge contributions from analog designers. As discussed in Sec.\u00a0I, S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o addresses the task of automating circuit schematic generation from netlists. At its core, S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o is an LLM fine-tuned using a curated dataset of human-created netlist-to-schematic pairs. Its development focuses on three key components: Task Definition: Constructing prompts that define the netlist-to-schematic conversion task. Guidance via Context: Incorporating few-shot or in-context examples within prompts to guide the model. Fine-Tuning: Adapting the model using a specialized dataset of netlist-schematic pairs. S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o supports two distinct formats for schematic generation: LTSpice (.asc): Scripts compatible with LTSpice schematic generation tools. While our experiments focus on LTSpice due to its data availability, the approach generalizes to any text-based solver or schematic script format. CircuitTikz (LaTeX): Code to generate schematics using the CircuitTikz library for LaTeX documents [8]. \nIII-A Task definition and guidance via context\n We consider the following tasks to evaluate the different models: Task 1: Translate netlist to LTSpice text-based schematic Task 2: Translate netlist to LaTeX code using CircuiTikz Furthermore, to develop and refine S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o, we consider different prompting techniques to guide the LLM effectively. The summary of these prompt variants are provided in Fig.\u00a02.\nAs shown in the figure, prompt 1 is a simple, task-oriented description that serves as the baseline for comparison. Prompt 2 introduces syntax-specific keywords relevant to the target format, such as LTSpice symbols (Sec.\u00a0II-B\n) or LaTeX libraries and structure (Sec.\u00a0II-C).\nPrompt 3 enhances guidance by including initial lines of the expected output, providing a stronger contextual foundation.\nPrompts 4 incorporate specific netlist-to-schematic conversion examples, offering the model concrete patterns for improved accuracy. Finally, Prompt 5 is merely a combination of prompt 3 and 4. Prompts 1\u20133 utilize zero-shot learning, while Prompts 4\u20135 employ one-shot learning. \nIII-B Fine-tuning\n S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o is developed by fine-tuning state-of-the-art LLMs using human-generated netlist-schematic data pairs. To address the limited availability of such data, we employ data augmentation techniques to expand the dataset, ensuring better coverage of diverse scenarios. To select the optimal base model, we first evaluate various LLMs using all proposed prompts. The combination of prompt and model yielding the highest performance is then fine-tuned resulting in S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o.\nThe comprehensive details on fine-tuning methodology, data preprocessing, and augmentation strategies are provided in Section IV. \nIV Experimental setup\n In our experiments, we use LTSpice schematics from GitHub. These schematics contain circuits with diverse features. We use this set for training and validation. In addition, we also consider the publicly available LTSpice circuit files packaged with LTSpice XVII (examples in folders, Eduational & Applications [22]) for testing.\nIn all the cases, we start from the available .asc files. These are then processed through LTSpice to create the corresponding netlists. Similarly, the .asc files are used to generate the LaTeX scripts using the open-source python library lt2circuiTikz222https://github.com/ckuhlmann/lt2circuitikz. Thus, for each .asc file, we create both the netlist and .tex files. Some .asc files cannot be compiled into netlist or converted into .tex files due to missing symbols in the default libraries of LTSpice and lt2circuiTikz. These .asc files are omitted from our dataset. We next describe the details of data preprocessing and filtering. \nIV-A Preprocessing and filtering datasets\n To facilitate Schemato to learn the translation task, we preprocess the dataset by removing unnecessary information for the schematic visualizations in LTSpice. This includes spice simulation commands, spice model definitions, user annotations, comments, etc. In contrary, we do not preprocess the LaTeX files as they do not contain unnecessary information for the schematic visualizations. We explain how we preprocess LTSpice compatible files below. LTSpice-specific .asc files: Replace SpiceModel and ModelFile with InstName while using the original model_name and model_file for component_name. This allows for any line with SYMATTR to be compiled and component_name can be set arbitrarily. Remove the lines starting with *, TEXT, RECTANGLE, WINDOW, LINE, and CIRCLE. Remove the user annotations/comments since they are not needed for the schematic visualization by LTSpice. Discard .asc files with symbols that are not defined in lt2circuitikz as they cannot be converted into LaTeX. Discard .asc files that do not contain SYMBOL-SYMATTR pair as this type of example is not composed of meaningful circuit components except for wires. Netlists:\nThe file format we use for netlist is .net.\nWe remove the lines with commands such as  .backanno,  .lib, and .model as they are not required for the schematic visualizations. We next consider the augmentation strategy used to increase the effective dataset size. \nIV-B Data augmentation\n For S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o to be agnostic to the line ordering of .asc files, we create an augmented data example by swapping the lines of the .asc files. Specifically, .asc files can have different orders of SYMBOL-SYMATTR pairs resulting in the same schematic representations. Therefore, we randomly shuffle their line ordering and generate five different augmentations for .asc files with more than three SYMBOL-SYMATTR pairs. For .asc files with two SYMBOL-SYMATTR pairs, we swap their order to generate two samples and we leave the files with one SYMBOL-SYMATTR pair unmodified.\nAfter the generation of these augmented .asc file samples, we convert them into netlists and LaTeX scripts to create the dataset as described in Sec.\u00a0IV-A. The changes of the line ordering in .asc files are reflected in the line ordering of both netlists and LaTeX scripts without any modifications in the schematic representation of the circuits. Initially, the self-prepared dataset contains 22,045 publicly available .asc circuit files. After filtering, especially discarding files that cannot be converted by lt2circuitikz, it is reduced to 2,886 different circuit files. This filtered dataset is split into 2,712 training and 174 validation circuit files, which after augmenting each circuit file into up to 5 samples, result in total of 12,974 and 857 samples for training and validation, respectively. For testing, we use the 3,946 circuit examples from LTSpice XVII [22]. After the data filtering, we end up with 133 schematics. Furthermore, removing samples that are part of our training set, we yield a test set of 49 circuits. All of these circuit files are converted into netlists and LaTeX files. \nIV-C Inference and fine-tuning\n We start with Llama 3.1 base & instruct versions with 8 billion parameters (Llama-3.1-8B) [10] to develop S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o. The tasks are first executed on the raw models to select the best performing prompts that are then used for fine-tuning. We use the recently proposed Torchtune\u00a0[23] framework for LLM fine-tuning. We first discuss the details of inference and fine-tuning of the models. During inference, S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o generates schematics in an\nauto-regressive way. Here, we used no special techniques to save\nmemory or compute except for KV-caches. We apply greedy search, where the LLM selects the most probable next samples. Decoding terminates either if maximum number of\ntokens (Ntokenssubscript\ud835\udc41tokensN_{\\text{tokens}}italic_N start_POSTSUBSCRIPT tokens end_POSTSUBSCRIPT) are reached or the stop-token is sampled. We choose Ntokens=8192subscript\ud835\udc41tokens8192N_{\\text{tokens}}=8192italic_N start_POSTSUBSCRIPT tokens end_POSTSUBSCRIPT = 8192 as this is the maximum number of tokens that can fit in a single\nNVIDIA Ada-6000 GPU. This limits the length of our test samples\nsince some of the LaTeX codes exceed 8192 tokens. For simplicity,\nwe omit these long samples for the performance evaluation.\nWith this setup, the inference on S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o achieved approximately 23232323 tokens/sec on a single NVIDIA Ada-6000. To adapt the LLM to our specific task of translating netlists into schematics, we employed supervised instruction tuning on a paired dataset\ncontaining prompts with netlists and corresponding reference schematics as responses.\nHowever, fine-tuning LLMs like S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o with 8 billion parameters requires a significant amount of GPU memory. To address this issue, we utilized\nfully-sharded data parallel (FSDP) training [24], which enables the distribution of the model\u2019s parameters across multi-GPUs while minimizing communication overhead.\nTo further reduce memory consumption during training, we applied several additional techniques: We used bfloat16 representation for model parameters to decrease the memory usage compared to float32. Activation checkpointing was employed to compute gradients in a more memory-efficient manner. Low-rank adaptation (LoRA) with rank r=8\ud835\udc5f8r=8italic_r = 8 and scaling factor \u03b1=16\ud835\udefc16\\alpha=16italic_\u03b1 = 16 was applied to reduce the memory requirements of the trainable weights. Cross entropy with chunked output loss [23] is used to measure the difference between the generated token and the ground truth. By combining these techniques, we fine-tune S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o over 9 epochs (in each epoch, S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o goes through the entire training set). The number of weight updates per epoch is then computed as: where Ntrainsubscript\ud835\udc41trainN_{\\text{train}}italic_N start_POSTSUBSCRIPT train end_POSTSUBSCRIPT is the number of training samples, Ntaskssubscript\ud835\udc41tasksN_{\\text{tasks}}italic_N start_POSTSUBSCRIPT tasks end_POSTSUBSCRIPT is the number of conversion tasks, Nitersubscript\ud835\udc41iterN_{\\text{iter}}italic_N start_POSTSUBSCRIPT iter end_POSTSUBSCRIPT is the number of iterations per accumulation step, NGPUsubscript\ud835\udc41GPUN_{\\text{GPU}}italic_N start_POSTSUBSCRIPT GPU end_POSTSUBSCRIPT is the number of NVIDIA Ada-6000 GPUs used for fine-tuning, and Nbatchsubscript\ud835\udc41batchN_{\\text{batch}}italic_N start_POSTSUBSCRIPT batch end_POSTSUBSCRIPT is the per-GPU batch size.\nThis setup achieved a throughput of approximately 563563563563 tokens/sec per GPU, allowing us to efficiently adapt the model to our specific task within a reasonable timeframe. We next discuss the different performance metrics used to evaluate the performance of the models. \nIV-D Performance metrics\n We use the following three metrics to evaluate the samples generated by the different models: (a) BLEU that measures an N-gram similarity, (b) CSR that measures syntatic correctness, and (c) MSSIM that measures similarity with human generated design. We describe these metrics next. The bilingual evaluation understudy (BLEU) compares N-grams of the generated text and the reference translation and counts the number of matches independently of the positions [25]. With this measure, we compare the N-gram similarity between the reference and generated text-based schematics. While BLEU is not perfectly suited for our tasks due to the syntactical nature of .asc and LaTeX files, it can indicate the extent to which the pre-trained models already understand their language syntax. Furthermore, we selected the standard BLEU as our metric over CodeBLEU [26] specialized to capture syntactic and semantic features of codes since this measure does not support .asc and LaTeX formats.\nWe compare the performance of the models by computing BLEU\u00af\u00afBLEU\\overline{\\text{BLEU}}over\u00af start_ARG BLEU end_ARG, that is the average BLEU score across all test samples. The compilation success rate (CSR) measures the syntactical correctness of the generated .asc and LaTeX scripts and is defined as where Ncompilablesubscript\ud835\udc41compilableN_{\\text{compilable}}italic_N start_POSTSUBSCRIPT compilable end_POSTSUBSCRIPT is the total number of compilable generated codes in the test set and Nrefsubscript\ud835\udc41refN_{\\text{ref}}italic_N start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT is the total number of reference samples. The structural similarity index measure (SSIM) is a metric that computes the structural similarity of two image signals based on the statistical comparisons in luminance, contrast, and structure [27].\nFor a given pair of image patches x\ud835\udc65xitalic_x and y\ud835\udc66yitalic_y, SSIM is defined as where \u03bcpsubscript\ud835\udf07\ud835\udc5d\\mu_{p}italic_\u03bc start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT and \u03c3psubscript\ud835\udf0e\ud835\udc5d\\sigma_{p}italic_\u03c3 start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT are the average and the standard deviation of an image patch p\ud835\udc5dpitalic_p, respectively,\n\u03c3x\u2062ysubscript\ud835\udf0e\ud835\udc65\ud835\udc66\\sigma_{xy}italic_\u03c3 start_POSTSUBSCRIPT italic_x italic_y end_POSTSUBSCRIPT is the square-root of the cross-correlation between the two image patches x\ud835\udc65xitalic_x and y\ud835\udc66yitalic_y,\nC1subscript\ud835\udc361C_{1}italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and C2subscript\ud835\udc362C_{2}italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT are constants added to avoid computational instability.\nTo evaluate the overall similarity between an image pair X\ud835\udc4bXitalic_X and Y\ud835\udc4cYitalic_Y, a sliding local window is applied over the entire images and SSIM is computed at each pixel step within this window. The mean SSIM (MSSIM) between two images X\ud835\udc4bXitalic_X and Y\ud835\udc4cYitalic_Y is then given by where xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and yisubscript\ud835\udc66\ud835\udc56y_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are the image signals in the i\ud835\udc56iitalic_i-th local window and M\ud835\udc40Mitalic_M is the number of local windows applied on the images.\nWe use this metric to derive the similarities between the reference schematics in the test set and the schematics generated by LLMs from netlists. To obtain the images of generated and reference schematics, we load each .asc file with LTSpice and take a screenshot. These images are subsequently cropped to eliminate the artifacts from the graphical user interface. For the case of LaTeX codes, the .tex files are converted into PDF files using pdflatex. These are then converted into images with pdf2image333https://github.com/Belval/pdf2image. For cases where the generated codes do not successfully compile into an image, we set the MSSIM to be equal to 0. We then compare the performance of the models using MSSIM\u00af\u00afMSSIM\\overline{\\mbox{MSSIM}}over\u00af start_ARG MSSIM end_ARG, that is the MSSIM score averaged over all test samples. \nV Results & Discussion\n In order to select the starting model for S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o, we evaluated the performance of Llama-3.1-8B raw models across various configurations. Table\u00a0I shows the performance of the different models for the netlist-to-.asc conversion task. We note that the MSSIM\u00af\u00afMSSIM\\overline{\\text{MSSIM}}over\u00af start_ARG MSSIM end_ARG values are normalized by the maximum MSSIM\u00af\u00afMSSIM\\overline{\\text{MSSIM}}over\u00af start_ARG MSSIM end_ARG value of 0.0228 achieved by S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o fine-tuned on prompt 4.\nAs can be seen, the performance did not vary significantly across different settings. Notably, moving from zero-shot to one-shot prompting had negligible impact on the performance for this task. In contrast, Table\u00a0II highlights a key observation for the netlist-to-LaTeX conversion task. Again, we note that the MSSIM\u00af\u00afMSSIM\\overline{\\text{MSSIM}}over\u00af start_ARG MSSIM end_ARG values are normalized by the maximum MSSIM\u00af\u00afMSSIM\\overline{\\text{MSSIM}}over\u00af start_ARG MSSIM end_ARG value of 0.0217 achieved by S\u2062c\u2062h\u2062e\u2062m\u2062a\u2062t\u2062o\ud835\udc46\ud835\udc50\u210e\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5cSchematoitalic_S italic_c italic_h italic_e italic_m italic_a italic_t italic_o fine-tuned on prompt 4 for the corresponding task. Here, Llama-3.1-8B instruct with prompt 2 achieved a CSR of 25% for LaTeX generation, while all other raw configurations failed to produce any compilable outputs. Based on these observations, we selected Llama-3.1-8B instruct with prompt 2 as the starting configuration. Additionally, we fine-tune Llama-3.1-8B instruct with prompt 4 to investigate the effect of one-shot prompting. During fine-tuning, training and validation losses (Fig.\u00a03) revealed overfitting for the netlist-to-.asc conversion after epoch 2 (since the validation loss increased with training epochs), while netlist-to-LaTeX validation loss stabilized after several epochs.\nSince the cross entropy loss does not directly help quantify the correctness of the code and the generated schematics, we use CSR, and MSSIM\u00af\u00afMSSIM\\overline{\\text{MSSIM}}over\u00af start_ARG MSSIM end_ARG metrics discussed in Section IV-D to evaluate the model performance during training epochs as shown in Fig.\u00a04. We then choose the model weights that gave the best pair of CSR and MSSIM\u00af\u00afMSSIM\\overline{\\text{MSSIM}}over\u00af start_ARG MSSIM end_ARG scores for further evaluation."}
{"text": "Scalable Wavelength Arbitration for Microring-based DWDM Transceivers This paper introduces the concept of autonomous microring arbitration, or wavelength arbitration, to address the challenge of multi-microring initialization in microring-based Dense-Wavelength-Division-Multiplexed (DWDM) transceivers.\nThis arbitration is inherently policy-driven, defining critical system characteristics such as the spectral ordering of microrings.\nFurthermore, to facilitate large-scale deployment, the arbitration algorithms must operate independently of specific wavelength information and be resilient to system variability.\nAddressing these complexities requires a holistic approach that encompasses the entire system, from device-level variabilities to the transceiver interface\u2014this system-wide perspective is the focus of this paper.\nTo support efficient analysis, we develop a hierarchical framework incorporating an ideal, wavelength-aware arbitration model to examine arbitration failures at both the policy and algorithmic levels.\nThe effectiveness of this approach is demonstrated in two ways: by analyzing the robustness of each policy in relation to device variabilities, and by developing an algorithm that achieves near-perfect alignment with the ideal model, offering superior robustness compared to the traditional sequential tuning method.\nThe simulator code used in this paper is available at https://github.com/wdmsim/wdm-simulator. \nI Introduction\n With the overarching demand for bandwidth and the maturation of silicon photonics technology, the next phase for microring-based DWDM transceiver technology targets large-scale deployment, leveraging its high-bandwidth and low-latency characteristics [1, 2, 3, 4, 5, 6].\nHowever, scaling up poses significant challenges in system design, particularly with regard to mass deployment scenarios.\nA cost-efficient, robust solution is essential, ideally equipped with a comprehensive analytical toolkit for large-scale deployment [7, 8, 9].\nFor microring-based DWDM transceivers, one major challenge is aligning microring resonances with laser wavelengths\u2014a problem of initialization [10]\u2014which requires judicious arbitration of the microrings to be cost-efficient, robust and autonomous. Microring resonance control, a common technique for addressing microring initialization [11, 12, 13, 14, 15, 16], involves sweeping the microring resonance across the tuning range and identifying the resonance alignment.\nFor a single microring with a laser tone, the resonance alignment is identified by the peak in intra-cavity optical power (wavelength search) [11, 12, 13], followed by a feedback loop that stabilize the resonance wavelength (wavelength lock) [14, 15, 16].\nHowever, in DWDM systems, the wavelength search would yield multiple peaks due to the multi-wavelength laser, necessitating an \u201cinformed\u201d decision to guarantee maximum wavelength allocation [17]; we will call this wavelength arbitration. Previous work has focused on circuit-level architectures [17, 18, 19, 20, 21], system-level tradeoffs [7, 22, 23] and wavelength allocation algorithms [24, 25, 26], addressing specific aspects of arbitration.\nTo achieve a scalable arbitration system, we introduce a hierarchical framework for wavelength arbitration.\nIn our framework, arbitration is fundamentally policy-driven, with policies defining the wavelength interface of the DWDM transceiver.\nThe implementing algorithm and supporting circuit-level architecture then operate according to these policies.\nThe primary function of arbitration, therefore, is to effectively coordinate microrings while accounting for device-level variations and system-wide constraints, ensuring both robustness and autonomy.\nWhile this holistic view is desirable, the challenge lies in efficiently integrating these policies with scalable algorithms and architectures to maintain optimal system performance under varying conditions. To address this, we introduce an arbitration model to decouple the analysis of policy and algorithm.\nThis model has two components: an ideal, wavelength-aware model and a wavelength-oblivious model.\nThe ideal model evaluates the arbitration policy under the assumption of wavelength-awareness, ensuring policies are effective and well-defined.\nIn contrast, the wavelength-oblivious model evaluates the algorithm under practical conditions, where conflict resolution and wavelength allocation are performed without specific wavelength information.\nThis approach allows us to evaluate arbitration robustness by analyzing failure probabilities across policies and algorithms, while also developing an algorithm that aligns closely with the ideal model, achieving scalability and robustness for high-volume deployment. This paper is organized as follows.\nWe begin with an overview of the system model in Section\u00a0II and provide a formal definition of robustness metrics in Section\u00a0III.\nSection\u00a0IV compares different arbitration policies and investigates system-level tradeoffs.\nSection\u00a0V presents our proposed arbitration algorithm and compares it with the baseline sequential tuning scheme.\nSection\u00a0VI concludes the paper. \nII Generalized Wavelength-Domain Model for Wavelength Arbitration\n Fig.\u00a01(a) shows the model of wavelength arbitration used in this paper.\nWe assume a typical DWDM transceiver architecture [2] where a multi-wavelength laser propagates into a shared waveguide bus and to downstream microring resonators.\nDuring transceiver initialization, each microring captures one of the light wavelengths within its tuning range.\nThis paper focuses on microring wavelength management at initialization, which is represented by the wavelength arbiter in Fig.\u00a01(a).\nFor simplicity, the wavelength initialization process is projected onto the wavelength space; thus, only the wavelength domain is considered throughout the paper. \nII-A Wavelength Arbiter\n In a typical microring-based DWDM transceiver, the local wavelength status is tracked and updated by the per-microring tuner and sensor circuit [8, 3, 4].\nWe define the wavelength arbiter as an entity responsible for global wavelength management, maintaining the global wavelength status and overseeing the coordination of the microrings.\nThe outcome of wavelength arbitration is determined by policy, while we define algorithm as the detailed steps to achieve this outcome.\nFor clarity, the algorithm discussed in this paper is wavelength-oblivious; it can access and update the local wavelength status only through the per-microring tuner and sensor. To explain policy, we first describe the role of microring resonance wavelength in PHY-level data transmission.\nJust as a port serves a transmission point in the spatial domain, a microring acts as a port in the wavelength-domain.\nFor instance, assume an electrical transmit lane drives a microring at wavelength \u03bb1subscript\ud835\udf061\\lambda_{1}italic_\u03bb start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, and on the receive side, another microring at resonance wavelength \u03bb1subscript\ud835\udf061\\lambda_{1}italic_\u03bb start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT is connected to an electrical receive lane.\nWhen the host sends packet P\u20621P1\\text{P}1P 1 through the electrical transmit lane, it is retransmitted optically at wavelength \u03bb1subscript\ud835\udf061\\lambda_{1}italic_\u03bb start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, and received by the receive microring, reaching the client side.\nThus, optical connectivity for P\u20621P1\\text{P}1P 1 is determined by wavelength \u03bb1subscript\ud835\udf061\\lambda_{1}italic_\u03bb start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, serving as a virtual optical lane for data transmission. In the context of DWDM, microring spectral ordering can be understood as optical lane ordering (or equivalently, optical port mapping).\nFrom a communication standpoint, the transmitting and receiving endpoints should agree on the data ordering.\nIn microring-based DWDM systems, it is preferred that they are cognizant of optical lane ordering during electrical-to-optical and optical-to-electrical conversions111One may argue that bit orderings at both endpoints are handled at MAC-level serialization and deserialization. However, from a transceiver perspective, it is useful to keep track of on-the-fly lane ordering, especially for low-latency applications [27]..\nWe assume an optical port remapper (bit-shuffler backend in Georgas et al.\u00a0[22]) at the optical transceiver backend to ensure the electrical interfaces at both ends see identical ordering and streamline the data communication.\nThe level of remapping, proportional to the power and latency cost of the backend, is determined by the enforcement level over the microring spectral ordering post-arbitration.\nIn this paper, we adopt the enforcement level as an arbitration policy. \nII-B Arbitration Policy\n As shown in Fig.\u00a01(b), we consider three wavelength arbitration policies: Lock-to-Deterministic (LtD), Lock-to-Cyclic (LtC) and Lock-to-Any (LtA), each corresponding to strong, medium and weak ordering enforcement, respectively. LtD policy\ndemonstrates the strongest enforcement on the post-arbitration microring spectral ordering.\nFor example, if the target microring spectral ordering (0,1,2,3,4,5)012345\\left(0,1,2,3,4,5\\right)( 0 , 1 , 2 , 3 , 4 , 5 ) is specified, the arbiter pairs microrings with laser wavelengths in that exact order and disallows any other spectral orderings.\nBy assigning microrings to a predetermined spectral ordering, it can effectively preserve optical lane ordering, bypassing any issues of optical lane shuffling.\nSpectral-domain link optimization techniques can be applied more easily, such as inter-microring crosstalk minimization [28].\nHowever, despite its apparent advantages, it is challenging to implement due to high microring tuning range requirements, as we will show in Section\u00a0IV. LtC policy\nrelaxes the enforcement by allowing cyclic equivalents of the specified target spectral ordering.\nFor example, if the target microring spectral ordering (0,1,2,3,4,5)012345\\left(0,1,2,3,4,5\\right)( 0 , 1 , 2 , 3 , 4 , 5 ) is specified as in Fig.\u00a01, the arbiter can assign microring wavelengths in cyclic equivalents of the ordering, such as (2,3,4,5,0,1)234501\\left(2,3,4,5,0,1\\right)( 2 , 3 , 4 , 5 , 0 , 1 ), (0,1,2,3,4,5)012345\\left(0,1,2,3,4,5\\right)( 0 , 1 , 2 , 3 , 4 , 5 ).\nThe required microring tuning range is smaller than that of the LtD policy.\nThis is because a linear shift in spectral ordering effectively cancels any global offsets in laser and microring wavelengths due to ring resonance periodicity222Assume R0, R1, R2, R3 with \\qty4\\nano FSR are assigned to \\qtylist1300;1301;1302;1303\\nano laser wavelengths in the (0,1,2,3)0123\\left(0,1,2,3\\right)( 0 , 1 , 2 , 3 ) order. Due to resonance periodicity by FSR, microrings assigned to \\qtylist1301;1302;1303;1304\\nano (shift-by-1 order) effectively shift wavelengths by \\qty1\\nano. [22, 23].\nOptical port remapping can also be done with smaller overhead using a configurable barrel-shifter in hardware [22] or byte-shifting in software. LtA policy\ndoes not impose any restrictions on the final microring spectral ordering.\nIn other words, it allows any spectral ordering post-arbitration, such as (2,0,1,4,5,3)201453\\left(2,0,1,4,5,3\\right)( 2 , 0 , 1 , 4 , 5 , 3 ), (2,3,4,5,0,1)234501\\left(2,3,4,5,0,1\\right)( 2 , 3 , 4 , 5 , 0 , 1 ), or (0,1,2,3,4,5)012345\\left(0,1,2,3,4,5\\right)( 0 , 1 , 2 , 3 , 4 , 5 ).\nIntuitively, it has the least requirement on microring tuning range and is most amenable to tuning power optimization techniques [24, 26].\nHowever, the resulting spectral ordering is random; optical port remapping should be done by a configurable crossbar (bit-reorder mux in Georgas et al.\u00a0[22]) or byte-reordering in software, potentially incurring overhead to the system. \nII-C Multi-Wavelength Laser and Microring Row Model\n In this section, we develop a wavelength-domain model for multi-wavelength lasers and microring rows.\nWe denote pre/post-fabrication laser and microring wavelengths \u03bblaser,i0subscriptsuperscript\ud835\udf060laser\ud835\udc56\\lambda^{0}_{\\text{laser},i}italic_\u03bb start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT laser , italic_i end_POSTSUBSCRIPT, \u03bbring,i0subscriptsuperscript\ud835\udf060ring\ud835\udc56\\lambda^{0}_{\\text{ring},i}italic_\u03bb start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ring , italic_i end_POSTSUBSCRIPT and \u03bblaser,isubscript\ud835\udf06laser\ud835\udc56\\lambda_{\\text{laser},i}italic_\u03bb start_POSTSUBSCRIPT laser , italic_i end_POSTSUBSCRIPT, \u03bbring,isubscript\ud835\udf06ring\ud835\udc56\\lambda_{\\text{ring},i}italic_\u03bb start_POSTSUBSCRIPT ring , italic_i end_POSTSUBSCRIPT, respectively.\nVariation parameters have confidence interval bounds denoted by \u03c3\ud835\udf0e\\sigmaitalic_\u03c3, and their sampled values are represented by \u0394\u0394\\Deltaroman_\u0394.\nThe model is summarized in Fig.\u00a02 and Table\u00a0I. The pre-fabrication laser model is derived from the existing commercial standards such as CW-WDM MSA [29].\nEach laser wavelength \u03bblaser,i0subscriptsuperscript\ud835\udf060laser\ud835\udc56\\lambda^{0}_{\\text{laser},i}italic_\u03bb start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT laser , italic_i end_POSTSUBSCRIPT can be written as where \u03bbcentersubscript\ud835\udf06center\\lambda_{\\text{center}}italic_\u03bb start_POSTSUBSCRIPT center end_POSTSUBSCRIPT denotes the center wavelength and i\ud835\udc56iitalic_i is the index of the wavelength in the wavelength-domain ordering, ranging from 00 to Nc\u2062h\u22121subscript\ud835\udc41\ud835\udc50\u210e1N_{ch}-1italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT - 1.\nDefault parameters for \u03bbcentersubscript\ud835\udf06center\\lambda_{\\text{center}}italic_\u03bb start_POSTSUBSCRIPT center end_POSTSUBSCRIPT, Nc\u2062hsubscript\ud835\udc41\ud835\udc50\u210eN_{ch}italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT and \u03bbg\u2062Ssubscript\ud835\udf06\ud835\udc54\ud835\udc46\\lambda_{gS}italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT are chosen as \\qty1300\\nano, 8 and \\qty1.12\\nano (\\qty200\\giga in O-band), respectively.\nNote that a specific choice of \u03bbcentersubscript\ud835\udf06center\\lambda_{\\text{center}}italic_\u03bb start_POSTSUBSCRIPT center end_POSTSUBSCRIPT is not relevant to our focus.\nFor wavelength arbitration, only the relative distances between wavelengths matter. The pre-fabrication microring model is derived in an analogous manner.\nThe difference is that microring wavelengths are pre-biased to the blue-side by resonance wavelength bias (\u03bbr\u2062Bsubscript\ud835\udf06\ud835\udc5f\ud835\udc35\\lambda_{rB}italic_\u03bb start_POSTSUBSCRIPT italic_r italic_B end_POSTSUBSCRIPT) to account for the strict red-shifting of microring thermal tuning [22, 2].\nAdditionally, we define i\ud835\udc56iitalic_i to correspond to the spatial location of a microring; the i\ud835\udc56iitalic_ith microring is the i\ud835\udc56iitalic_ith closest microring in a row from the light input side (See Fig.\u00a01(a)).\nrisubscript\ud835\udc5f\ud835\udc56r_{i}italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is defined as a map between the spatial ordering and spectral ordering of the microring row at pre-fabrication333sisubscript\ud835\udc60\ud835\udc56s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is similarly similarly, but refers to the target post-arbitration ordering..\nWe note that risubscript\ud835\udc5f\ud835\udc56r_{i}italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT does not need to follow the natural ordering; it can be chosen by the designer for purposes such as for crosstalk optimization [28].\nThe pre-fabrication microring wavelength \u03bbring,i0subscriptsuperscript\ud835\udf060ring\ud835\udc56\\lambda^{0}_{\\text{ring},i}italic_\u03bb start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ring , italic_i end_POSTSUBSCRIPT is written as Default parameter for \u03bbr\u2062Bsubscript\ud835\udf06\ud835\udc5f\ud835\udc35\\lambda_{rB}italic_\u03bb start_POSTSUBSCRIPT italic_r italic_B end_POSTSUBSCRIPT is chosen as \\qty4.48\\nano to maximize the distance from the laser center wavelength and keep the microring resonances lower than the laser wavelengths [22]. A major difference with the post-fabrication model is the addition of global and local variations due to the high sensitivity of photonic devices to fabrication and environmental variations.\nVarious variability models have been proposed for integrated photonics [30, 31, 32, 33, 34, 35]; however, for our focus, we adopt a simplified approach for our purpose: considering only global (all-channel) and local (per-channel) variations.\nThey are denoted in our model as \u03c3l\u2062G\u2062Vsubscript\ud835\udf0e\ud835\udc59\ud835\udc3a\ud835\udc49\\sigma_{lGV}italic_\u03c3 start_POSTSUBSCRIPT italic_l italic_G italic_V end_POSTSUBSCRIPT and \u03c3l\u2062L\u2062Vsubscript\ud835\udf0e\ud835\udc59\ud835\udc3f\ud835\udc49\\sigma_{lLV}italic_\u03c3 start_POSTSUBSCRIPT italic_l italic_L italic_V end_POSTSUBSCRIPT for the laser and \u03c3r\u2062G\u2062Vsubscript\ud835\udf0e\ud835\udc5f\ud835\udc3a\ud835\udc49\\sigma_{rGV}italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_G italic_V end_POSTSUBSCRIPT and \u03c3r\u2062L\u2062Vsubscript\ud835\udf0e\ud835\udc5f\ud835\udc3f\ud835\udc49\\sigma_{rLV}italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_L italic_V end_POSTSUBSCRIPT for the microring, respectively.\nTo simplify further, we lump the global variations (\u03c3l\u2062G\u2062Vsubscript\ud835\udf0e\ud835\udc59\ud835\udc3a\ud835\udc49\\sigma_{lGV}italic_\u03c3 start_POSTSUBSCRIPT italic_l italic_G italic_V end_POSTSUBSCRIPT and \u03c3r\u2062G\u2062Vsubscript\ud835\udf0e\ud835\udc5f\ud835\udc3a\ud835\udc49\\sigma_{rGV}italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_G italic_V end_POSTSUBSCRIPT) into the \u201cgrid offset variation\u201d between the microring row and laser wavelengths, which we call the grid offset (\u03c3g\u2062Osubscript\ud835\udf0e\ud835\udc54\ud835\udc42\\sigma_{gO}italic_\u03c3 start_POSTSUBSCRIPT italic_g italic_O end_POSTSUBSCRIPT).\nThis is possible because only the relative distances matter in our discussion.\nSince those global variations are uncorrelated, we simply sum the two to derive the grid offset444Strictly speaking, \u03c3\ud835\udf0e\\sigmaitalic_\u03c3 typically denotes the statistical variance where uncorrelated sums are sum-of-squares. In this paper, we use a linear sum for simplicity..\nWithout loss of generality, we add the grid offset to the post-fabrication laser wavelength model. The post-fabrication multi-wavelength laser model can be written as where \u0394g\u2062Osubscript\u0394\ud835\udc54\ud835\udc42\\Delta_{gO}roman_\u0394 start_POSTSUBSCRIPT italic_g italic_O end_POSTSUBSCRIPT denotes the sampled grid offset and \u0394l\u2062L\u2062V,isubscript\u0394\ud835\udc59\ud835\udc3f\ud835\udc49\ud835\udc56\\Delta_{lLV,i}roman_\u0394 start_POSTSUBSCRIPT italic_l italic_L italic_V , italic_i end_POSTSUBSCRIPT denotes the sampled laser local variation at the i\ud835\udc56iitalic_ith laser wavelength.\nThe default parameter for \u03c3l\u2062L\u2062Vsubscript\ud835\udf0e\ud835\udc59\ud835\udc3f\ud835\udc49\\sigma_{lLV}italic_\u03c3 start_POSTSUBSCRIPT italic_l italic_L italic_V end_POSTSUBSCRIPT corresponds to the channel bandwidth in the CW-WDM MSA specification [29], which is 25% of the grid spacing (\u03bbg\u2062Ssubscript\ud835\udf06\ud835\udc54\ud835\udc46\\lambda_{gS}italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT).\nThis value is a conservative estimate since reported multi-wavelength laser sources have consistently achieved better local variation [36, 37, 38, 39].\n\u03c3l\u2062G\u2062Vsubscript\ud835\udf0e\ud835\udc59\ud835\udc3a\ud835\udc49\\sigma_{lGV}italic_\u03c3 start_POSTSUBSCRIPT italic_l italic_G italic_V end_POSTSUBSCRIPT can be understood as the sum of center wavelength offset and variation range in CW-WDM MSA, which corresponds to the fabrication and environmental variations and specified as \\qty5\\nano and \\qty4\\nano, respectively.\nThus, we assume the total laser global variation (\u03c3l\u2062G\u2062Vsubscript\ud835\udf0e\ud835\udc59\ud835\udc3a\ud835\udc49\\sigma_{lGV}italic_\u03c3 start_POSTSUBSCRIPT italic_l italic_G italic_V end_POSTSUBSCRIPT) to be \\qty9\\nano.\nFor \u03c3r\u2062G\u2062Vsubscript\ud835\udf0e\ud835\udc5f\ud835\udc3a\ud835\udc49\\sigma_{rGV}italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_G italic_V end_POSTSUBSCRIPT, we choose a default value of \\qty6\\nano, which we explain in the next paragraph. The post-fabrication microring row model is expressed as where \u0394r\u2062L\u2062V,isubscript\u0394\ud835\udc5f\ud835\udc3f\ud835\udc49\ud835\udc56\\Delta_{rLV,i}roman_\u0394 start_POSTSUBSCRIPT italic_r italic_L italic_V , italic_i end_POSTSUBSCRIPT denotes the sampled resonance local variation at the i\ud835\udc56iitalic_ith microring wavelength.\nIt is useful to note that most of the microring variations arise from global resonance variation (wafer-to-wafer and inter-die [32]), which can be partially mitigated by barrel-shifting the microring spectral ordering.\nGlobal process variations can be as large as \\qty12\\nano [33] due to substrate thickness and etch depth variations during fabrication [33, 40, 35, 41].\nHowever, we assume the total microring global variation (\u03c3r\u2062G\u2062Vsubscript\ud835\udf0e\ud835\udc5f\ud835\udc3a\ud835\udc49\\sigma_{rGV}italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_G italic_V end_POSTSUBSCRIPT) to be \\qty6\\nano, as process variation can also be mitigated by die binning [33].\nExternal thermal aggressions can also impact global resonance variation; however, it is difficult to characterize this effect without a well-defined deployment environment, such as that described by Lee et al.\u00a0[8], which we do not include in our analysis.\nMicroring local variation (\u03c3r\u2062L\u2062Vsubscript\ud835\udf0e\ud835\udc5f\ud835\udc3f\ud835\udc49\\sigma_{rLV}italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_L italic_V end_POSTSUBSCRIPT), often referred to as channel-to-channel spacing variation, is the result of intra-die process variation and systematic error.\nHowever, it is challenging to extract a simple model for local variation due to the following factors:\nSystematic error refers to any deviation from ideal channel spacing that arises during the implementation of resonance stepping, which is typically achieved by incrementing ring radii.\nThis process is particularly prone to design-time errors.\nIn addition, intra-die process variation has a strong dependency on the physical distance between the microrings [42, 31, 33, 35], making it difficult to generalize across designs.\nRather than pursuing a highly detailed local variation model, we employ a zeroth-order approach, selecting a single-valued \u03c3r\u2062L\u2062Vsubscript\ud835\udf0e\ud835\udc5f\ud835\udc3f\ud835\udc49\\sigma_{rLV}italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_L italic_V end_POSTSUBSCRIPT as an upper bound to capture worst-case variations.\nThe default value is set to \\qty2.24\\nano which is 2\u00d7\u03bbg\u2062S2subscript\ud835\udf06\ud835\udc54\ud835\udc462\\times\\lambda_{gS}2 \u00d7 italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT.\nHowever, for most cases, we sweep this value from \\qty0.28\\nano (0.25\u00d7\u03bbg\u2062S0.25subscript\ud835\udf06\ud835\udc54\ud835\udc460.25\\times\\lambda_{gS}0.25 \u00d7 italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT) to \\qty8.96\\nano (8\u00d7\u03bbg\u2062S8subscript\ud835\udf06\ud835\udc54\ud835\udc468\\times\\lambda_{gS}8 \u00d7 italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT) to observe the system\u2019s behavior under different microring variations. The tuning range of the i\ud835\udc56iitalic_ith microring can be represented as a union of intervals with a width corresponding to the tuning range and spaced by the free spectral range (FSR): where \u039bT\u2062R,isubscript\u039b\ud835\udc47\ud835\udc45\ud835\udc56\\Lambda_{TR,i}roman_\u039b start_POSTSUBSCRIPT italic_T italic_R , italic_i end_POSTSUBSCRIPT denotes the total tuning range, \u03bbF\u2062S\u2062R,isubscript\ud835\udf06\ud835\udc39\ud835\udc46\ud835\udc45\ud835\udc56\\lambda_{FSR,i}italic_\u03bb start_POSTSUBSCRIPT italic_F italic_S italic_R , italic_i end_POSTSUBSCRIPT denotes the FSR of the i\ud835\udc56iitalic_ith microring, \u03bbT\u2062R,isubscript\ud835\udf06\ud835\udc47\ud835\udc45\ud835\udc56\\lambda_{TR,i}italic_\u03bb start_POSTSUBSCRIPT italic_T italic_R , italic_i end_POSTSUBSCRIPT denotes the tuning range of the i\ud835\udc56iitalic_ith microring, and j\u2208[0,\u00b11,\u2026]\ud835\udc570plus-or-minus1\u2026j\\in\\left[0,\\pm 1,\\ldots\\right]italic_j \u2208 [ 0 , \u00b1 1 , \u2026 ].\nAdditionally, \u03bbF\u2062S\u2062R,i=\u03bb\u00afF\u2062S\u2062R+\u0394F\u2062S\u2062R,isubscript\ud835\udf06\ud835\udc39\ud835\udc46\ud835\udc45\ud835\udc56subscript\u00af\ud835\udf06\ud835\udc39\ud835\udc46\ud835\udc45subscript\u0394\ud835\udc39\ud835\udc46\ud835\udc45\ud835\udc56\\lambda_{FSR,i}=\\bar{\\lambda}_{FSR}+\\Delta_{FSR,i}italic_\u03bb start_POSTSUBSCRIPT italic_F italic_S italic_R , italic_i end_POSTSUBSCRIPT = over\u00af start_ARG italic_\u03bb end_ARG start_POSTSUBSCRIPT italic_F italic_S italic_R end_POSTSUBSCRIPT + roman_\u0394 start_POSTSUBSCRIPT italic_F italic_S italic_R , italic_i end_POSTSUBSCRIPT and \u03bbT\u2062R,i=\u03bb\u00afT\u2062R+\u0394T\u2062R,isubscript\ud835\udf06\ud835\udc47\ud835\udc45\ud835\udc56subscript\u00af\ud835\udf06\ud835\udc47\ud835\udc45subscript\u0394\ud835\udc47\ud835\udc45\ud835\udc56\\lambda_{TR,i}=\\bar{\\lambda}_{TR}+\\Delta_{TR,i}italic_\u03bb start_POSTSUBSCRIPT italic_T italic_R , italic_i end_POSTSUBSCRIPT = over\u00af start_ARG italic_\u03bb end_ARG start_POSTSUBSCRIPT italic_T italic_R end_POSTSUBSCRIPT + roman_\u0394 start_POSTSUBSCRIPT italic_T italic_R , italic_i end_POSTSUBSCRIPT, where \u0394F\u2062S\u2062R,isubscript\u0394\ud835\udc39\ud835\udc46\ud835\udc45\ud835\udc56\\Delta_{FSR,i}roman_\u0394 start_POSTSUBSCRIPT italic_F italic_S italic_R , italic_i end_POSTSUBSCRIPT is the sampled FSR variation at the i\ud835\udc56iitalic_ith microring and \u0394T\u2062R,isubscript\u0394\ud835\udc47\ud835\udc45\ud835\udc56\\Delta_{TR,i}roman_\u0394 start_POSTSUBSCRIPT italic_T italic_R , italic_i end_POSTSUBSCRIPT is the sampled tuning range variation at the i\ud835\udc56iitalic_ith microring.\nIn a typical DWDM architecture, microring FSR is maximally filled with wavelength grids, or \u03bb\u00afF\u2062S\u2062R=Nc\u2062h\u00d7\u03bbg\u2062Ssubscript\u00af\ud835\udf06\ud835\udc39\ud835\udc46\ud835\udc45subscript\ud835\udc41\ud835\udc50\u210esubscript\ud835\udf06\ud835\udc54\ud835\udc46\\bar{\\lambda}_{FSR}=N_{ch}\\times\\lambda_{gS}over\u00af start_ARG italic_\u03bb end_ARG start_POSTSUBSCRIPT italic_F italic_S italic_R end_POSTSUBSCRIPT = italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT \u00d7 italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT, which is the value we choose (\\qty8.96\\nano or \\qty1.6\\tera).\nThe variation in the FSR (\u03c3F\u2062S\u2062Rsubscript\ud835\udf0e\ud835\udc39\ud835\udc46\ud835\udc45\\sigma_{FSR}italic_\u03c3 start_POSTSUBSCRIPT italic_F italic_S italic_R end_POSTSUBSCRIPT) is known to be relatively well-controlled, typically around 1% [23, 41, 43].\nWhile we assign a default parameter of \\qty2.24\\nano for tuning range (\u03bb\u00afT\u2062Rsubscript\u00af\ud835\udf06\ud835\udc47\ud835\udc45\\bar{\\lambda}_{TR}over\u00af start_ARG italic_\u03bb end_ARG start_POSTSUBSCRIPT italic_T italic_R end_POSTSUBSCRIPT), we typically sweep the value from \\qty1.12\\nano (1\u00d7\u03bbg\u2062S1subscript\ud835\udf06\ud835\udc54\ud835\udc461\\times\\lambda_{gS}1 \u00d7 italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT) to \\qty10.08\\nano (9\u00d7\u03bbg\u2062S9subscript\ud835\udf06\ud835\udc54\ud835\udc469\\times\\lambda_{gS}9 \u00d7 italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT) [44] and observe the trend.\nFor tuning range variation (\u03c3T\u2062Rsubscript\ud835\udf0e\ud835\udc47\ud835\udc45\\sigma_{TR}italic_\u03c3 start_POSTSUBSCRIPT italic_T italic_R end_POSTSUBSCRIPT), we assume a 10% variation based on the typical process, voltage and temperature variations of the tuner circuit. The final simplification concerns the variation distributions, which we model as uniform distributions bounded by their confidence intervals (\u03c3\u2217subscript\ud835\udf0e\\sigma_{*}italic_\u03c3 start_POSTSUBSCRIPT \u2217 end_POSTSUBSCRIPT).\nThis represents a significant departure from the commonly assumed Gaussian distribution.\nHowever, this approach can be seen as a conservative simplification of a trimmed Gaussian distribution, where the extremities are sampled more frequently.\nThis simplification enables a more sample-efficient exploration of the statistical bounds necessary for achieving perfect arbitration success.\nAdditionally, it can be easily adjusted for specific use cases, such as yield analysis or global sensitivity analysis. \nIII Evaluation Methodology\n In this section, we introduce the evaluation methodology for arbitration robustness.\nPreviously, tuning strategies have been evaluated based on their resulting tuning power, which facilitates comparisons of algorithm efficiency [22, 24, 25, 26].\nHowever, tuning power assumes arbitration success, whereas evaluating robustness requires accounting for arbitration failures.\nMoreover, policy and algorithm represent distinct aspects of arbitration design\u2014system specification and implementation\u2014and their separation must be accounted for in an efficient, hierarchical analysis.\nThis necessitates a more detailed approach to devising simulation experiments and assessing arbitration robustness. \nIII-A Policy Evaluation\n As the policy dictates wavelength arrangement post-arbitration, our simulation setup assumes wavelength-awareness during arbitration, which we refer to as the \u201cideal arbitration model.\u201d\nThis model arbitrates based on absolute wavelength values, effectively evaluating the arbiter\u2019s specification independently from the wavelength-oblivious algorithm implementation.\nIn particular, different policies correspond to varying levels of spectral ordering enforcement, leading to distinct sets of device parameters for arbitration success, which can be deduced from the wavelength-aware arbitration experiments. We define Arbitration Failure Probability (AFP) as a metric to assess arbitration robustness under specific policy and varying system configurations.\nThe system-under-test is generated by sampling multi-wavelength lasers and microring rows, with variations determined by their respective sources.\nEach system-under-test is subjected to an arbitration test using a specific policy and the ideal arbitration model.\nThe number of arbitration failures is recorded, and AFP is calculated by dividing this number by the total number of trials.\nEssentially, AFP reflects the arbitration yield, where failure to arbitrate successfully is treated as transceiver failure, assuming accurate device models. \nIII-B Algorithm Evaluation\n Since the algorithm operates in a wavelength-oblivious manner, the simulation setup uses the wavelength-oblivious arbitration model to reflect the algorithm\u2019s operational constraints.\nWhile AFP can be used for algorithm evaluation, it may obscure the effectiveness of the algorithm due to the influence of the policy it implements.\nTo isolate the effect of the algorithm implementation, we introduce a new metric: Conditional Arbitration Failure Probability (CAFP). CAFP is defined as the conditional failure probability of the arbitration algorithm, given that the ideal arbitration under the same policy is successful.\nThis metric quantifies how closely the algorithm approximates the behavior of the ideal arbitration model.\nCAFP is calculated by running tests using a specific algorithm and the wavelength-oblivious model, and recording failures that occur when the same system-under-test, under the corresponding policy, achieves arbitration success in the ideal model.\nThe total failure rate, considering both the algorithm and the policy, can also be computed using conditional probability as follows: where, according to our notations, Palg\u2062(fail)subscript\ud835\udc43algfailP_{\\text{alg}}(\\text{fail})italic_P start_POSTSUBSCRIPT alg end_POSTSUBSCRIPT ( fail ) denotes the total failure probability, Palg|succ\u2062(fail)subscript\ud835\udc43conditionalalgsuccfailP_{\\text{alg}|\\text{succ}}(\\text{fail})italic_P start_POSTSUBSCRIPT alg | succ end_POSTSUBSCRIPT ( fail ) represents the CAFP, P\u2062(fail)\ud835\udc43failP(\\text{fail})italic_P ( fail ) is the AFP of the corresponding policy, and P\u2062(succ)=1\u2212P\u2062(fail)\ud835\udc43succ1\ud835\udc43failP(\\text{succ})=1-P(\\text{fail})italic_P ( succ ) = 1 - italic_P ( fail ) is the probability of policy success.\nNote that Palg|fail\u2062(fail)=1subscript\ud835\udc43conditionalalgfailfail1P_{\\text{alg}|\\text{fail}}(\\text{fail})=1italic_P start_POSTSUBSCRIPT alg | fail end_POSTSUBSCRIPT ( fail ) = 1, as the algorithm will always fail when the ideal arbitration fails. \nIV Analysis of Arbitration Policy\n In this section, we compare different arbitration policies and analyze their tradeoffs.\nWe assume that three key parameters are considered during the arbitration design phase: arbitration policy, pre-fabrication microring spectral ordering (ri)subscript\ud835\udc5f\ud835\udc56\\left(r_{i}\\right)( italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) and target post-arbitration microring spectral ordering (si)subscript\ud835\udc60\ud835\udc56\\left(s_{i}\\right)( italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ).\nWe select four main test cases for policy evaluation, namely LtA-N/A, LtA-P/A, LtC-N/N, and LtC-P/P, which are summarized in Table\u00a0II.\nLtA and LtC stand for Lock-to-Any and Lock-to-Cyclic policies, as defined in Section\u00a0II-A, according to their levels of spectral ordering enforcement.\nFor pre-fabrication spectral ordering, we consider two specific instances: Natural and Permuted ordering.\nNatural refers to a straightforward ordering of (0,1,2,\u22ef,Nc\u2062h\u22121)012\u22efsubscript\ud835\udc41\ud835\udc50\u210e1\\left(0,1,2,\\cdots,N_{ch}-1\\right)( 0 , 1 , 2 , \u22ef , italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT - 1 ) for Nc\u2062hsubscript\ud835\udc41\ud835\udc50\u210eN_{ch}italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT-DWDM.\nPermuted refers to (0,Nc\u2062h/2,1,Nc\u2062h/2+1,\u22ef)0subscript\ud835\udc41\ud835\udc50\u210e21subscript\ud835\udc41\ud835\udc50\u210e21\u22ef\\left(0,N_{ch}/2,1,N_{ch}/2+1,\\cdots\\right)( 0 , italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT / 2 , 1 , italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT / 2 + 1 , \u22ef ), representing a sufficiently \u201cshuffled\u201d ordering instance.\nFor post-arbitration spectral ordering, we assume the designer aims to match the pre-fabrication ordering555A case of si\u2260risubscript\ud835\udc60\ud835\udc56subscript\ud835\udc5f\ud835\udc56s_{i}\\neq r_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2260 italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, or \u201cwavelength reconfiguration\u201d [21] is beyond the scope of this paper., except in the case of the LtA policy, where post-arbitration spectral ordering is labeled \u201cAny\u201d indicating no specific enforcement.\nFor each experiment, we conduct 10,000 trials, using 100 multi-wavelength lasers and 100 microring row samples.\nUnless otherwise noted, the default model parameters listed in Table\u00a0I are used throughout the experiments. \nIV-A General Trend\n Fig.\u00a04 illustrates the AFP across local resonance variation (\u03c3r\u2062L\u2062Vsubscript\ud835\udf0e\ud835\udc5f\ud835\udc3f\ud835\udc49\\sigma_{rLV}italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_L italic_V end_POSTSUBSCRIPT) and microring tuning range (\u03bb\u00afT\u2062Rsubscript\u00af\ud835\udf06\ud835\udc47\ud835\udc45\\bar{\\lambda}_{TR}over\u00af start_ARG italic_\u03bb end_ARG start_POSTSUBSCRIPT italic_T italic_R end_POSTSUBSCRIPT) values for different arbitration policies.\nIn the plot, darker colors indicate higher AFP, with dark purple (A\u2062F\u2062P=1\ud835\udc34\ud835\udc39\ud835\udc431AFP=1italic_A italic_F italic_P = 1) representing complete arbitration failures for all trials, while brighter colors, particularly white (A\u2062F\u2062P=0\ud835\udc34\ud835\udc39\ud835\udc430AFP=0italic_A italic_F italic_P = 0), indicate near or complete success.\nThe plot exhibits a shmoo pattern: lower tuning ranges and higher resonance variations are more likely to result in arbitration failures, whereas higher tuning ranges and lower variations increase the likelihood of arbitration success.\nFor each microring resonance variation value, we identify the minimum tuning range that achieves complete arbitration success, which we call the minimum tuning range.\nWe use this minimum tuning range as a proxy to compare different policies and system configurations below. The LtA policy has the lowest tuning range requirement, followed by the LtC and the LtD policy, reflecting how relaxed spectral ordering reduces the microring tuning range requirement.\nFig.\u00a05 shows the tuning range trends for different DWDM configurations, including wdm8/16, which refers to systems with 8 or 16 channels (Nc\u2062hsubscript\ud835\udc41\ud835\udc50\u210eN_{ch}italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT), and g200/400, indicating channel spacings of \\qty200\\giga or \\qty400\\giga (\u03bbg\u2062Ssubscript\ud835\udf06\ud835\udc54\ud835\udc46\\lambda_{gS}italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT).\nBoth LtA and LtC policies exhibit a similar behavior, with their minimum tuning ranges saturating beyond a certain resonance variation.\nFor LtA, saturation occurs around \u03c3r\u2062L\u2062V\u223c4\u00d7\u03bbg\u2062Ssimilar-tosubscript\ud835\udf0e\ud835\udc5f\ud835\udc3f\ud835\udc494subscript\ud835\udf06\ud835\udc54\ud835\udc46\\sigma_{rLV}\\sim 4\\times\\lambda_{gS}italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_L italic_V end_POSTSUBSCRIPT \u223c 4 \u00d7 italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT for wdm8 and \u223c8\u00d7\u03bbg\u2062Ssimilar-toabsent8subscript\ud835\udf06\ud835\udc54\ud835\udc46\\sim 8\\times\\lambda_{gS}\u223c 8 \u00d7 italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT for wdm16, as the microring resonance can be located anywhere within the FSR once the variation range, spanning \u2212\u03c3r\u2062L\u2062Vsubscript\ud835\udf0e\ud835\udc5f\ud835\udc3f\ud835\udc49-\\sigma_{rLV}- italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_L italic_V end_POSTSUBSCRIPT to +\u03c3r\u2062L\u2062Vsubscript\ud835\udf0e\ud835\udc5f\ud835\udc3f\ud835\udc49+\\sigma_{rLV}+ italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_L italic_V end_POSTSUBSCRIPT (totaling 2\u00d7\u03c3r\u2062L\u2062V2subscript\ud835\udf0e\ud835\udc5f\ud835\udc3f\ud835\udc492\\times\\sigma_{rLV}2 \u00d7 italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_L italic_V end_POSTSUBSCRIPT or the microring FSR), covers the entire FSR.\nLtC saturates earlier than LtA, likely because its minimum tuning range exceeds the microring FSR before the knee point of LtA\u2019s tuning range requirement.\nBeyond the microring FSR, LtC arbitration success is expected to be guaranteed due to the resonance periodicity. Before reaching saturation, both LtA and LtC exhibit a near-linear ramp in minimum tuning range, as shown in Fig.\u00a05(e-h).\nThe slope of the ramp is approximately 2, which we explain as follows:\n\u03bb\u00afm\u2062i\u2062n\u2062T\u2062Rsubscript\u00af\ud835\udf06\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc47\ud835\udc45\\bar{\\lambda}_{minTR}over\u00af start_ARG italic_\u03bb end_ARG start_POSTSUBSCRIPT italic_m italic_i italic_n italic_T italic_R end_POSTSUBSCRIPT denotes the minimum tuning range at \u03c3r\u2062L\u2062Vsubscript\ud835\udf0e\ud835\udc5f\ud835\udc3f\ud835\udc49\\sigma_{rLV}italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_L italic_V end_POSTSUBSCRIPT and \u03bb\u00afm\u2062i\u2062n\u2062T\u2062R0subscriptsuperscript\u00af\ud835\udf060\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc47\ud835\udc45\\bar{\\lambda}^{0}_{minTR}over\u00af start_ARG italic_\u03bb end_ARG start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m italic_i italic_n italic_T italic_R end_POSTSUBSCRIPT denotes the minimum tuning range when \u03c3r\u2062L\u2062V=0subscript\ud835\udf0e\ud835\udc5f\ud835\udc3f\ud835\udc490\\sigma_{rLV}=0italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_L italic_V end_POSTSUBSCRIPT = 0.\n\u0394\u2062\u03bbr\u2062L\u2062V,i\u0394subscript\ud835\udf06\ud835\udc5f\ud835\udc3f\ud835\udc49\ud835\udc56\\Delta\\lambda_{rLV,i}roman_\u0394 italic_\u03bb start_POSTSUBSCRIPT italic_r italic_L italic_V , italic_i end_POSTSUBSCRIPT denotes the sampled local resonance variation for the i\ud835\udc56iitalic_ith microring, and assume that microrings R0 through R(Nc\u2062h\u22122subscript\ud835\udc41\ud835\udc50\u210e2N_{ch}-2italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT - 2) exhibit \u0394\u2062\u03bbr\u2062L\u2062V,i=\u2212\u03c3r\u2062L\u2062V\u0394subscript\ud835\udf06\ud835\udc5f\ud835\udc3f\ud835\udc49\ud835\udc56subscript\ud835\udf0e\ud835\udc5f\ud835\udc3f\ud835\udc49\\Delta\\lambda_{rLV,i}=-\\sigma_{rLV}roman_\u0394 italic_\u03bb start_POSTSUBSCRIPT italic_r italic_L italic_V , italic_i end_POSTSUBSCRIPT = - italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_L italic_V end_POSTSUBSCRIPT while R(Nc\u2062h\u22121subscript\ud835\udc41\ud835\udc50\u210e1N_{ch}-1italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT - 1) exhibits \u0394\u2062\u03bbr\u2062L\u2062V,i=\u03c3r\u2062L\u2062V\u0394subscript\ud835\udf06\ud835\udc5f\ud835\udc3f\ud835\udc49\ud835\udc56subscript\ud835\udf0e\ud835\udc5f\ud835\udc3f\ud835\udc49\\Delta\\lambda_{rLV,i}=\\sigma_{rLV}roman_\u0394 italic_\u03bb start_POSTSUBSCRIPT italic_r italic_L italic_V , italic_i end_POSTSUBSCRIPT = italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_L italic_V end_POSTSUBSCRIPT.\nDue to the flexible target spectral ordering that allows cyclic-equivalent arrangements, LtA and LtC can achieve the minimum tuning distance of \u03bb\u00afm\u2062i\u2062n\u2062T\u2062R0subscriptsuperscript\u00af\ud835\udf060\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc47\ud835\udc45\\bar{\\lambda}^{0}_{minTR}over\u00af start_ARG italic_\u03bb end_ARG start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m italic_i italic_n italic_T italic_R end_POSTSUBSCRIPT when all microrings exhibit a local resonance variation of \u0394\u2062\u03bbr\u2062L\u2062V,i=\u2212\u03c3r\u2062L\u2062V\u0394subscript\ud835\udf06\ud835\udc5f\ud835\udc3f\ud835\udc49\ud835\udc56subscript\ud835\udf0e\ud835\udc5f\ud835\udc3f\ud835\udc49\\Delta\\lambda_{rLV,i}=-\\sigma_{rLV}roman_\u0394 italic_\u03bb start_POSTSUBSCRIPT italic_r italic_L italic_V , italic_i end_POSTSUBSCRIPT = - italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_L italic_V end_POSTSUBSCRIPT.\nHowever, when only R(Nc\u2062h\u22121subscript\ud835\udc41\ud835\udc50\u210e1N_{ch}-1italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT - 1) exhibits \u0394\u2062\u03bbr\u2062L\u2062V,i=\u03c3r\u2062L\u2062V\u0394subscript\ud835\udf06\ud835\udc5f\ud835\udc3f\ud835\udc49\ud835\udc56subscript\ud835\udf0e\ud835\udc5f\ud835\udc3f\ud835\udc49\\Delta\\lambda_{rLV,i}=\\sigma_{rLV}roman_\u0394 italic_\u03bb start_POSTSUBSCRIPT italic_r italic_L italic_V , italic_i end_POSTSUBSCRIPT = italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_L italic_V end_POSTSUBSCRIPT, an additional 2\u00d7\u03c3r\u2062L\u2062V2subscript\ud835\udf0e\ud835\udc5f\ud835\udc3f\ud835\udc492\\times\\sigma_{rLV}2 \u00d7 italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_L italic_V end_POSTSUBSCRIPT tuning distance is required, leading to a minimum tuning range of \u03bb\u00afm\u2062i\u2062n\u2062T\u2062R=\u03bb\u00afm\u2062i\u2062n\u2062T\u2062R0+2\u00d7\u03c3r\u2062L\u2062Vsubscript\u00af\ud835\udf06\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc47\ud835\udc45subscriptsuperscript\u00af\ud835\udf060\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc47\ud835\udc452subscript\ud835\udf0e\ud835\udc5f\ud835\udc3f\ud835\udc49\\bar{\\lambda}_{minTR}=\\bar{\\lambda}^{0}_{minTR}+2\\times\\sigma_{rLV}over\u00af start_ARG italic_\u03bb end_ARG start_POSTSUBSCRIPT italic_m italic_i italic_n italic_T italic_R end_POSTSUBSCRIPT = over\u00af start_ARG italic_\u03bb end_ARG start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m italic_i italic_n italic_T italic_R end_POSTSUBSCRIPT + 2 \u00d7 italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_L italic_V end_POSTSUBSCRIPT.\nLtA policy shows a slower ramp beyond \u03c3r\u2062L\u2062V\u223c3\u00d7\u03bbg\u2062Ssimilar-tosubscript\ud835\udf0e\ud835\udc5f\ud835\udc3f\ud835\udc493subscript\ud835\udf06\ud835\udc54\ud835\udc46\\sigma_{rLV}\\sim 3\\times\\lambda_{gS}italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_L italic_V end_POSTSUBSCRIPT \u223c 3 \u00d7 italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT, likely because it can find a matching pair that does not require additional 2\u00d7\u03c3r\u2062L\u2062V2subscript\ud835\udf0e\ud835\udc5f\ud835\udc3f\ud835\udc492\\times\\sigma_{rLV}2 \u00d7 italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_L italic_V end_POSTSUBSCRIPT tuning distance, especially at higher resonance variations. Across DWDM configurations, wdm16-400g requires the most tuning range, followed by wdm8-400g, wdm16-200g, and wdm8-200g for LtA.\nLtC follows a similar trend, except that it has the same tuning range requirement for wdm8-400g and wdm16-200g, and it shows a larger gap between wdm16 and wdm8 configurations compared to LtA.\nThis suggests that, for 16-channel cases (and likely 32-channel cases and beyond), LtA exhibits more favorable scaling than LtC in terms of tuning range requirements.\nHowever, when local variations are kept within 3\u00d7\u03bbg\u2062S3subscript\ud835\udf06\ud835\udc54\ud835\udc463\\times\\lambda_{gS}3 \u00d7 italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT, LtC becomes a competitive candidate due to its more deterministic spectral ordering.\nDenser channel spacing can also reduce the tuning range requirement, but this must be balanced against inter-microring crosstalk and multi-wavelength laser costs. Lastly, the minimum tuning range for LtA-N/A shows no significant difference from LtA-P/A, nor for LtC-N/N compared to LtC-P/P.\nThis can be explained using a simple 2-WDM model:\nIf two arbitration trials with microrings R0 and R1 are identical but one has a pre-fabrication spectral ordering ri=(0,1)subscript\ud835\udc5f\ud835\udc5601r_{i}=\\left(0,1\\right)italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( 0 , 1 ) and the other has ri=(1,0)subscript\ud835\udc5f\ud835\udc5610r_{i}=\\left(1,0\\right)italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( 1 , 0 ), then the required tuning for R0 and R1 will effectively swap, but the minimum of two would be the same for both trials.\nThus, for ideal wavelength-aware arbitration, the difference in pre-fabrication ordering (with post-arbitration ordering si=risubscript\ud835\udc60\ud835\udc56subscript\ud835\udc5f\ud835\udc56s_{i}=r_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT for LtC) does not affect the minimum tuning range for either LtA or LtC.\nThis suggests that pre-fabrication and post-arbitration spectral ordering can be flexibly assigned, and their selection is not necessarily constrained by tuning range requirements. Next, we turn to case-studies to explore the impact of individual design parameter in greater detail. \nIV-B Case of Lock-to-Deterministic Policy\n The LtD policy requires a significantly larger microring tuning range than both the LtC and LtA policies to achieve perfect arbitration success, as demonstrated in previous work [22] and Fig.\u00a04.\nSince LtD requires microrings to be tuned by at least the grid offset to match corresponding laser wavelengths, we analyze the dependency of the minimum tuning range on the grid offset, as illustrated in Fig.\u00a06.\nFor small grid offsets (\u03c3g\u2062O\u2264subscript\ud835\udf0e\ud835\udc54\ud835\udc42absent\\sigma_{gO}\\leqitalic_\u03c3 start_POSTSUBSCRIPT italic_g italic_O end_POSTSUBSCRIPT \u2264 \\qty4\\nano), LtD exhibits a linear increase in the minimum tuning range until it saturates, whereas larger grid offsets (\u03c3g\u2062O\u2265subscript\ud835\udf0e\ud835\udc54\ud835\udc42absent\\sigma_{gO}\\geqitalic_\u03c3 start_POSTSUBSCRIPT italic_g italic_O end_POSTSUBSCRIPT \u2265 \\qty4\\nano) result in tuning ranges exceeding the FSR for any resonance local variation (\u03c3r\u2062L\u2062Vsubscript\ud835\udf0e\ud835\udc5f\ud835\udc3f\ud835\udc49\\sigma_{rLV}italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_L italic_V end_POSTSUBSCRIPT).\nThe linear slope is approximately 1, because LtD requires microrings to be tuned exactly to the laser wavelength order, and when microrings exhibit a blue-shift (\u0394\u2062\u03bbr\u2062L\u2062V,i=\u2212\u03c3r\u2062L\u2062V\u0394subscript\ud835\udf06\ud835\udc5f\ud835\udc3f\ud835\udc49\ud835\udc56subscript\ud835\udf0e\ud835\udc5f\ud835\udc3f\ud835\udc49\\Delta\\lambda_{rLV,i}=-\\sigma_{rLV}roman_\u0394 italic_\u03bb start_POSTSUBSCRIPT italic_r italic_L italic_V , italic_i end_POSTSUBSCRIPT = - italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_L italic_V end_POSTSUBSCRIPT), the required tuning distance increases by \u03c3r\u2062L\u2062Vsubscript\ud835\udf0e\ud835\udc5f\ud835\udc3f\ud835\udc49\\sigma_{rLV}italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_L italic_V end_POSTSUBSCRIPT.\nAs the grid offset adds directly to the required tuning distance, LtD quickly drives the minimum required tuning range beyond the FSR, which limits its practicality.\nTo keep the grid offset below \\qty4\\nano and mitigate the large tuning range demands of LtD, both laser global variation (\u03c3l\u2062G\u2062Vsubscript\ud835\udf0e\ud835\udc59\ud835\udc3a\ud835\udc49\\sigma_{lGV}italic_\u03c3 start_POSTSUBSCRIPT italic_l italic_G italic_V end_POSTSUBSCRIPT) and microring global variation (\u03c3r\u2062G\u2062Vsubscript\ud835\udf0e\ud835\udc5f\ud835\udc3a\ud835\udc49\\sigma_{rGV}italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_G italic_V end_POSTSUBSCRIPT) must be controlled and minimized.\nHowever, microring global variation alone can easily surpass \\qty4\\nano, primarily due to substrate thickness variations (see Section\u00a0II-C), which may create difficulties in adopting the LtD policy.\nAdditionally, while the result is somewhat sensitive to the blue-shift fabrication bias (\u03bbr\u2062Bsubscript\ud835\udf06\ud835\udc5f\ud835\udc35\\lambda_{rB}italic_\u03bb start_POSTSUBSCRIPT italic_r italic_B end_POSTSUBSCRIPT in Table\u00a0I) in microrings relative to laser wavelengths, this does not significantly alter our observation. \nIV-C Impact of Laser and Microring Variabilities\n Next, we examine the impact of device variation parameters on arbitration robustness.\nFig.\u00a07 shows the local sensitivity analysis of laser and microring variations on the minimum tuning range.\nFig.\u00a07(a) sweeps grid offset (\u03c3g\u2062Osubscript\ud835\udf0e\ud835\udc54\ud835\udc42\\sigma_{gO}italic_\u03c3 start_POSTSUBSCRIPT italic_g italic_O end_POSTSUBSCRIPT) from \\qty0\\nano to \\qty1.12\\nano (the grid spacing), since for LtA and LtC arbiters, a grid offset larger than the grid spacing does not change the required tuning distance.\nThis is because barrel-shifting the target spectral ordering effectively re-centers the microring row by multiples of \u03bbg\u2062Ssubscript\ud835\udf06\ud835\udc54\ud835\udc46\\lambda_{gS}italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT.\nThis adjustment cancels out that amount from the required tuning distance and compensates for grid offsets larger than \u03bbg\u2062S=subscript\ud835\udf06\ud835\udc54\ud835\udc46absent\\lambda_{gS}=italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT = \\qty1.12\\nano.\nFig.\u00a07(b) sweeps laser local variation (\u03c3l\u2062L\u2062Vsubscript\ud835\udf0e\ud835\udc59\ud835\udc3f\ud835\udc49\\sigma_{lLV}italic_\u03c3 start_POSTSUBSCRIPT italic_l italic_L italic_V end_POSTSUBSCRIPT) from 1% to 45%, which nearly doubles the range of our default value, 25%.\nFig.\u00a07(c) sweeps microring tuning range variation (\u03c3T\u2062Rsubscript\ud835\udf0e\ud835\udc47\ud835\udc45\\sigma_{TR}italic_\u03c3 start_POSTSUBSCRIPT italic_T italic_R end_POSTSUBSCRIPT) from 0% to 20%, twice the nominal circuit variation range 10%.\nFig.\u00a07(d) sweeps microring FSR variation (\u03c3F\u2062S\u2062Rsubscript\ud835\udf0e\ud835\udc39\ud835\udc46\ud835\udc45\\sigma_{FSR}italic_\u03c3 start_POSTSUBSCRIPT italic_F italic_S italic_R end_POSTSUBSCRIPT) from 0% to 5%, with our nominal value chosen as 1%. Overall, the analysis shows that the primary factors determining the required microring tuning range are microring local resonance variation and arbitration policy, while other laser and microring variations, such as tuning range variation and FSR variation, play secondary or tertiary roles.\nMoreover, the trend remains consistent between pre-fabrication and post-arbitration spectral orderings, as discussed in Section\u00a0IV-A.\nIn detail, both LtA and LtC show sensitivity to laser local variation, while LtC is additionally sensitive to microring tuning range variation and FSR variation.\nDenoting the minimum tuning range as \u03bb\u00afm\u2062i\u2062n\u2062T\u2062Rsubscript\u00af\ud835\udf06\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc47\ud835\udc45\\bar{\\lambda}_{minTR}over\u00af start_ARG italic_\u03bb end_ARG start_POSTSUBSCRIPT italic_m italic_i italic_n italic_T italic_R end_POSTSUBSCRIPT, we deduce that \u2202(\u03bb\u00afm\u2062i\u2062n\u2062T\u2062R)/\u2202(\u03c3l\u2062L\u2062V)\u223c0.56\u2062nm/25%similar-tosubscript\u00af\ud835\udf06\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc47\ud835\udc45subscript\ud835\udf0e\ud835\udc59\ud835\udc3f\ud835\udc490.56nmpercent25\\partial\\left(\\bar{\\lambda}_{minTR}\\right)/\\partial\\left(\\sigma_{lLV}\\right)%\n\\sim 0.56\\text{nm}/25\\%\u2202 ( over\u00af start_ARG italic_\u03bb end_ARG start_POSTSUBSCRIPT italic_m italic_i italic_n italic_T italic_R end_POSTSUBSCRIPT ) / \u2202 ( italic_\u03c3 start_POSTSUBSCRIPT italic_l italic_L italic_V end_POSTSUBSCRIPT ) \u223c 0.56 nm / 25 % for both LtC and LtA.\nThis can be explained by the following example:\nSuppose a set of laser wavelengths {\u03bblaser,0,\u03bblaser,1,\u03bblaser,2,\u03bblaser,3}subscript\ud835\udf06laser0subscript\ud835\udf06laser1subscript\ud835\udf06laser2subscript\ud835\udf06laser3\\{\\lambda_{\\text{laser},0},\\lambda_{\\text{laser},1},\\lambda_{\\text{laser},2},%\n\\lambda_{\\text{laser},3}\\}{ italic_\u03bb start_POSTSUBSCRIPT laser , 0 end_POSTSUBSCRIPT , italic_\u03bb start_POSTSUBSCRIPT laser , 1 end_POSTSUBSCRIPT , italic_\u03bb start_POSTSUBSCRIPT laser , 2 end_POSTSUBSCRIPT , italic_\u03bb start_POSTSUBSCRIPT laser , 3 end_POSTSUBSCRIPT } has 25% local wavelength variations i.e., \u00b1plus-or-minus\\pm\u00b1\\qty0.28\\nano.\nIn one possible scenario, {\u03bblaser,0,\u03bblaser,1,\u03bblaser,2}subscript\ud835\udf06laser0subscript\ud835\udf06laser1subscript\ud835\udf06laser2\\{\\lambda_{\\text{laser},0},\\lambda_{\\text{laser},1},\\lambda_{\\text{laser},2}\\}{ italic_\u03bb start_POSTSUBSCRIPT laser , 0 end_POSTSUBSCRIPT , italic_\u03bb start_POSTSUBSCRIPT laser , 1 end_POSTSUBSCRIPT , italic_\u03bb start_POSTSUBSCRIPT laser , 2 end_POSTSUBSCRIPT } is shifted by \u2212--\\qty0.28\\nano and \u03bblaser,3subscript\ud835\udf06laser3\\lambda_{\\text{laser},3}italic_\u03bb start_POSTSUBSCRIPT laser , 3 end_POSTSUBSCRIPT is shifted by +++\\qty0.28\\nano.\nA microring assigned to \u03bblaser,3subscript\ud835\udf06laser3\\lambda_{\\text{laser},3}italic_\u03bb start_POSTSUBSCRIPT laser , 3 end_POSTSUBSCRIPT must shift by an additional \\qty0.56\\nano, which roughly matches our observation.\nWhile the sensitivities to \u03c3T\u2062Rsubscript\ud835\udf0e\ud835\udc47\ud835\udc45\\sigma_{TR}italic_\u03c3 start_POSTSUBSCRIPT italic_T italic_R end_POSTSUBSCRIPT and \u03c3F\u2062S\u2062Rsubscript\ud835\udf0e\ud835\udc39\ud835\udc46\ud835\udc45\\sigma_{FSR}italic_\u03c3 start_POSTSUBSCRIPT italic_F italic_S italic_R end_POSTSUBSCRIPT in LtC arbitration can be explained similarly, it is interesting to note that in other cases, the impact of these variations is significantly reduced.\nThis is likely due to the looser enforcement of spectral ordering in LtA, which allows the system to \u201cabsorb\u201d or tolerate larger variations without requiring substantial adjustments in the tuning range. \nIV-D FSR Design Guideline\n Lastly, we examine the design space of microring FSR (\u03bb\u00afF\u2062S\u2062Rsubscript\u00af\ud835\udf06\ud835\udc39\ud835\udc46\ud835\udc45\\bar{\\lambda}_{FSR}over\u00af start_ARG italic_\u03bb end_ARG start_POSTSUBSCRIPT italic_F italic_S italic_R end_POSTSUBSCRIPT), as shown in Fig.\u00a08.\nWhile the nominal design point is \\qty8.96\\nano, which is Nc\u2062h\u00d7\u03bbg\u2062Ssubscript\ud835\udc41\ud835\udc50\u210esubscript\ud835\udf06\ud835\udc54\ud835\udc46N_{ch}\\times\\lambda_{gS}italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT \u00d7 italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT, \u03bb\u00afF\u2062S\u2062Rsubscript\u00af\ud835\udf06\ud835\udc39\ud835\udc46\ud835\udc45\\bar{\\lambda}_{FSR}over\u00af start_ARG italic_\u03bb end_ARG start_POSTSUBSCRIPT italic_F italic_S italic_R end_POSTSUBSCRIPT is swept from \\qty6.72\\nano (6\u00d7\u03bbg\u2062S6subscript\ud835\udf06\ud835\udc54\ud835\udc466\\times\\lambda_{gS}6 \u00d7 italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT) to \\qty15.68\\nano (14\u00d7\u03bbg\u2062S14subscript\ud835\udf06\ud835\udc54\ud835\udc4614\\times\\lambda_{gS}14 \u00d7 italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT) to investigate both under-design and over-design cases.\nFor both LtA and LtC policies, we observe a tolerance range of approximately \u00b1plus-or-minus\\pm\u00b1\\qty0.5\\nano around the nominal FSR, within which the increase in the minimum tuning range remains within \\qty0.5\\nano.\nExceeding this tolerance range in the under-designed case causes a sharp increase in the required tuning range due to resonance aliasing, where \u03bb\u00afF\u2062S\u2062R\u2264Nc\u2062h\u00d7\u03bbg\u2062Ssubscript\u00af\ud835\udf06\ud835\udc39\ud835\udc46\ud835\udc45subscript\ud835\udc41\ud835\udc50\u210esubscript\ud835\udf06\ud835\udc54\ud835\udc46\\bar{\\lambda}_{FSR}\\leq N_{ch}\\times\\lambda_{gS}over\u00af start_ARG italic_\u03bb end_ARG start_POSTSUBSCRIPT italic_F italic_S italic_R end_POSTSUBSCRIPT \u2264 italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT \u00d7 italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT may cause a single microring to align with multiple laser wavelengths, particularly under a 25% local laser variation666James et al.\u00a0 [43] proposed a multi-FSR scheme that allows \u03bb\u00afF\u2062S\u2062R\u2264Nc\u2062h\u00d7\u03bbg\u2062Ssubscript\u00af\ud835\udf06\ud835\udc39\ud835\udc46\ud835\udc45subscript\ud835\udc41\ud835\udc50\u210esubscript\ud835\udf06\ud835\udc54\ud835\udc46\\bar{\\lambda}_{FSR}\\leq N_{ch}\\times\\lambda_{gS}over\u00af start_ARG italic_\u03bb end_ARG start_POSTSUBSCRIPT italic_F italic_S italic_R end_POSTSUBSCRIPT \u2264 italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT \u00d7 italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT by configuring the FSR and channel spacing to be coprime and assuming the use of a low-variation comb source, which is beyond the scope of our analysis..\nOver-designing the FSR results in a more gradual increase in AFP.\nThis occurs because as the FSR widens, the distance between the last microring grid and the first microring grid of the next FSR increases, which in turn increases the required tuning distances for the LtA and LtC policies.\nTherefore, to minimize AFP, it is preferable to design the FSR close to Nc\u2062h\u00d7\u03bbg\u2062Ssubscript\ud835\udc41\ud835\udc50\u210esubscript\ud835\udf06\ud835\udc54\ud835\udc46N_{ch}\\times\\lambda_{gS}italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT \u00d7 italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT, with small deviations being acceptable. \nV Wavelength Arbitration Algorithm\n In this section, we explain an efficient and robust wavelength arbitration algorithm.\nWe focus on the LtC policy implementation, while extensions to other policies are discussed in Section\u00a0V-E.\nOur work extends the autonomous algorithm explained in Hattink et al.\u00a0[17] by generalizing concepts such as target spectral ordering, conflict resolution, and maximum matching. \nV-A Overview\n The algorithm consists of two phases: record and matching, as illustrated in Fig.\u00a09(a).\nThe record phase tabulates the wavelength-domain relationship between the microring wavelength search outcomes of microring pairs.\nIt is followed by the matching phase, where each microring is assigned to different laser wavelengths according to the identified relationships and the specified microring spectral ordering.\nFor convenience, we refer to the relationship between microring wavelength search outcomes as the microring relation, and the process of identifying this relationship as the relation search.\nFig.\u00a09(b) summarizes the structure of the scheme. During the record phase, Nc\u2062hsubscript\ud835\udc41\ud835\udc50\u210eN_{ch}italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT (4 in Fig.\u00a09(b)) relation searches are conducted to discover the microring relations between consecutive microring pairs.\nThese pairs are derived from the target spectral ordering sisubscript\ud835\udc60\ud835\udc56s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.\nIn Fig.\u00a09(b), given si=(0,1,2,3)subscript\ud835\udc60\ud835\udc560123s_{i}=\\left(0,1,2,3\\right)italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( 0 , 1 , 2 , 3 ), the target pairs are (0,1)01\\left(0,1\\right)( 0 , 1 ), (1,2)12\\left(1,2\\right)( 1 , 2 ), (2,3)23\\left(2,3\\right)( 2 , 3 ), and (3,0)30\\left(3,0\\right)( 3 , 0 ).\nIf si=(0,2,1,3)subscript\ud835\udc60\ud835\udc560213s_{i}=\\left(0,2,1,3\\right)italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( 0 , 2 , 1 , 3 ), the pairs are (0,2)02\\left(0,2\\right)( 0 , 2 ), (2,1)21\\left(2,1\\right)( 2 , 1 ), (1,3)13\\left(1,3\\right)( 1 , 3 ) and (3,0)30\\left(3,0\\right)( 3 , 0 ).\nWe note that relation searches are not run across all Nc\u2062h\u00d7(Nc\u2062h\u22121)/2subscript\ud835\udc41\ud835\udc50\u210esubscript\ud835\udc41\ud835\udc50\u210e12N_{ch}\\times(N_{ch}-1)/2italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT \u00d7 ( italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT - 1 ) / 2 microring pairs to construct a comprehensive \u201crelation map\u201d of wavelength search outcomes.\nInstead, we run relation searches on Nc\u2062hsubscript\ud835\udc41\ud835\udc50\u210eN_{ch}italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT microring pairs.\nThis is because the wavelength allocation for the LtC policy is semi-deterministic; if consecutive microring pairs from sisubscript\ud835\udc60\ud835\udc56s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are allocated to neighboring wavelengths, then the wavelengths are assigned as desired.\nFor example, it is sufficient for R(i+1)\ud835\udc561\\left(i+1\\right)( italic_i + 1 ) to be assigned to the next laser wavelength of that assigned to Ri\ud835\udc56iitalic_i when si=(0,1,2,3)subscript\ud835\udc60\ud835\udc560123s_{i}=\\left(0,1,2,3\\right)italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( 0 , 1 , 2 , 3 ).\nThis property allows us to simplify the global arbitration problem into a set of arbitrations between consecutive pairs while guaranteeing robust operation. Leveraging this simplification, the matching phase performs a non-iterative microring-to-laser matching by building the global lock allocation table.\nThe table is constructed by rearranging the microring search tables based on the relations between consecutive pairs established during the record phase.\nFrom this table, the final wavelength allocation is derived in a single pass, with each microring assigned its lock target from its respective search table.\nThe full procedure is detailed in Section\u00a0V-C. Before we delve into each phase, we define the nomenclature that are used in the rest of the sections.\nThe search table in Fig.\u00a09(b) is the recorded tuner codes of the microring corresponding to peak intra-cavity optical power during wavelength search.\nWe denote the search table of the i\ud835\udc56iitalic_ith microring as ST(i)\ud835\udc56\\left(i\\right)( italic_i ).\nA microring relation connects the two search tables and is mathematically formulated as the relation index, representing the offset between the search table entry locations according to the wavelength correspondence.\nWe denote the relation index between the i\ud835\udc56iitalic_ith and the j\ud835\udc57jitalic_jth microring as R\u2062I\u2062(i,j)\ud835\udc45\ud835\udc3c\ud835\udc56\ud835\udc57RI\\left(i,j\\right)italic_R italic_I ( italic_i , italic_j ).\nWe use the relation index to align the search tables by wavelength alignment, forming the lock allocation table for efficient arbitration.\nFig.\u00a09(c) depicts the arbitration success, while Fig.\u00a09(d-f) depicts different failures cases: two microrings are assigned to the same wavelengths (Dupl-Lock in Fig.\u00a09(d)); one or more microrings are not assigned to any wavelength (Zero-Lock in Fig.\u00a09(e)); lane-order mismatch or microring spectral ordering requirement is not met (Lane-Order Error in Fig.\u00a09(f)). \nV-B Record Phase\n A core operation of the record phase is the relation search, which is illustrated in Fig.\u00a010.\nThe procedure is described in Hattink et al.\u00a0[17] which utilizes the natural precedence order in light receive between the microrings.\nIn a typical microring-based DWDM architecture, when light propagates to the downstream microrings, a microring physically closer to the light input has priority in light receive over farther rings.\nIn other words, physical prior maps to the precedence order in capturing the wavelengths.\nUsing this property, the relation search assigns the aggressor and victim roles within the microring pair according to their physical order.\nBoth microrings first undergo the wavelength search, recording their initial search tables.\nThen, the aggressor microring \u201cinjects\u201d aggression into the victim microring by wavelength lock to a specific target from ST(i)\ud835\udc56\\left(i\\right)( italic_i ).\nIf the target is set appropriately, the victim microring will find one of the entries in its search table masked, indicating that the target chosen from ST(i)\ud835\udc56\\left(i\\right)( italic_i ) and the masked entry in ST(j)\ud835\udc57\\left(j\\right)( italic_j ) correspond to the same wavelength.\nFurthermore, other relations between the search table pairs can be inferred.\nFor example, in Fig.\u00a010, if 3rd entry of ST(i)\ud835\udc56\\left(i\\right)( italic_i ) and 2nd entry of ST(j)\ud835\udc57\\left(j\\right)( italic_j ) correspond to the same wavelength, then 2nd entry of ST(i)\ud835\udc56\\left(i\\right)( italic_i ) and 1st entry of ST(j)\ud835\udc57\\left(j\\right)( italic_j ) also correspond to the same wavelength, given that the microring tuning is monotonic in wavelength-domain.\nWe call these identified and inferred relations a relation map between the microrings, and define Relation Index as the difference in indices between the masked entry and the aggressor target entry from each search table.\nNote that the aggressor target wavelength should be within the victim microring tuning range, which is not trivial in a wavelength-oblivious scenario. To achieve a proper aggressor injection, it is most straightforward to loop through the aggressor search table entries and repeat the relation search.\nHowever, for efficient arbitration, it is desirable to find a more systematic way that determines the aggressor lock target, as shown in Fig.\u00a011(a).\nWithout loss of generality, we assume i<j\ud835\udc56\ud835\udc57i<jitalic_i < italic_j.\nFrom our notation in Section\u00a0II-C, Ri\ud835\udc56iitalic_i physically precedes Rj\ud835\udc57jitalic_j, making Ri\ud835\udc56iitalic_i the aggressor and Rj\ud835\udc57jitalic_j the victim.\nIn Fig.\u00a011(a), which depicts the case of \u03bbring,i<\u03bbring,jsubscript\ud835\udf06ring\ud835\udc56subscript\ud835\udf06ring\ud835\udc57\\lambda_{\\text{ring},i}<\\lambda_{\\text{ring},j}italic_\u03bb start_POSTSUBSCRIPT ring , italic_i end_POSTSUBSCRIPT < italic_\u03bb start_POSTSUBSCRIPT ring , italic_j end_POSTSUBSCRIPT, we notice that setting the aggressor lock target to the last entry (Lock-to-Last) achieves the aggressor injection when the microring relation exists.\nWhen \u03bbring,i>\u03bbring,jsubscript\ud835\udf06ring\ud835\udc56subscript\ud835\udf06ring\ud835\udc57\\lambda_{\\text{ring},i}>\\lambda_{\\text{ring},j}italic_\u03bb start_POSTSUBSCRIPT ring , italic_i end_POSTSUBSCRIPT > italic_\u03bb start_POSTSUBSCRIPT ring , italic_j end_POSTSUBSCRIPT, as shown in Fig.\u00a011(b), setting the aggressor target to the first entry (Lock-to-First) achieves the aggressor injection.\nWhile the pre-fabrication spectral ordering sets the initial microring ordering, process variations can cause the post-fabrication spectral orderings to be \u201cflipped\u201d.\nThus, we run both aggressions and combine the results for R\u2062I\u2062(i,j)\ud835\udc45\ud835\udc3c\ud835\udc56\ud835\udc57RI\\left(i,j\\right)italic_R italic_I ( italic_i , italic_j ). However, in case of large microring variations, neither aggressor Lock-to-First nor Lock-to-Last achieves the aggressor injection.\nFig.\u00a011(c) illustrates the case of large tuning range variation, and Fig.\u00a011(d) shows the case of large FSR variation, where both the first and last targets are outside the victim microring\u2019s tuning range.\nIn these cases, we re-run the relation search with the aggressor Lock-to-Second and find the relation.\nThis simple scheme, which we call Variation-Tolerant Relation Search (VT-RS) trades off robustness with additional overhead.\nWe further study this in Section\u00a0V-D. \nV-C Matching Phase\n We describe our non-iterative matching algorithm for the LtC arbitration, which we call the Single-Step Matching (SSM) algorithm.\nA key data structure is the lock allocation table, which provides a global view of the wavelength lock candidates for microrings in the wavelength domain.\nAs illustrated in Fig.\u00a012(a), the table is constructed by organizing the search tables into a two-dimensional structure, where the horizontal axis represents different microrings and the vertical axis represents search table entries, or lock candidates.\nThe relation index is used to vertically offset the search tables so that entries at the same vertical position correspond to the same wavelength, while the tables are arranged horizontally based on the target spectral ordering (sisubscript\ud835\udc60\ud835\udc56s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT).\nThe algorithm then assigns lock targets to each microring through an ordered assignment, achieving a non-iterative matching that follows the specified target ordering.\nWhile this approach works smoothly when all relation searches return a valid relation index, special attention is needed when any search fails to find a relation index, which we denote as R\u2062I=\u03d5\ud835\udc45\ud835\udc3citalic-\u03d5RI=\\phiitalic_R italic_I = italic_\u03d5.\nIn such cases, we assume that the microring resonance wavelengths are \u201cgrouped\u201d and form the sub-allocation tables according to the occurrences of R\u2062I=\u03d5\ud835\udc45\ud835\udc3citalic-\u03d5RI=\\phiitalic_R italic_I = italic_\u03d5. Fig.\u00a012(b) illustrates the formation of sub-allocation tables and lock target assignment when certain relation searches fail to find a relation index.\nIn Fig.\u00a012(b), with si=(0,1,2,3)subscript\ud835\udc60\ud835\udc560123s_{i}=\\left(0,1,2,3\\right)italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( 0 , 1 , 2 , 3 ), relation searches are conducted between the pairs (R0, R1), (R1, R2), (R2, R3) and (R3, R0).\nThe relation searches between (R0, R1) and (R2, R3) returns empty indices (R\u2062I\u2062(0,1)=\u03d5\ud835\udc45\ud835\udc3c01italic-\u03d5RI\\left(0,1\\right)=\\phiitalic_R italic_I ( 0 , 1 ) = italic_\u03d5 and R\u2062I\u2062(2,3)=\u03d5\ud835\udc45\ud835\udc3c23italic-\u03d5RI\\left(2,3\\right)=\\phiitalic_R italic_I ( 2 , 3 ) = italic_\u03d5).\nBased on these R\u2062I=\u03d5\ud835\udc45\ud835\udc3citalic-\u03d5RI=\\phiitalic_R italic_I = italic_\u03d5 occurrences, we interpret (R1, R2) and (R3, R0) as clustered and form sub-allocation tables accordingly.\nWe then assign the lock targets of R1 and R0\u2014the aggressor/victim microrings in the relation search that yields R\u2062I=\u03d5\ud835\udc45\ud835\udc3citalic-\u03d5RI=\\phiitalic_R italic_I = italic_\u03d5\u2014to the first and last entries of their respective microring search tables.\nThis strategy can replicate the outcome of the ideal wavelength-aware LtC arbitration, and we prove this by contradiction:\nIf the wavelength-aware LtC allocation is feasible but this strategy fails, it implies the existence of an alternative valid assignment.\nWithout loss of generality, assume the valid assignment for R1 is an entry other than the first entry in its search table, while R0 is assigned the last entry of ST0.\nR0 and R1 should be assigned neighboring wavelengths (si=(0,1,2,3)subscript\ud835\udc60\ud835\udc560123s_{i}=\\left(0,1,2,3\\right)italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( 0 , 1 , 2 , 3 )); However, the first entry of ST1 would fall between those wavelengths, contradicting the assumption of a valid assignment. Fig.\u00a013 summarizes the proposed Single-Step Matching algorithm.\nThe algorithm is applied on a case-by-case basis, depending on the number of R\u2062I=\u03d5\ud835\udc45\ud835\udc3citalic-\u03d5RI=\\phiitalic_R italic_I = italic_\u03d5 occurrences.\nIf R\u2062I=\u03d5\ud835\udc45\ud835\udc3citalic-\u03d5RI=\\phiitalic_R italic_I = italic_\u03d5 does not occur, a single lock allocation table is created, and the assignment problem becomes a diagonal matching process.\nWhen R\u2062I=\u03d5\ud835\udc45\ud835\udc3citalic-\u03d5RI=\\phiitalic_R italic_I = italic_\u03d5 occurs more than once, sub-allocation tables are formed, separated by the microring pairs that corresponds to R\u2062I=\u03d5\ud835\udc45\ud835\udc3citalic-\u03d5RI=\\phiitalic_R italic_I = italic_\u03d5.\nThe first microrings in each sub-table are assigned the first entries from their search tables, while the last microrings are assigned the last entries.\nIn case where R\u2062I=\u03d5\ud835\udc45\ud835\udc3citalic-\u03d5RI=\\phiitalic_R italic_I = italic_\u03d5 occurs once, a single allocation table is formed, starting and ending with the microring pair corresponding to R\u2062I=\u03d5\ud835\udc45\ud835\udc3citalic-\u03d5RI=\\phiitalic_R italic_I = italic_\u03d5.\nFig.\u00a013 also compares the results of the Single-Step Matching algorithm with the sequential tuning scheme, where the first available wavelength is assigned to each microring sequentially.\nSequential tuning fails at the last microring for both Fig.\u00a013(b) and (c).\nThis failure occurs because the sequential assignment can cause earlier microrings to \u201csteal\u201d all available wavelengths, leaving none for the later microrings\u2014a problem that our proposed algorithm avoids through its tabulated approach. \nV-D Evaluation of Proposed Schemes\n We evaluate the proposed algorithms using Conditional Arbitration Failure Probability (CAFP), as described in Section\u00a0III.\nMeasured CAFP indicates how effectively the algorithm implements the LtC policy by evaluating its failure rate when the ideal LtC arbitration succeeds.\nThe algorithms are denoted as RS/SSM and VT-RS/SSM.\nRS/SSM represents the Relation Search for the record phase and Single-Step Matching for the matching phase, while VT-RS/SSM refers to the Variation-Tolerant Relation Search for the record phase and Single-Step Matching for the matching phase.\nThe baseline used is the sequential tuning scheme, which sequentially tunes the microrings and assigns them to the nearest wavelengths (Lock-to-Nearest) [6, 22, 23, 45].\nWe focus on two representative target microring spectral orderings (sisubscript\ud835\udc60\ud835\udc56s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT): Natural (N) and Permuted (P), as defined in Section\u00a0IV.\nThe baseline sequential tuning is extended to arbitrary spectral orderings as follows:\nFor Natural ordering (si=(0,1,2,3\u2062\u2026)subscript\ud835\udc60\ud835\udc560123\u2026s_{i}=\\left(0,1,2,3\\ldots\\right)italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( 0 , 1 , 2 , 3 \u2026 )), R0 is tuned first, followed by R1, and so on.\nFor Permuted ordering (si=(0,4,1,5\u2062\u2026)subscript\ud835\udc60\ud835\udc560415\u2026s_{i}=\\left(0,4,1,5\\ldots\\right)italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( 0 , 4 , 1 , 5 \u2026 )), R0 is tuned first, followed by R2 (the microring corresponding to the second spectral order), and so on.\nAs in Section\u00a0IV, we assume the pre-fabrication spectral ordering (risubscript\ud835\udc5f\ud835\udc56r_{i}italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) matches the post-arbitration target ordering (ri=sisubscript\ud835\udc5f\ud835\udc56subscript\ud835\udc60\ud835\udc56r_{i}=s_{i}italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT).\nThe default model parameters in Table\u00a0I are used for the experiments, with 10,000 arbitration tests conducted using 100 multi-wavelength laser and microring row samples. Fig.\u00a014 shows the CAFP across different local resonance variations (\u03c3l\u2062L\u2062Vsubscript\ud835\udf0e\ud835\udc59\ud835\udc3f\ud835\udc49\\sigma_{lLV}italic_\u03c3 start_POSTSUBSCRIPT italic_l italic_L italic_V end_POSTSUBSCRIPT) and microring tuning ranges (\u03bb\u00afT\u2062Rsubscript\u00af\ud835\udf06\ud835\udc47\ud835\udc45\\bar{\\lambda}_{TR}over\u00af start_ARG italic_\u03bb end_ARG start_POSTSUBSCRIPT italic_T italic_R end_POSTSUBSCRIPT), with the choice of sweep ranges explained in Section\u00a0II-C.\nWe observe that the proposed schemes consistently outperform the baseline in all cases, with VT-RS/SSM closely approximating the ideal LtC arbitration.\nThe errors observed in RS/SSM at tuning ranges around \\qty8\\nano likely stem from a tuning range variation of \\qty0.8\\nano (10% of \\qty8\\nano), which is significant enough to introduce errors during the relation search, as discussed in Section\u00a0V-B.\nThese errors can be systematically mitigated by reducing the tuning range below \\qty8\\nano or using VT-RS instead.\nFurthermore, as demonstrated in Fig.\u00a014(b), (d), and (f), the scheme is applicable to any target spectral ordering, assuming si=risubscript\ud835\udc60\ud835\udc56subscript\ud835\udc5f\ud835\udc56s_{i}=r_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. The plotted CAFP metric includes lane-order mismatch errors (Fig.\u00a09(f)) due to the assumed LtC arbitration, whereas the sequential tuning scheme may not necessarily target a specific spectral ordering.\nFor fair comparison, in Fig.\u00a015, we break down CAFP of sequential tuning into two categories: zero/duplicate lock errors (Fig.\u00a09(d) and (e)) and lane-order mismatch errors (Fig.\u00a09(f)).\nWhen the microring tuning range is larger than approximately the FSR (\\qty8.96\\nano), lane-order mismatch errors dominate the CAFP.\nOn the other hand, when the tuning range is smaller than the FSR, the sequential tuning scheme shows significant zero/duplicate lock errors, even with ideal laser variations and microring FSR/tuning range variations.\nThis is likely due to microring local resonance variations larger than \\qty0.28\\nano, which is 1/4 of the channel spacing, still contributing significantly to lock errors in the sequential tuning scheme.\nEmpirical observations suggest that many sequential tuning failures are due to missing the last few microrings.\nFuture work will explore this issue further. Finally, Fig.\u00a016 compares the CAFP of the proposed RS/SSM and VT-RS/SSM under high microring FSR and tuning range variations.\nThe plot suggests an interesting trend, with bands of CAFP forming around tuning ranges of \\qty3\\nano and \\qty8\\nano.\nThese bands can be linked to the relation search failure cases illustrated in Fig.\u00a011(c) and (d).\nSpecifically, we associate the band around \\qty3\\nano with high FSR variation, while the band around \\qty8\\nano would correspond to both high tuning range and FSR variations.\nFig.\u00a016(c) and (d) show similar bands, implying a consistent trend across different target spectral orderings. While an FSR variation of 5% and a tuning range variation of 20% may be unrealistically high, Fig.\u00a016(b) and (d) demonstrate that VT-RS/SSM still performs well under these harsh conditions.\nHowever, this performance comes with additional overhead, including extra steps such as aggressor Lock-to-Second and victim wavelength searches in VT-RS/SSM.\nTherefore, selecting an algorithm should be done holistically, taking into account factors such as microring variations, failure tolerance levels (yield impact), and initialization costs. \nV-E Discussion\n The design and evaluation of the wavelength arbitration algorithms has been discussed.\nThese algorithms complement previous studies on physical implementations, such as wavelength search [11] and lock [14, 15], innovative architectures like thermal crosstalk-aware control [46], and efficient DWDM microring wavelength lock architectures [18, 19, 20].\nThe schemes are applicable to both transmitters (TX) and receivers (RX) as the arbitration pertains to wavelength allocation step.\nFor instance, a DWDM TX would then require a maximum Optical Modulation Amplitude wavelength lock similar to those in Sun et al.\u00a0[11] and Grimaldi et al.\u00a0[15], after allocation. We outline several potential directions for future work.\nFirst, this study does not fully explore the implementations of the LtD and LtA policies.\nFor the LtD policy, a pilot signal is likely necessary since the relation search only reveals relative wavelength positions.\nWithout it, distinguishing between wavelengths would be impractical.\nFor the LtA policy, constructing a full relation map for allocation to any spectral ordering would require Nc\u2062h\u00d7(Nc\u2062h\u22121)/2subscript\ud835\udc41\ud835\udc50\u210esubscript\ud835\udc41\ud835\udc50\u210e12N_{ch}\\times(N_{ch}-1)/2italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT \u00d7 ( italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT - 1 ) / 2 relation searches between all microring pairs.\nAdditionally, typical maximum matching algorithms, which involve backtracking, would significantly diverge from the proposed single-step method.\nSecond, scaling the microring FSR beyond 32/64-channel configurations may not be feasible if the FSR is set to Nc\u2062h\u00d7\u03bbg\u2062Ssubscript\ud835\udc41\ud835\udc50\u210esubscript\ud835\udf06\ud835\udc54\ud835\udc46N_{ch}\\times\\lambda_{gS}italic_N start_POSTSUBSCRIPT italic_c italic_h end_POSTSUBSCRIPT \u00d7 italic_\u03bb start_POSTSUBSCRIPT italic_g italic_S end_POSTSUBSCRIPT.\nIn such cases, the analysis and the algorithms should be adapted to the multi-FSR scheme proposed by James et al.\u00a0[43].\nLastly, future research could extend the current framework to account for transceiver-level interactions [24, 25, 26], which would provide a more comprehensive evaluation of system performance and refine the algorithms to ensure robustness across both TX and RX. \nVI Conclusion\n In this paper, we addressed the critical challenge of aligning microring resonances with laser wavelengths in microring-based DWDM transceivers, a process we termed wavelength arbitration.\nWe introduced an ideal arbitration model that decouples the analysis of policy and algorithm, providing a framework for evaluating the efficacy of different arbitration strategies.\nOur hierarchical formulation allows for the independent assessment of arbitration policies from algorithm implementations, simplifying the analysis of both layers. To effectively define the DWDM interface, we introduced three arbitration policies\u2014LtA, LtC, and LtD\u2014categorized by their spectral ordering enforcement levels and evaluated their tradeoffs in the design space.\nFurthermore, we developed a wavelength-oblivious algorithm for LtC policy implementation, including the RS/SSM and VT-RS/SSM.\nThrough extensive simulations, we demonstrated that our schemes closely approximate the ideal model, significantly outperforming the traditional sequential tuning approach. Our findings underscore the importance of adopting a holistic approach to wavelength arbitration, where policy-driven arbitration, alongside algorithmic implementations, enables scalable and reliable wavelength allocation.\nBy evaluating arbitration strategies through AFP and CAFP metrics, we provide a comprehensive view of how these strategies perform across varying device-level conditions and system configurations.\nThis work establishes a foundation for advanced arbitration techniques, demonstrating the need to integrate both high-level policy design and practical, wavelength-oblivious algorithm implementation.\nMoving forward, further research into variations such as LtA policy implementations will be crucial for enhancing the scalability and robustness of microring-based DWDM transceivers. Acknowledgments This work was supported in part by task 3132.015 of the Center for Ubiquitious Connectivity (CUbiC), sponsored by Semiconductor Research Corporation (SRC) and Defence Advanced Research Projects Agency (DARPA) under the JUMP 2.0 program.\nSunjin Choi would like to acknowledge the support from Korea Foundation for Advanced Studies (KFAS) and Qualcomm Innovation Fellowship (QIF).\nThe authors would like to thank Shenggao Li from TSMC and Pavan Bhargava from Ayar Labs for their valuable discussions and feedback.\nThe authors would like to give special thanks to Yue Dai for her valuable assistance in writing the manuscript.\nThe authors also acknowledge the students, staff, faculty and sponsors of the Berkeley Wireless Research Center.\n References"}
{"text": "../imgs/ Swift: A Multi-FPGA Framework for Scaling Up Accelerated Graph Analytics Graph analytics are vital in fields such as social networks, biomedical research, and graph neural networks (GNNs). However, traditional CPUs and GPUs struggle with the memory bottlenecks caused by large graph datasets and their fine-grained memory accesses. While specialized graph accelerators address these challenges, they often support only moderate-sized graphs (under 500 million edges). Our paper proposes Swift, a novel scale-up graph accelerator framework that processes large graphs by leveraging the flexibility of FPGA custom datapath and memory resources, and optimizes utilization of high-bandwidth 3D memory (HBM). Swift supports up to 8 FPGAs in a node. Swift introduces a decoupled, asynchronous model based on the Gather-Apply-Scatter (GAS) scheme. It subgraphs across FPGAs, and each subgraph into intervals based on source vertex IDs. Processing on these intervals is decoupled and executed asynchronously, instead of bulk-synchonous operation, where throughput is limited by the slowest task. This enables simultaneous processing within each multi-FPGA node and optimizes the utilization of communication (PCIe), off-chip (HBM), and on-chip BRAM/URAM resources. Swift demonstrates significant performance improvements compared to prior scalable FPGA-based frameworks, performing 12.8 times better than the ForeGraph. Performance against Gunrock on NVIDIA A40 GPUs is mixed, because NVlink gives the GPU system a nearly 5X bandwidth advantage, but the FPGA system nevertheless achieves 2.6x greater energy efficiency. \nI Introduction\n The growth of graph data in fields such as social network analysis, biomedical research, and graph neural networks (GNNs)\u00a0[1, 2, 3, 4, 5] has created a greater need for efficient and scalable graph analysis\u00a0[6, 7, 8, 9, 10]. However, existing hardware platforms face limitations in handling large-scale graphs. CPUs[11, 7, 12, 13, 14, 15] and GPUs\u00a0[16, 17, 18, 19]\noften result in fine-grained random memory accesses\u00a0[20, 21, 22, 23, 24, 25], which typically do not use memory bandwidth efficiently, limiting processing throughput. The emergence of modern FPGAs, with High Bandwidth Memory (HBM) technology (up to\u00a0460\u00a0GB/stimes460dividegigabytesecond460\\text{\\,}\\mathrm{GB}\\text{/}\\mathrm{s}start_ARG 460 end_ARG start_ARG times end_ARG start_ARG start_ARG roman_GB end_ARG start_ARG divide end_ARG start_ARG roman_s end_ARG end_ARG) and custom parallel pipelines for data processing, presents an alternative to address the challenges associated with accelerating graph processing\u00a0[26, 27, 28, 29, 30, 31, 32, 33, 34]. These state-of-the-art FPGAs offer significant parallelism compared to CPUs and GPUs, while maintaining a more efficient power profile. The inclusion of HBM allows graph data to be distributed across HBM channels and processed by specialized pipelines that achieve better memory access patterns, leveraging HBM\u2019s high memory bandwidth and enhancing parallelism. This feature makes HBM-enabled FPGAs a promising solution for graph processing. From single to multi-FPGA graph processing: Prior research on single FPGA graph accelerators is comprehensive but typically supports graphs with fewer than 500 million edges\u00a0[29, 34, 32]. To improve scalability, multi-FPGA methods often use a single FPGA solution replicated across multiple machines or rely on inter-FPGA interconnects\u00a0[41, 40, 36, 39, 37], which can perform poorly in sharing fine-grained graph data. Such approaches can lead to inefficient processing, higher power consumption, increased memory usage, and complex inter-machine communication. Furthermore, scaling up a single-FPGA design within a machine is limited by the PCIe communication bandwidth. Table\u00a0I presents a performance comparison of the widely-benchmarked PageRank algorithm among various single- and multi-FPGA graph processing frameworks. To enable fair comparison, metrics such as interconnect bandwidth and memory bandwidth of the FPGAs are also provided. As shown, multi-FPGA frameworks like Foregraph\u00a0[36] and FDGLib\u00a0[39] often have lower throughput than single-FPGA frameworks such as ThunderGP\u00a0[35, 29] across various graph algorithms and workloads. This is because PCIe-connected FPGAs (multi-FPGA frameworks) exhibit lower latency compared to network-connected FPGAs (single-FPGA frameworks) due to fast/high bandwidth on-chip HBM memory coupled with PCIe DMA, providing direct access to host memory without the need to navigate the network stack [42] [43] [44]. This challenges the presumed superiority of multi-FPGA setups. \u201cScaling up\u201d refers to adding more compute power (FPGAs) to a single machine via PCIe, while \u201cscaling out\u201d involves adding more machines with the same compute power via a specialized network\u2014in this context, adding more machines with a single FPGA. Scaling up single machine multi-FPGA graph processing and addressing communication overhead: A key factor in the performance gap of multi-FPGA frameworks is the communication overhead among FPGAs. Frameworks like Foregraph\u00a0[36] necessitate costly inter-FPGA communication for exchanging vertex property information at each graph iteration due to the memory-bound nature of graph processing. FPGA memory bandwidth, such as with High Bandwidth Memory (HBM) at up to\u00a0460\u00a0GB/stimes460dividegigabytesecond460\\text{\\,}\\mathrm{GB}\\text{/}\\mathrm{s}start_ARG 460 end_ARG start_ARG times end_ARG start_ARG start_ARG roman_GB end_ARG start_ARG divide end_ARG start_ARG roman_s end_ARG end_ARG, far exceeds that of inter-FPGA channels like PCIe at around\u00a017\u00a0GB/stimes17dividegigabytesecond17\\text{\\,}\\mathrm{GB}\\text{/}\\mathrm{s}start_ARG 17 end_ARG start_ARG times end_ARG start_ARG start_ARG roman_GB end_ARG start_ARG divide end_ARG start_ARG roman_s end_ARG end_ARG Swift: a decoupled Gather-Apply-Scatter graph execution model: In order to address the communication bottleneck problem, we decouple the main stages of the Gather-Apply-Scatter (GAS) graph processing scheme. This separation allows pipelining and overlapping the GAS compute and memory operations on the multi-FPGA system, enabling higher throughput while processing large graphs. With Swift, our decoupled graph processing model, four operations can run simultaneously: 1) processing edges at a given region (vertex intervals) for a given iteration;\n2) applying vertex updates to generate active frontiers at a second region.\n3) exporting active vertices (frontiers) to remote FPGAs in a third, different region.\n4) importing active frontiers from remote FPGAs in a fourth, different region.\nSwift\u2019s overlapping of GAS operations allows for higher utilization of available channels such as inter-FPGA communication channel (PCIe), intra-FPGA memory bandwidth (HBM), and on-chip BRAMs/URAMs. Furthermore, it improves throughput and conceals latency overheads. Swift adapts the open-source ACTS\u00a0[34] FPGA accelerator for single-FPGA graph processing, and introduces decoupled, asynchronous GAS processing to overcome inter-FPGA communication latency and bandwidth limits, outperforming previous FPGA graph accelerator solutions. \nII Background and Related Work\n \nII-A Gather-Apply-Scatter (GAS)\n The Gather-Apply-Scatter (GAS)\u00a0[45, 46, 6] model provides a high-level abstraction for various graph processing algorithms and is widely adopted by software-based\u00a0[47, 48, 9, 7, 49] and accelerator-based frameworks\u00a0[36, 35, 29, 50, 51, 52, 53]. The two main variants of the GAS model are the vertex-centric and edge-centric approaches. Swift adopts the edge-centric variant, which facilitates high throughput streaming memory accesses, leveraging HBM\u2019s high memory bandwidth. Algorithm 1 shows the pseudo code describing the Edge-centric Gather-Apply-Scatter graph processing model\u00a0[47, 15]. As shown, this model employs streaming partitions by logically splitting the graph into intervals by source vertex IDs during pre-processing. Next, an input of an unordered set of directed edges is streamed and processed in the Process_Edge stage where edge data, source and destination vertex properties generate an update value (res). Only intervals with active vertices are processed, avoiding redundant reads to all edges. Furthermore, intervals are based on source IDs, and source vertex properties are read once from DRAM per iteration. In Apply, these updates are applied to destination vertices to compute new vertex properties. These functions iterate until a convergence criterion is reached. \nII-B ACTS: Near-Memory FPGA Graph Processing\n ACTS\u00a0[34] is a graph processing accelerator that utilizes the edge-centric GAS model on FPGAs and employs HBM to address the memory bandwidth bottlenecks of prior single-FPGA-based graph processing designs. The key idea behind ACTS is an online recursive partitioning mechanism that converts (via partitioning) the low-locality vertex updates generated from processing the edges of an active sub-graph, into high-locality vertex-update partitions in efficient time. This partitioning is done across the destination vertex IDs. Through this, ACTS improves both read and write bandwidth performance, even as graph size increases. Consequently, ACTS achieved an average speedup of 1.5\u00d7, with a peak speedup of 4.6\u00d7 compared to Gunrock\u00a0[16], a state-of-the-art GPU-based graph processing accelerator, on the NVIDIA Titan X GPU. Furthermore, ACTS demonstrates an average speedup of 3.6\u00d7, with a peak speedup of 16.5\u00d7 over GraphLily\u00a0[32], a modern FPGA-based graph accelerator utilizing HBM. These speedups are found in their paper. We therefore use this as the starting point for Swift\u2019s multi-FPGA solution. \nII-C ForeGraph: Scalable FPGA Graph Processing\n ForeGraph\u00a0[36, 39] tackles scaling by using the Catapult torus interconnect in an FPGA simulated environment; however, it cannot scale beyond 48 nodes or maintain optimal performance as the number of nodes increases. As the number of FPGA nodes increases, the interconnet becomes a bottleneck, limiting scalability and degrading performance. \nIII Swift\n \nIII-A Graph Processing Decoupled Pipeline\n The Swift graph processing accelerator builds upon ACTS by further decoupling its pipeline into five distinct stages: process-edge, partition-updates, apply-updates, import-frontier and export-frontier operations. This decoupling allows Swift to hide latency by exploiting overlap among operations, speeding up overall execution time. Figure\u00a01 illustrates the connection among three key stages\u2014Process-edge, Partition-updates, and Apply-updates\u2014involved in graph partitioning within an FPGA. Each stage interfaces with the HBM channels to receive specific data: edges for Process-edge, vertex updates for Partition-updates, and both vertex updates and properties for Apply-updates. The output data from each stage serves as the input for the subsequent stage, creating a continuous processing flow. Process-edge: This operation generates vertex-update messages from the active sub-graph (source intervals). As shown in Figure\u00a01, edges are read from HBM into EdgeProperty Buffers (BRAM), and their source vertex properties are read into VertexProperty Buffers (URAMs). Edges of active vertices are processed, and a user-defined edge function generates vertex-update messages, following the proposed scheme in Algorithm\u00a01. The vertex updates are buffered in DRAM for the partition-updates operation. The vertex-update tuple is formatted as (Value, Dst), where Dst is the destination vertex, and the value is the message. Partition-updates:\nThe partition-updates stage, introduced in the ACTS paper\u00a0[34], addresses the challenge of random accesses and low spatial locality in vertex-updates generated from the process-edges stage. The operation is online and happens on the device side to further decompose the vertex updates generated from the process-edges. Partitioning enhances memory locality by converting low locality vertex updates (from edge processing) into high locality, enabling efficient use of fast URAMs for updating vertices. Due to the initial static partitioning of the graph, edge and vertex layouts within each HBM channel are optimized for online partitioning, which is confined to each HBM channel. The partition-updates operationconverts low-locality vertex updates into fine-grained, high-locality vertex-update partitions. The vertex updates generated from the process-edges operation are loaded into fast on-chip URAMs and BRAMs, and then partitioned using FPGA logic into high-locality partitions. This allows updates, represented by key-value pairs, to leverage the Ultra-RAM (URAM) multi-port parallelism and high capacity in Xilinx\u00a0[54] FPGAs when applied to destination vertices. However, with large graphs, URAM capacity is still limited, which can lead to partitioning overheads when swapping vertex updates. As shown in Figure\u00a01, a recursive BRAM tree (l\u2062o\u2062g2\u2062(D\u2062s\u2062t)\ud835\udc59\ud835\udc5csubscript\ud835\udc542\ud835\udc37\ud835\udc60\ud835\udc61log_{2}(Dst)italic_l italic_o italic_g start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_D italic_s italic_t )) manages DRAM access latency with multilevel passes as updates move between BRAM and HBM, improving vertex-update locality with each level, thus reducing DRAM access latency. This makes it preferable to conventional bucket partitioning, especially for large graphs with low spatial locality. By breaking the task into recursive steps and buffering intermediate partial-partitioned results in HBM, the recursive BRAM tree strategy improves overall partitioning throughput and efficiency. The number of passes is the logarithm of the range of destination vertex IDs. This ensures efficient data transfer from BRAM to HBM, surpassing bucket partitioning. To illustrate the advantage of the recursive BRAM tree over conventional bucket-based partitioning, assume we have (N) vertex updates in HBM from processing active graph edges. Conventional bucket partitioning reads chunks of updates into the FPGA, splits them into (P) buckets based on destination vertex IDs, and writes them back to HBM. As the graph size increases, both (N) and (P) grow, requiring more partitions to maintain locality, which increases DRAM access latency and degrades performance. In contrast, the recursive BRAM tree strategy splits partitioning into successive steps, reducing latency and performance degradation.\nIn each pass, after the buckets are filled, they are streamed into HBM, using its full bandwidth. Then in the next pass, each bucket is read back from memory (also streaming), and partitioned again, until sufficient locality is achieved. Although it may require a logarithmic number of passes for large graphs, the recursive BRAM partitioning allows for better performance than prior art, because it maintains high locality in the HBM accesses. Apply-updates:\nWhen receiving a vertex update, the apply updates stage resolves this update to its destination vertex using a user-defined Apply function. This apply operation generates an active frontier property. Because the earlier (i.e., Partition-updates) operation outputs vertex-update chunks with high BRAM locality, the Apply operation can benefit from fast URAM memory. This is because several high-locality vertex-update partitions (generated from the partition-updates stage) and their corresponding destination vertex properties are streamed into independent high-speed URAMs, each connected to a separate apply-update logic. Therefore, several updates can be applied concurrently, allowing parallelism. In this way, the Apply operation can benefit from fast URAM memory. Import-frontier and export-frontier operations:\nA multi-FPGA graph processing context requires periodically exchanging graph data between FPGAs. Export-frontier operations send active frontiers from a given FPGA to its remote neighbors via PCIe through the host. These active frontiers are gathered and merged at the remote FPGA end using the import-frontier operation. Host-FPGA communication uses a host-managed shared buffer for DMA transfers over PCIe. This buffer moves active vertex properties (frontiers) between host memory and FPGA\u2019s HBM during export/import operations (Section III). The DMA engine handles memory transfers, reading from host memory (H2C) and writing to FPGA, and vice-versa. \nIII-B Understanding the Swift Pipeline\n The Swift pipeline leverages the time window between edge processing within an FPGA and its next iteration to overlap with other intra-FPGA computation and inter-FPGA communication, using separate FPGA resources concurrently. Key to our flow model is that regions within the active sub-graph (vertex intervals) can start the next operation in the pipeline once dependencies are met. This contrasts to the bulk-synchronous model adopted by various prior art that require each operation to finish on the entire sub-graph before proceeding to the next. This allows for overlapping operations on two levels: Inter-FPGA: Overlapping computation (within FPGAs) with communication operations (between FPGAs). Intra-FPGA: Operations within the same FPGA, hiding expensive, throughput-limiting operations within each other in the processing pipeline and improving throughput. Figure\u00a02 shows Swift\u2019s decoupled execution flow versus the conventional bulk-synchronous model. In the conventional model, stages happen sequentially, starting only after the previous one is completed. For example, exporting active frontiers to remote FPGAs (export-frontiers stage) occurs only after applying updates to the active sub-graph (apply-updates stage). Similarly, processing edges (process-edges stage) occur only after receiving all import frontiers.\nIn Swift\u2019s model (Figure\u00a02), a decoupled flow exploits potential overlaps within and between FPGAs. The graph is divided into partitions, each assigned to an FPGA. Within each FPGA, partitions are divided by source vertex IDs into vertex intervals, illustrated in Figure\u00a02 during pre-processing. Graph layout details are in Section\u00a0IV-B. For simplicity, four vertex intervals are shown. Each interval goes through five stages as in Section\u00a0III-B. Unlike the conventional model, there\u2019s no bulk-synchronous constraint. An interval can start its next operation as soon as its dependencies are satisfied. This is explained in Section\u00a0III-C. To better understand the Swift decoupled pipeline, let us look at each overlapping feature when processing graphs: Overlap between computation within an FPGA and communication between FPGAs: In Figure\u00a02, F\u2062P\u2062G\u2062A0\ud835\udc39\ud835\udc43\ud835\udc3asubscript\ud835\udc340FPGA_{0}italic_F italic_P italic_G italic_A start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT starts processing edges using the process_edge operation (denoted by P\u2062E0\ud835\udc43subscript\ud835\udc380PE_{0}italic_P italic_E start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT) in src interval 0 as soon as its active frontiers are imported from remote FPGAs. This happens concurrently with src interval 1 importing its active frontiers (I\u2062F1\ud835\udc3csubscript\ud835\udc391IF_{1}italic_I italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT). Similarly, Dst interval 0 in F\u2062P\u2062G\u2062A0\ud835\udc39\ud835\udc43\ud835\udc3asubscript\ud835\udc340FPGA_{0}italic_F italic_P italic_G italic_A start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT exports active frontiers to remote FPGAs (E\u2062F0\ud835\udc38subscript\ud835\udc390EF_{0}italic_E italic_F start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT), concurrently with interval 1 generating vertex updates (A\u2062U1\ud835\udc34subscript\ud835\udc481AU_{1}italic_A italic_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT), allowing computation overlap within an FPGA and communication between FPGAs. Overlap between multiple operations within the same FPGA: For example, import-frontier operation for interval 1 in iteration 2 runs concurrently with process_edge and partition-updates for interval 0, and apply-updates for interval 3, overlapping operations within each FPGA and keeping HBM, URAM, and compute resources simultaneously busy. Due to strict dependencies, some FPGA operations cannot overlap. Partition-updates and apply-updates are such operations. Apply-updates can begin only after vertex updates from process-edges are partitioned online. Additionally, process-edges and partition-updates can be merged into a single step. \nIII-C Swift Flow Example\n This section will demonstrate how Swift operates within FPGA hardware, using a cluster of four FPGAs as an example. For simplicity, we will focus on the operations within a single FPGA (F\u2062P\u2062G\u2062A0\ud835\udc39\ud835\udc43\ud835\udc3asubscript\ud835\udc340FPGA_{0}italic_F italic_P italic_G italic_A start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT.) The workload graph assigned to each FPGA is first divided into vertex intervals based on vertex IDs, as shown in Figure\u00a02 and\u00a03. A vertex interval consists of vertices along with their incoming edges. Inside each FPGA, five execution modules carry out the following operations: Process-edges (P\u2062EM\ud835\udc43subscript\ud835\udc38\ud835\udc40PE_{M}italic_P italic_E start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT), Partition-updates (P\u2062UM\ud835\udc43subscript\ud835\udc48\ud835\udc40PU_{M}italic_P italic_U start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT), Apply-updates (A\u2062UM\ud835\udc34subscript\ud835\udc48\ud835\udc40AU_{M}italic_A italic_U start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT), Export-frontier (E\u2062FM\ud835\udc38subscript\ud835\udc39\ud835\udc40EF_{M}italic_E italic_F start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT), and Import-frontier (I\u2062FM\ud835\udc3csubscript\ud835\udc39\ud835\udc40IF_{M}italic_I italic_F start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT). At any given time, each vertex interval can be in one of three states: \u201dready-for-process,\u201d \u201dready-for-export,\u201d and \u201dready-for-import.\u201d An interval in the \u201dready-for-process\u201d state indicates that all dependencies needed for the process-edges operation on that interval have been met, allowing the process-edges module to execute that interval directly. The same principle applies to the \u201dready-for-export\u201d and \u201dready-for-import\u201d states. The modules continuously check the state of vertex intervals to carry out computation, export, and import operations. The steps below demonstrate the Swift execution flow in FPGA hardware, focusing on one FPGA (F\u2062P\u2062G\u2062A0\ud835\udc39\ud835\udc43\ud835\udc3asubscript\ud835\udc340FPGA_{0}italic_F italic_P italic_G italic_A start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT). Initialization: During processing initiation, all vertex intervals containing active vertices in F\u2062P\u2062G\u2062A0\ud835\udc39\ud835\udc43\ud835\udc3asubscript\ud835\udc340FPGA_{0}italic_F italic_P italic_G italic_A start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT to F\u2062P\u2062G\u2062A3\ud835\udc39\ud835\udc43\ud835\udc3asubscript\ud835\udc343FPGA_{3}italic_F italic_P italic_G italic_A start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT are set to the ready-to-process state. Process-edges and Partition-updates: The process-edge module (P\u2062EM\ud835\udc43subscript\ud835\udc38\ud835\udc40PE_{M}italic_P italic_E start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT) in F\u2062P\u2062G\u2062A0\ud835\udc39\ud835\udc43\ud835\udc3asubscript\ud835\udc340FPGA_{0}italic_F italic_P italic_G italic_A start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT activates in the ready-to-process state, executing process-edge and partition-update operations on all vertex intervals. Concurrently, the partition-updates module (P\u2062UM\ud835\udc43subscript\ud835\udc48\ud835\udc40PU_{M}italic_P italic_U start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT) partitions low-locality vertex updates into high-locality vertex-update partitions. Apply-updates After generating and partitioning vertex updates, the apply-updates module (A\u2062UM\ud835\udc34subscript\ud835\udc48\ud835\udc40AU_{M}italic_A italic_U start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT) processes each partition to create active frontiers and flags the intervals as ready-for-export. Export-frontiers: The export-frontier module (E\u2062FM\ud835\udc38subscript\ud835\udc39\ud835\udc40EF_{M}italic_E italic_F start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT) starts exporting active frontiers to the host CPU when triggered by the ready-for-export flag. This enables overlap between apply-updates and export-frontier operations until all vertex interval frontiers are processed. Figure\u00a02 illustrates this with A\u2062U0\ud835\udc34subscript\ud835\udc480AU_{0}italic_A italic_U start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, A\u2062U1\ud835\udc34subscript\ud835\udc481AU_{1}italic_A italic_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, A\u2062U2\ud835\udc34subscript\ud835\udc482AU_{2}italic_A italic_U start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, A\u2062U3\ud835\udc34subscript\ud835\udc483AU_{3}italic_A italic_U start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT overlapping E\u2062F0\ud835\udc38subscript\ud835\udc390EF_{0}italic_E italic_F start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, E\u2062F1\ud835\udc38subscript\ud835\udc391EF_{1}italic_E italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, E\u2062F2\ud835\udc38subscript\ud835\udc392EF_{2}italic_E italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, E\u2062F3\ud835\udc38subscript\ud835\udc393EF_{3}italic_E italic_F start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT. Import-frontiers: Active frontiers associated with vertex intervals are marked ready-for-import by the export-frontier module. Thus, the import-frontier module (I\u2062FM\ud835\udc3csubscript\ud835\udc39\ud835\udc40IF_{M}italic_I italic_F start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT) in remote FPGAs can overlap their operations. This is shown by the overlap of E\u2062F0\ud835\udc38subscript\ud835\udc390EF_{0}italic_E italic_F start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, E\u2062F1\ud835\udc38subscript\ud835\udc391EF_{1}italic_E italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, E\u2062F2\ud835\udc38subscript\ud835\udc392EF_{2}italic_E italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, E\u2062F3\ud835\udc38subscript\ud835\udc393EF_{3}italic_E italic_F start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT from iteration 1 with I\u2062F0\ud835\udc3csubscript\ud835\udc390IF_{0}italic_I italic_F start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, I\u2062F1\ud835\udc3csubscript\ud835\udc391IF_{1}italic_I italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, I\u2062F2\ud835\udc3csubscript\ud835\udc392IF_{2}italic_I italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, I\u2062F3\ud835\udc3csubscript\ud835\udc393IF_{3}italic_I italic_F start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT from iteration 2 in Figure\u00a02. Cycle Continuation: This cycle continues until the algorithm converges (i.e., no more active frontiers) or until each vertex interval has completed a given number of iterations. \nIV Graph Partitioning and Workload Balancing\n \nIV-A Graph Partitioning\n Figure 3 illustrates the layout of a graph within Swift FPGA cluster. The graph is initially partitioned by its destination vertex IDs across different FPGAs. Graph partitioning is performed on the host side as a pre-processing step, as represented by different colors. Since Swift is designed for static graphs (i.e., graphs with a fixed topology), this is treated as a one-time cost that can be amortized over multiple iterations. Each data type is partitioned differently. As in some prior work, vertex properties are represented using two dimensions: source/destination. Each FPGA holds a full copy of the source vertex properties, while destination vertex properties are distributed across all HBM channels and all FPGAs, as are edges, which are partitioned by destination vertex IDs. Each processing element is connected to one HBM channel, processing edges and destination vertex properties in that channel. Each destination range and its incoming edges are assigned to a unique FPGA. Within each FPGA, the graph partition is further divided by source IDs. Each FPGA in the cluster has a dedicated HBM channel, the \u201cfrontier HBM,\u201d which accommodates active frontiers imported from the communication channel. The remaining HBM channels in the FPGA, the \u201cworker HBMs,\u201d each store a segment of the graph\u2019s destination vertices and their incoming edges. Each processing element (PE) is linked to a worker HBM and handles the edges within that specific channel. To prevent graph data duplication and maintain storage efficiency, unique edges and vertices are distributed across HBM channels. The vertices within each HBM are categorized into vertex intervals, with the range being VN\u2062U\u2062M\u2062_\u2062P\u2062E\u2062s\ud835\udc49\ud835\udc41\ud835\udc48\ud835\udc40_\ud835\udc43\ud835\udc38\ud835\udc60\\frac{V}{NUM\\_PEs}divide start_ARG italic_V end_ARG start_ARG italic_N italic_U italic_M _ italic_P italic_E italic_s end_ARG, where V\ud835\udc49Vitalic_V represents the vertex properties that can fit in URAM. N\u2062U\u2062M\u2062_\u2062P\u2062E\u2062s\ud835\udc41\ud835\udc48\ud835\udc40_\ud835\udc43\ud835\udc38\ud835\udc60NUM\\_PEsitalic_N italic_U italic_M _ italic_P italic_E italic_s denotes the number of processing elements in the cluster. Consequently, the combined range of vertex intervals across all PEs in the cluster is V\ud835\udc49Vitalic_V. \nIV-B Workload Balancing\n Optimizing performance in a cluster-scale environment with HBM-enabled FPGAs requires an efficient workload placement strategy that leverages parallelism at multiple levels. The first level of parallelism comes from the independent FPGAs in the cluster. Each FPGA has 32 independent HBM channels, adding a second level of parallelism. Swift configuration allows up to 128 Processing Elements (PEs) to operate independently in a 4-FPGA cluster. The challenge is to prevent any straggler PE from becoming a bottleneck, which requires a graph placement strategy that ensures uniform workload balance. Prior schemes distributes the graph across machines using a pre-processing step, maintaining balance but sacrificing throughput due to the graphs\u2019 unstructured nature [55, 56]. This resulted in the creation of cutting edges across various machines, disrupting the sequential ordering of vertex IDs. As a result, vertex translation was necessary for storage efficiency, and it also introduced communication bottlenecks between FPGAs. Swift avoids translations at receiver FPGAs by consistently referencing vertices using global IDs across all FPGAs. Additionally, it enforces a vertex-interval-based strategy for workload placement. This means that all vertices and edges within a vertex interval are placed across the entire cluster before moving to the next interval. This approach allows imported active frontiers to fit into low-latency URAM, enhancing throughput. In summary, our proposed strategy for placing graph workloads balances workload distribution and optimizes throughput in a cluster-scale, HBM-enabled FPGA environment. We achieve efficient graph processing without sacrificing overall performance by addressing translation bottlenecks and leveraging parallelism. \nV Performance Evaluation\n \nV-A Experimental Methodology\n We studied three commonly used graph algorithms: Pagerank (PR), Sparse Matrix-Vector Multiplication (SpMV), and Hyperlink-Induced Topic Search (HITS) to explore their distinct contributions within Swift. These algorithms capture memory access patterns that are common to various other graph algorithms. Our experiments involved using both synthetic and real-world datasets as shown in Table\u00a0II. We choose these datasets because they express diverse cache behaviors. The synthetic datasets were generated from the RMAT graph generator [58], while the real-world datasets were obtained from the University of Florida\u2019s Sparse Matrix Collection [59]. Because of the limited HBM memory capacity (8GB per FPGA), we have postponed the exploration of very large graphs for future research. Future HBMs are expected to deliver up to 32GB, allowing for much larger graphs to be run. In our study, we conducted a comprehensive comparison of Swift with several state-of-the-art clusterscale systems, including ForeGraph (FPGA-based), PowerGraph (CPU-based), TurboGraph (FPGA-based), FPGP (FPGA-based), FDGLib (FPGA-based), and Gunrock (GPU-based). We compared Swift with Gunrock, as it was open-sourced. The complete implementation of Swift, including I/O and FPGA kernel invocation costs, was carried out using four Xilinx Alveo Ultrascale+ FPGA Accelerator Cards. These cards are equipped with HBM (High Bandwidth Memory) capable of delivering up to 460GB/s per FPGA. Gunrock, on the other hand, was tested on four NVIDIA A40 GPUs with HBM2 memory supporting 696GB/s per GPU (as shown in Table\u00a0III). Communication among FPGAs in Swift occurs via PCIe, with data routed through the host. Our model uses PCIe\u2019s duplex feature for simultaneous read/write to optimize transfer, using Gen3 x16 PCIe, which delivers up to 17 GB/s. The RTL code was generated from the C++ HLS source using the Xilinx HLS tool, and the design was synthesized and run on the Xilinx Alveo FPGA board using the Xilinx Vitis tool. It is important to note that Vitis was only able to synthesize up to 24 Processing Elements (PEs), resulting in a clock frequency of 150 MHz. Further improvements to the synthesis, to enable more PEs and a higher clock frequency, are ongoing work, but this configuration already shows the potential of Swift. Timing measurements for both Swift and Gunrock begin once the graph is loaded onto the accelerator and end upon completion of kernel processing, capturing all exchanges during execution. The graph loading time is excluded, as it is a one-time cost. Data movement is facilitated by the host using the DMA engine, which reads graph data from the host\u2019s allocated memory (for H2C) and writes it directly into the FPGA\u2019s HBM memory during import, and vice versa for export. The synchronization process is overlapped with the FPGA kernels performing graph processing across multiple FPGAs. This overlap is achieved through double buffering, out-of-order command queuing, asynchronous event handling, and non-blocking calls, all implemented on the host side. \nV-B Results\n Figure\u00a04\nand\u00a05 present a comparative analysis of Swift with prior accelerators, leading to several noteworthy observations. Swift exhibits mixed performance compared to Gunrock in Figure 4. Some datasets such as SK-2005 and UK-2005 are characterized by high regularity and cache hit rates, and have significant benefits from the advanced caching mechanism of the GPU. In contrast, Swift demonstrates superior throughput with relatively unstructured datasets over Gunrock. We could not collect results for HITS on Swift due to out-of-memory error. It\u2019s important to note that the evaluation of Gunrock is on a GPU cluster using A40 GPUs with NVlink; the A40 offers higher off-chip memory bandwidth (768 GB/s vs. 345 GB/s) and NVlink offers higher inter-device bandwidth (112 GB/s vs. 17 GB/s) compared to the Alveo U280 FPGAs. To evaluate the benefit the GPU system derives from NVLink vs. the slower PCIe interconenct, we evaluated Gunrock on PageRank on R8 with NVlink disabled. Without NVlink, Gunrock is 4.8X slower than with NVlink, similar to the bandwidth difference. This suggests that with a similar high-speed interconnect, the multi-FPGA system would consistently outperform Gunrock for all our algorithms and datasets. Swift exhibits superior performance over prior multi-FPGA-based (Foregraph, Hadoop with FPGAs) and CPU-based (Powergraph) clusterscale graph accelerators. As shown in Figure 6(c), Swift outperforms Foregraph by up to 12x, Hadoop by 890x, and Powergraph by 57x. This superiority can be attributed to two main factors: Swift effectively manages random accesses related to vertex-to-vertex communication within each FPGA by restructuring vertex updates during processing and leveraging fast URAMs to perform apply-update operations (refer to section III-A) Swift\u2019s decoupling strategy enables tight interleaving between computation (within FPGAs) and communication (between FPGAs), as well as between computation operations within the same FPGA. This reduces idle times. We compared Swift against the bulk-synchronous GAS approach (where no overlapping exists) to gain insights into the impact of our decoupling approach and better quantify the performance impact of overlapping communication with computation during graph processing. The result are plotted in Figure 6(a). To achieve this we turned off the asynchronous behavior in Swift to enforce that each iteration completes a bulk-synchronous step before the next commences. The results prove that Swift\u2019s decoupling mechanism provides about 2-3X improvement to throughput. Swift (FPGA-based) and Gunrock (GPU-based) were run on different platforms with different characteristics. The Alveo U280 FPGA has an off-chip memory bandwidth of 460GB/s, while the Tesla A40 GPU supports up to 768GB/s. To compare, we use bandwidth efficiency (MTEPS/bandwidth), and energy efficiency (MTEPS/Watt)\nWe query GPU power using Nvidia-smi and FPGA using Xilinx\u2019s xbutil.\nBased on our observations, Swift experiences about 1.5X better bandwidth efficiency than Gunrock and about 2X better energy efficiency. Further profiling of power consumption in Swift revealed that as much as 80% of Swift\u2019s overall power is used by the HBM while only about 20% is spent in on-chip FPGA activity.\n\n Figure 6(b) shows the throughput for a number of datasets plotted across an increasing number of FPGAs, to gain insights into how Swift scales. Some datasets (TW, UK & R32) were too large to fit in a 2-FPGA setup and their 2-FPGA numbers were omitted. As shown, Swift\u2019s throughput increases relatively linearly as more FPGAs are added. This linear stability facilitated by the workload balancing mechanism (Section IV-B) allows a graph workload to be uniformly distributed across the different FPGAs in the cluster. \nVI Conclusions\n The paper introduces Swift, a clusterscale graph accelerator for FPGAs with HBM. Swift leverages the open-source ACTS\u00a0[34] framework and addresses key challenges not present in single-FPGA accelerators, in particular the limited bandwidth of FPGA-to-FPGA communication and inefficiency in prior workload balancing strategies. To overcome these challenges, Swift allows overlapping of crucial graph processing primitives, such as edge processing within a local FPGA, importing of active frontiers from remote FPGAs, and exporting of active frontiers to remote FPGAs. This approach maximizes communication bandwidth across PCIe, off-chip (HBM/DDR), and on-chip (SRAM), effectively concealing inter-FPGA communication with intra-FPGA computation.\nSwift outperforms prior FPGA-based frameworks. Results compared to Gunrock on a multi-GPU system are mixed, because the GPU system benefits from 5X higher inter-card bandwidth due to NVlink, but still achieves over 2X greater energy efficiency. If the FPGA system had a similar high-bandwidth interconnect, it should consistently outperform the GPU.\n Acknowledgements This work was funded in part by PRISM, one of seven centers in JUMP 2.0, an SRC program sponsored by DARPA; the NSF I/UCRC MIST Center,\nand Booz Allen Hamilton under contract FA-8075-18-D-0004. We also thank the anonymous reviewers for their helpful suggestions. References"}
{"text": "AssertLLM: Generating Hardware Verification Assertions from Design Specifications via Multi-LLMs Assertion-based verification (ABV) is a critical method to ensure logic designs comply with their architectural specifications. ABV requires assertions, which are generally converted from specifications through human interpretation by verification engineers. Existing methods for generating assertions from specification documents are limited to sentences extracted by engineers, discouraging their practical applications.\nIn this work, we present AssertLLM, an automatic assertion generation framework that processes complete specification documents. AssertLLM can generate assertions from both natural language and waveform diagrams in specification files. It first converts unstructured specification sentences and waveforms into structured descriptions using natural language templates. Then, a customized Large Language Model (LLM) generates the final assertions based on these descriptions.\nOur evaluation demonstrates that AssertLLM can generate more accurate and higher-quality assertions compared to GPT-4o and GPT-3.5. \n1. Introduction Hardware functional verification is critical in the VLSI design flow. It addresses the following question: whether an implementation adheres to its specification.\nThe specifications are typically drafted in natural language by architects and then translated into RTL code by designers. Verification engineers then check the functional correctness of the RTL designs according to the specifications.\nIn the verification process, assertion-based verification (ABV)\u00a0(Witharana et\u00a0al., 2022) is a widely adopted technique, which utilizes assertions crafted from specifications to verify the functional behavior of RTL designs. ABV can be performed either by simulation or formal property verification (FPV), where assertions are often written in the form of SystemVerilog Assertions (SVAs). However, a significant challenge in ABV is the generation of sufficient, high-quality assertions. Currently, designing SVAs manually is a time-consuming and error-prone task, demanding unignorable human effort. To address this challenge, research has focused on generating SVAs automatically, which can be mainly categorized into two types: (1) dynamic mining from simulation traces and (2) static analysis of specifications.\nDynamic methods\u00a0(Germiniani and Pravadelli, 2022; Danese et\u00a0al., 2017; Vasudevan et\u00a0al., 2010) combine simulation with static design constraint analysis but risk generating incorrect SVAs due to their reliance on potentially flawed RTL designs.\nStatic methods utilize predefined templates\u00a0(Orenes-Vera et\u00a0al., 2021; Fang et\u00a0al., 2023) or natural language processing (NLP)-based machine learning (ML) technologies\u00a0(Harris and Harris, 2016; Krishnamurthy and Hsiao, 2019a; Zhao and Harris, 2019; Krishnamurthy and Hsiao, 2019b; Frederiksen et\u00a0al., 2020; Keszocze and Harris, 2019; Parthasarathy et\u00a0al., [n.\u2009d.]; Aditi and Hsiao, 2022; Chang et\u00a0al., 2024). Recently, the potential of generative AI technologies like Large Language Models (LLMs) has gained significant attention in hardware design process\u00a0(Lu et\u00a0al., 2024; Liu et\u00a0al., 2024a; Chen et\u00a0al., 2024; Liu et\u00a0al., 2023; Li et\u00a0al., 2024; Pei et\u00a0al., 2024; Wu et\u00a0al., 2024). Researchers are also exploring the use of LLMs to generate hardware assertions\u00a0(Kande et\u00a0al., 2023; Orenes-Vera et\u00a0al., 2023; Sun et\u00a0al., 2023; Pulavarthi et\u00a0al., 2024; Shang et\u00a0al., 2024). We further categorize the existing static ML-based methods based on their application in different design phases: the RTL and pre-RTL stages.\nTable\u00a01 details these ML-based SVA generation methods in both the RTL stage and the pre-RTL stage. During the RTL stage, the process typically involves using LLMs to process both human-written specification sentences and the RTL design to generate SVAs describing security or functional properties\u00a0(Kande et\u00a0al., 2023; Liu et\u00a0al., 2024b; Orenes-Vera et\u00a0al., 2023; Sun et\u00a0al., 2023). However, similar to the dynamic methods, inaccuracies in RTL implementations could result in flawed SVAs. Regarding the pre-RTL stage, with the specification document finalized, RTL designers proceed to implement behavioral designs satisfying the specification.\nNumerous studies\u00a0(Harris and Harris, 2016; Krishnamurthy and Hsiao, 2019a; Zhao and Harris, 2019; Krishnamurthy and Hsiao, 2019b; Frederiksen et\u00a0al., 2020; Keszocze and Harris, 2019; Parthasarathy et\u00a0al., [n.\u2009d.]; Aditi and Hsiao, 2022) have used NLP techniques to generate SVAs from natural language sentences, focusing on sentence-level analysis from extensive specifications. However, this approach has several limitations. It is labor-intensive, requiring manual extraction of relevant sentences. It also struggles with diverse grammatical structures, and its evaluation often relies on design-specific checkers, limiting broader applicability. Furthermore, specifications frequently include waveform diagrams that illustrate functional behaviors across different signals. Nevertheless, there is no existing technique that can generate SVAs from these diagrams currently, representing a significant gap in the field. Here we summarize three key challenges that currently hinder the practical application of SVA generation from specification documents: Natural language VLSI specifications are often unstructured and are hard to be directly used for assertion generation. Even with structured specifications, translating natural language into assertions remains a highly complex task, requiring both a deep understanding of the design functionality and an expertise in SVA. While waveforms commonly exist in specification documents, no current research focuses on capturing functional behaviors from waveforms and generating corresponding SVAs. To tackle these challenges in SVA generation, in this work, we propose AssertLLM, a novel automatic assertion generation framework incorporating multiple LLMs to deal with the decomposed tasks separately.\nThis framework is designed to process the entire specification files as they are, and automatically produce SVAs across different signals, significantly benefits both design-time bug prevention and verification-time bug detection.\nAssertLLM operates in three main steps. First, it processes natural language in specification documents, using an LLM to convert unstructured information into a structured format via a unified template. Second, it analyzes waveform diagrams in the specification files, employing another LLM to create natural language templates and extract waveform descriptions based on these templates. Unlike previous methods\u00a0(Orenes-Vera et\u00a0al., 2021; Fang et\u00a0al., 2023),\nAssertLLM does not require an additional human input to create templates.\nFinally, a customized LLM translates the extracted information into SVAs. The resulting SVAs check various design aspects, including bit-width, connectivity, and functionality.\n Our contributions in this work are summarized below: To the best of our knowledge, AssertLLM is the first automatic assertion generation method that can handle the complete specification files.\n We decompose the assertion-generation process into three key steps: extracting structural information from natural language, generating descriptions from waveform diagrams, and translating the information into SVAs. These SVAs support checks for bit-width, connectivity, and functionality. To demonstrate the effectiveness of AssertLLM, we conduct a comprehensive evaluation on a set of designs. The result shows that 88% of generated SVAs are evaluated to be correct both syntactically and functionally. Furthermore, these correct SVAs achieve 97% cone of influence coverage, demonstrating the high quality of the generated SVAs. \n2. Methodology \n2.1. Workflow Overview Figure\u00a01 illustrates the process of generating and evaluating SVAs for AssertLLM. Our approach integrates three LLMs to generate hardware verification assertions from comprehensive specification documents. These models perform the following tasks: 1) extract relevant information from the natural language in the specification necessary for SVA generation; 2) extract behavioral descriptions from waveform diagrams for SVA generation; 3) generate high-quality SVAs based on the previous extracted information. In the subsequent subsections, we will detail the functionalities of each LLM of the assertion generation flow. Following this, our SVA evaluation methodology will be presented. \n2.2. Natural Language Analyzer A comprehensive natural language specification typically includes seven key sections: 1) introduction: outlines the design\u2019s concepts and features; 2) IO ports: provides detailed information for the interface; 3) registers: describes all the architecture-level registers in the design; 4) operation: explains the operational procedures for dataflow and control; 5) architecture: the high-level workflow and dataflow of the design; and 6) usage examples: offers basic usage scenarios for the design. For signals, the specification may only define critical architecture-level IO ports and registers, leaving the designers to detail internal signals for RTL implementations. 7) waveform diagram: describe behaviors for different signals. The first step of our AssertLLM framework is to extract structured information from natural language specification documents to enable SVA generation.\nAs we discussed in Section\u00a01, the first key challenge of SVA generation lies in the inherent unstructured nature of the original specifications, which contain background information, functional descriptions, microarchitecture designs, and various diagrams, including dataflow and waveform, etc. Meanwhile, the existence of assertion-relevant information across different sections further complicates the direct utilization of the original specifications for SVA generation. To address the challenge of processing original, unstructured, full-size specification documents, we utilize an LLM tailored to extract structural and relevant information for each defined signal, thereby further facilitating the SVA generation process. Specifically, in our LLM  1 Natural Language Analyzer, we first utilize system instructions to customize the LLM.\nThe model takes the full-size specification file as the input, and the multi-modal function is employed to analyze the file containing text, tables, figures, etc.\nThen for each signal, the LLM is required to extract all the related information of the signal. Here, we design a unified and structured template to guide the LLM in extracting all essential signal-related information. This template contains three key components: the signal\u2019s name, its description, and the interconnection signals. We demonstrate the details of each part as follows: Name: The identifier of the signal in the specification, ensuring clear and unambiguous reference. Description: To facilitate SVA generation, we divide the descriptions into four categories, including 1) definitions such as bit-width and signal type. 2) functionality which contains all the function-related information of the target signal in the entire specification file. 3) interconnection relationship with all other signals. 4) additional information that is not included in the above three types. Interconnection Signals: A list of signals that interact or are associated with the target signal, which are essential for the subsequent assertion generation process. Note that the extracted information is summarized across different sections of the original specification, which contains all the textual information needed for assertion generation.\nIn order to incorporate specifications from the waveform,\nwe employ another LLM to extract information from the waveform diagrams as presented in next subsection, thereby enriching the set of SVAs and potentially enhancing their overall quality. \n2.3. Waveform Analyzer In this work, we are interested in extracting design behaviors from waveforms in the specification document, where these waveforms are presented in the form of images (a two-dimensional array of pixels).\nUnlike those prior methods\u00a0(Vasudevan et\u00a0al., 2010; Germiniani and Pravadelli, 2022; Danese et\u00a0al., 2017) that assume a numeric and structured waveform, such as those stored in the Value Change Dump (VCD) format, AssertLLM needs to first interpret the images of waveforms to obtain a description of behaviors.\nTechniques like optical character recognition (OCR)\u00a0(Mori et\u00a0al., 1999) may be used to convert diagrams of a fixed format into text, however, it would be hard to accommodate various waveform styles. Whereas, LLM is advantageous in its flexibility.\nDespite that multi-modal LLMs can take images as input and there are prior works like image captioning\u00a0(Anderson et\u00a0al., 2018; Aneja et\u00a0al., 2018) that interpret input images with human languages. However, they are not suitable for waveform interpretation. To address this problem, we propose another LLM  2 Waveform Analyzer to extract behavior descriptions from waveform diagrams. This analyzer employs natural language to describe the behavior of various signals in the waveform through a two-step process. In the first step, Waveform Analyzer automatically creates a set of templates suitable for describing the behaviors in the given waveform. Subsequently, Waveform Analyzer produces the behavioral descriptions based on these templates. It requires significant effort for a human to write templates to describe behaviors on a single signal or among various signals on the waveform diagrams. To solve this problem, we employ prompt engineering to automate the template generation process. Fig.\u00a03 shows the prompt and corresponding response in the step of template generation. These automatically generated templates serve as references for producing descriptions of given waveform diagrams in the subsequent step. In this step, Waveform Analyzer takes our unified prompt, extracted waveform diagrams, and generated templates as inputs to produce descriptions of the behavior for the given waveform. Fig.\u00a04 shows the corresponding prompt and response in the description generation step. By processing and analyzing the waveform diagrams and the templates, our Waveform Analyzer comprehensively investigates the functional information in the waveforms, thereby identifying potential relationships among different signals. For example, it can determine whether the output signal is valid when a specific signal is enabled or deduce that a signal should remain stable for a specific number of cycles when the corresponding condition is met. Furthermore, we prompt the LLM to directly use the provided templates or extend them if necessary. This flexibility allows the Waveform Analyzer to explore additional behaviors in the waveform. \n2.4. Automatic Assertion Generation For the conversion from textual descriptions to assertions,\nprior research has explored traditional NLP techniques for pre-RTL stages and LLM-based approaches for RTL designs, each with its limitations. NLP methods require detailed syntax and semantics analysis, limiting adaptability to sentence structure variations. LLM approaches, focusing on the RTL stage, depend on HDL code and annotations but risk generating inaccurate SVAs from unverified RTL code, potentially compromising the verification process. To address these challenges, our work introduces the LLM  3 SVA Generator, dedicated to generating assertions for each signal utilizing the previously extracted structural information. Considering the precise syntax and writing rules inherent to SVAs and the potential for the original LLM failing to generate syntactically correct SVAs, as discussed in\u00a0(Orenes-Vera et\u00a0al., 2023), we incorporate the Retrieval Augmented Generation (RAG) technique\u00a0(Yih, 2020) to enhance the LLM\u2019s capability for SVA generation. This approach is enriched by a knowledge database comprising tutorials and textbooks on SVA and formal property verification\u00a0(Seligman et\u00a0al., 2023; Mehta, 2020; Vijayaraghavan and Ramanathan, 2005), providing a robust foundation for the LLM to access and retrieve relevant SVA knowledge based on the input query, thereby enhancing the quality of the generated SVAs. In addition to the RAG technique, we guide the SVA Generator\nwith the overall architecture diagram of the design. The LLM is provided with the structured specifications extracted from the previous LLMs for each signal. Then the LLM is required to generate SVAs that adhere strictly to the specifications.\nWe categorize SVAs into three types: 1) width: check if the signal bit width satisfies the specification; 2) connectivity: check if the signal can be correctly exercised and also the value propagation among all connected signals; 3) function: check if the function defined in the specification is implemented as expected. Fig.\u00a05 demonstrates an example of generating SVAs for a signal. \n2.5. Evaluation of Generated Assertions After the SVA generation process, we then evaluate the quality of the generated assertions.\nWhile some previous studies such as\u00a0(Zhao and Harris, 2019; Frederiksen et\u00a0al., 2020) suggest using specifically-designed checkers for this purpose, such an approach is limited to particular design types like protocols and processors and lacks generalizability to all VLSI designs. Other methods like\u00a0(Orenes-Vera et\u00a0al., 2023) involve manual inspection by the engineers of FPV results using the generated assertions.\nWhile in our approach, we assume there are golden RTL implementations for the designs. Especially, we pick such designs as test cases as they have been thoroughly tested and can be regarded as bug-free golden references.  For evaluation, we also utilize the FPV method. The generated SVAs and the golden RTL designs are fed into a model checker. After performing FPV, we compute the following metrics to evaluate the quality of SVAs: 1) syntax: checks if the generated SVAs have syntax errors; 2) FPV pass/fail: assuming the RTL designs are bug-free, an SVA that passes the FPV check is considered semantically correct, and conversely, a failure indicates an incorrect SVA. 3) COI coverage: Cone of Influence (COI) coverage measures the percentage of design logic that is structurally connected to the properties. It is a common metric to evaluate the quality and usefulness of the generated properties. \n3. Case Study \n3.1. Experimental Setup In our study, the original specification documents are provided in PDF format, including a variety of multi-modal content including text, tables, and figures. The signal definition files and the golden RTL designs are formatted in Verilog. To assess the quality of the generated SVAs, we utilize Cadence JasperGold, one of the leading commercial model checking tools. This evaluation leverages the FPV app in JasperGold to ensure a thorough analysis. Our experimental setup involves the evaluation of three types of LLMs using our developed generation and evaluation methodology: GPT-3.5: The freely available commercial version, GPT-3.5 Turbo, supports a context window of up to 16K tokens. GPT-4o: The state-of-the-art commercial solution, GPT-4o, offers up to 128K token context window and multi-modal capabilities, making it adept at handling the diverse content in the specification documents. AssertLLM: Customized GPT-4o by incorporating specialized techniques such as RAG and custom instructions, tailoring the models specialized to the SVA generation task. \n3.2. Evaluation Metrics To conduct a thorough evaluation of the generated SVAs, we propose a set of metrics that align with our evaluation methodology. This approach ensures a detailed assessment of the SVAs\u2019 quality on both a per-signal and per-design basis. For each assertion type of an individual signal, our evaluation includes the following metrics: 1) the number of generated SVAs; 2) the number of syntax-correct SVAs; 3) the number of FPV-passed SVAs; 4) COI coverage for all FPV-passed SVAs. We consider an SVA as \u201cpassed\u201d if the model checker Jaspergold cannot find any counterexample to it\nwithin 5 hours. Furthermore, all SVAs are produced directly from LLMs without any subsequent modifications. Once the evaluation for each signal is complete, we aggregate the statistics of the generated SVAs for each design. We then calculate the proportion of these SVAs that are syntactically correct and can pass the FPV checks. Finally, we calculate the COI coverages for all passed SVAs.\n \n3.3. Assertion Generation Quality To illustrate the efficacy of AssertLLM, we apply it to an illustrative design case: the \u201cI2C\u201d protocol. The complete specification document for the \u201cI2C\u201d design is structured into six main sections, as discussed in Subsection\u00a02.2.\nNote that the specification for each signal is unstructured, mainly across the sections like IO ports, registers, and operation.\nAdditionally, we provide the signal definition file containing the IO ports and architectural registers and all the internal wires and registers defined for detailed RTL implementation. The specification for the \u201cI2C\u201d design uses\nnatural language to define 23 signals, comprising 17 IO ports and 6 architecture-level registers. For the IO ports, we categorize them into 4 types: clock, reset, control signal, and data signal. The architecture-level registers are similarly categorized, based on their functionality, into control and data types. Furthermore, the specification for the \u201cI2C\u201d design utilizes two waveform diagrams to describe the behaviors on 5 different signals. AssertLLM will extract the described behaviors in the waveforms and generate SVAs for these signals. \u22c6 The results represent the number of generated SVAs, syntax-correct SVAs, and FPV-passed SVAs. \u2020 The percentages indicate the proportion of syntax-correct SVAs and FPV-passed SVAs. The evaluation of SVAs generated by our AssertLLM is presented in Table\u00a02. For each signal, we first verify each type of the generated SVAs separately. Then we summarize all the SVAs to provide design-level statistics.\nAll SVAs related to bit-width checking performed correctly. However, a minor portion of connectivity and function SVAs contained errors. For SVAs generated from natural language, the errors are attributed to misinterpretations of the specification or hallucinations of language model. For SVAs generated from waveform diagrams, the failures are mainly due to AssertLLM\u2019s inability to infer behaviors not explicitly depicted in the waveform diagrams, resulting in incomplete assertions that do not pass the FPV check. Overall, 86% of the SVAs are both syntactically correct and functionally valid.\n In addition to assessing AssertLLM\u2019s performance, we also conduct an ablation study to compare the SVA generation capabilities of the original GPT-4o and GPT-3.5 models without the additional techniques, demonstrated in Table\u00a02. When generating SVAs from natural language, the absence of a mechanism for extracting structured signal specifications significantly limits GPT-4o\u2019s ability to produce accurate SVAs. Specifically, GPT-4o fails to generate any correct SVAs for I/O ports and only succeeds in creating reset check assertions for registers, resulting in an overall accuracy of just 11%. Furthermore, when generating SVAs from waveform diagrams, the lack of a specialized waveform analysis method leads to fewer assertions and a lower FPV-passing rate.\nAs for GPT-3.5, due to the lack of multi-modal processing capabilities, it cannot generate SVAs directly from the original, multi-modal specification files.  We further examine the COI coverage for various signal-type SVAs, as illustrated in Fig.\u00a06.\nOur results demonstrate that SVAs generated by AssertLLM achieve high COI coverage, with a total coverage of 93.44% (different types of assertions could cover different parts of the design, so the overall rate is higher than individual ones). Additionally, we observe that SVAs generated for registers exhibit higher COI coverage compared to those for IO signals. This can be attributed to the fact that registers typically connect to more logic elements within the design. However, when using GPT-4o to generate SVAs, the COI coverage only reaches 82.05%, primarily due to its inability to generate a sufficient number of correct SVAs. \n3.4. Assertion Generation for More Designs To further analyze the capability of our AssertLLM, we extended its application to generate SVAs for additional designs. In addition to \u201cI2C,\u201d we evaluate \u201cECG\u201d and \u201cPairing\u201d designs. The \u201cECG\u201d design calculates the addition of two elements in the elliptic curve group, while the \u201cPairing\u201d design implements Tate bilinear pairing in the elliptic curve group\u00a0(Frey and R\u00fcck, 1994). Table\u00a03 presents the overall results, demonstrating AssertLLM\u2019s ability to generate high-quality SVAs across various designs. All generated SVAs maintain syntactic correctness and a significant proportion of SVAs can pass FPV. Moreover, the FPV-passed SVAs achieve high COI coverage. These indicate good generalizability of our approach. In contrast, GPT-4o struggles to maintain consistent performance across different designs. It fails to generate any FPV-passed SVAs for the \u201cECG\u201d design and produces only one FPV-passed SVA for the \u201cPairing\u201d design. Consequently, the COI coverage for GPT-4o\u2019s SVAs in these two examples is 0%, as we only calculate COI coverage for FPV-passed assertions. The results share the same format as\u00a0Table\u00a02. \n3.5. Discussion: the Impact of Specification Quality The generation of high-quality SVAs from natural language specifications relies not only on the capabilities of LLMs but also on the intrinsic quality of the specification documents themselves. A specification that provides only the basic information of signals, such as their names and simple descriptions, without delving into detailed functionalities or connectivities, inherently limits the potential for generating meaningful SVAs, regardless of the power of the LLMs employed. Conversely, specifications that offer comprehensive details, including clear definitions of signal functionalities and connectivities, can facilitate the generation of SVAs even with relatively simple LLMs. This observation can be concluded in the Table\u00a03. The \u201cI2C\u201d specification provides detailed information on registers and functionality, enabling the generation of more SVAs. By contrast, both AssertLLM and GPT-4o can only generate a limited number of SVAs for \u201cECG\u201d and \u201cPairing\u201d due to their less detailed specifications. \n4. Conclusion In this paper, we introduce AssertLLM, an automated framework designed for generating assertions from entire specification documents. AssertLLM breaks down this intricate task into three phases: natural language information extraction, waveform description extraction, and assertion generation, leveraging specialized LLMs for each phase. Experimental results show that AssertLLM generates assertions with an average of 88% passing both syntax-checking and FPV checking. Furthermore, these assertions achieve 97% COI coverage on average, achieving a notable improvement from GPT-4o and GPT-3.5. References"}
{"text": "numbersnone\\lstKV@SwitchCases#1none:\nleft:\nright:\n\n\n\n\n\n             \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Auto-SPICE: Leveraging LLMs for Dataset\nCreation via Automated SPICE Netlist\nExtraction from Analog Circuit Diagrams Auto-SPICE is the first fully automated framework leveraging large language models (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis (SPICE) netlists. It addresses a long-standing challenge in automating netlist generation for analog circuits within circuit design automation.\nAutomating this workflow could accelerate the creation of finetuned LLMs for analog circuit design and verification.\nWe identify key challenges in this automation and evaluate the multi-modal capabilities of state-of-the-art LLMs, particularly GPT-4, to address these issues. We propose a three-step workflow to overcome current limitations: labeling analog circuits, prompt tuning, and netlist verification. This approach aims to create an end-to-end SPICE netlist generator from circuit schematic images, tackling the long-standing hurdle of accurate netlist generation. Our framework demonstrates significant performance improvements, tested on approximately 2,100 schematics of varying complexity. We open-source this solution for community-driven development. \nI Introduction\n Large Language Models (LLMs) have received significant attention due to their wide-ranging applications, from text summarization to code generation, and have a growing impact across various fields. For hardware design, LLMs have primarily demonstrated potential in the digital domain.\nThis includes tasks such as Verilog code generation\u00a0[1, 2, 3, 4], assertion generation\u00a0[5, 6], bug fixing\u00a0[7, 8], and electronic design automation (EDA) tool scripting\u00a0[9, 10].\nThe success of these domain-tailored LLMs relies on access to large and high-quality datasets. For instance, 75K Verilog files from GitHub were used to train the Verigen code generation model\u00a0[4].\nData scraped from Verilog textbooks was also used to improve the performance of LLMs for Verilog\u00a0[4].\nTextbooks have also proven useful in other domains, for instance, in systems biology\u00a0[11] and in understanding protein interactions\u00a0[12]. Building on the success of LLMs in the digital domain, it is natural to explore their application in the analog domain, specifically, in the automated generation of analog circuits from natural language specifications. Analog circuits\nare described in SPICE (and its many variants), the industry-standard textual representation for simulating analog circuits. SPICE is a low-level description that defines the interconnections between analog components like resistors, capacitors, inductors, and transistors.\nOpen-source datasets for SPICE, unfortunately, are very limited compared to Verilog code, creating a gap in research in this area\u00a0[13, 14, 15]. However, analog circuit textbooks and research papers are plentiful and contain a trove of analog circuit diagrams,\nbut these are usually in image (or figure) format. These images must then be manually converted to SPICE netlists, which is painstaking and time-consuming. To address this issue, a recently proposed method, AMSNet\u00a0[13], introduced a semi-automated way to create a SPICE dataset from analog circuit figures using the capabilities of multi-modal LLMs like GPT-4o that take both image and text inputs and produce both image and text outputs. However, AMSNet still requires manual annotations of nets to obtain accurate SPICE netlists.\nEven with manual annotations, AMSNet still fails to extract accurate netlists of complex circuits (the reasons for this are described in Section\u00a0III). Contributions: Auto-SPICE is an automated method\nfor SPICE netlist generation from analog circuit schematics. We incorporate various techniques, including (i) fine-tuned object detectors to extract passive and active components, (ii) deep Hough transform priors to identify nets, (iii) extensive prompt tuning for LLMs to fix common errors in netlist extraction, and (iv) post-extraction verification.\nWe make four contributions: An empirical case study to understand the limitations of state-of-the-art multi-modal LLMs like GPT-4o (shown in AMSNet\u00a0[13]) in SPICE netlist extraction from schematics. Build Auto-SPICE, the first fully automated framework using LLMs along with custom-trained deep network models for large-scale SPICE netlist extraction from circuit schematics present in textbooks and papers. Use Auto-SPICE to collect a dataset of \u223csimilar-to\\sim\u223c2,100 SPICE netlists (along with metadata like figure captions) from a sample textbook. We open-source our flow to the community to further research in this area\u00a0[16]. Fine-tune GPT models using the Auto-SPICE dataset to demonstrate the potential use-case of the dataset in automated SPICE netlist generation from English prompts. \nII Related Work\n LLMs have gained significant attention in chip design\u00a0[17]. Significant progress has been made in improving Verilog code generation\u00a0[1, 2, 3, 4], with studies introducing new methodologies that enhance the quality of generated Verilog code. These advances demonstrate how LLMs can streamline and improve digital hardware design workflows. In addition to code generation, prompt engineering has proven effective in chip design\u00a0[18, 19, 20]. By using LLMs, researchers have conceptualized and designed complex digital hardware architectures efficiently, facilitating faster and more accurate chip development. Beyond code generation, LLMs have found applications in assistant chatbots, script generation, and bug analysis\u00a0[10]. Similarly,\u00a0[9] explores LLMs in planning and executing tasks in the Electronic Design Automation (EDA) flow.\nThe use of LLMs in generating assertions and testbenches for verifying the correctness of Integrated Circuit (IC) designs has also seen notable improvements\u00a0[5, 6, 8, 7]. [21, 22] have explored schematic-to-netlist generation using ML-based approaches. However, these solutions are not reproducible due to the unavailability of any open-source implementation.\nRecent advancements have extended LLM applications into analog design. Closed-loop Python code generation for analog circuits using LLM agents has shown promising results\u00a0[14]. LLMs have also demonstrated the ability to generate various circuit topologies from given specifications\u00a0[23]. Furthermore, a dataset for exploring SPICE netlist generation was recently released in\u00a0[13]. However, existing approaches require manual modifications of circuit schematics, which significantly limits their scalability for large-scale applications. \nIII Key Challenges in SPICE Netlist Extraction\n Despite advances in multi-modal LLM, as demonstrated by AMSNet\u00a0[13], current solutions like GPT-4o do not automatically extract accurate SPICE netlists from circuit schematics.\nOur work begins with an in-depth analysis of the specific failure modes encountered by GPT-4o. Building on these insights, we propose solutions to overcome these limitations, advancing the field toward a fully automated SPICE netlist extraction process that significantly reduces dependence on manual annotations. \nIII-A Can GPT-4o detect electrical components accurately?\n When generating a SPICE netlist, accurately identifying all components is essential for designers but challenging for language models. To evaluate GPT-4o\u2019s ability to recognize electrical components in circuit schematics, we curated a set of analog schematics featuring resistors, capacitors, inductors, MOSFETs, and various sources from a popular analog textbook\u00a0[24]. As illustrated in Fig.\u00a01, GPT-4o often misclassifies components, confusing NMOS and PMOS transistors (left and middle schematics) and even omitting key elements such as PMOS transistors and current sources in some cases. However, GPT-4o demonstrates the ability to identify all components accurately in some cases (final schematic), revealing promise despite its inconsistencies. Solution 1: Electrical Component Detection using Object Detection Networks -\nRecognizing the limitations of GPT-4o in this task, we propose a dedicated solution using a CNN-based object detection model, using the strong performance of models such as YOLO\u00a0[25]. By training a CNN model specifically designed for schematic components, we significantly improve the accuracy in detecting and bounding all circuit elements, a method detailed in more detail in Section\u00a0IV. Solution 2: Prompt Tuning -\nWe developed a targeted prompt enhancement strategy to refine LLM component differentiation, especially between NMOS and PMOS transistors. By emphasizing structural differences in prompts (see Fig.\u00a02), we achieved notable improvements in accuracy. Our empirical findings support this approach in Section\u00a0IV. \nIII-B Can GPT-4o Properly Connect Circuit Components?\n Once all components are detected, the next step is ensuring that the components are connected correctly so the resulting netlist accurately reflects the schematic. While GPT-4o successfully maps 2-terminal devices, our study uncovered several critical failure modes that compromise the accuracy of netlist generation. These failure modes are illustrated in\u00a0Fig.\u00a03.\nFirstly, GPT-4o incorrectly assumes that intersecting nets are connected, even when no connection exists. This error arises partly from inconsistencies in how different schematic notations define connectivity.\nSecondly, GPT-4o frequently mixes up MOSFET terminals (drain, gate, source). Despite the drain and source\u2019s electrical equivalence, this misidentification disrupts the final netlist\u2019s correctness.\nFinally, differential input and output voltage pairs pose significant challenges for GPT-4o. For example, in the left schematic (Fig.\u00a03), while components were correctly identified, the source connections for (M1 and M2) were incorrect, and the gate voltage for (M3 and M4) was wrongly assumed to be \u2018Vb1\u2019. Similar errors were observed in the right schematic, where differential output was mishandled, and the drain of M5 was incorrectly mapped. Solution 1: Prompt Tuning -\nWe enhanced the input prompts by explicitly specifying critical design features such as differential pairs and diode-connected topologies. This approach improved GPT-4o\u2019s ability to map terminals to their corresponding nets for simpler designs. However, the solution\u2019s scalability was limited, as it struggled with larger schematics containing more complex net connections. Solution 2: Automatic Net Annotation -\nLeveraging our knowledge of component locations, we developed a framework to annotate nets with unique identifiers. By incorporating these annotations into the prompt (see Section\u00a0IV), GPT-4o\u2019s performance in translating schematics to SPICE netlists improved significantly. This systematic annotation reduced ambiguities and enhanced the LLM\u2019s ability to correctly interpret and connect components, particularly in complex schematics. \nIV Methodology \n This section details the methodology developed in this study, addressing the significant challenges outlined in\u00a0Section\u00a0III. Specifically, the accurate generation of SPICE netlists remains a critical bottleneck for LLMs due to difficulties in correctly identifying circuit components and establishing net connections.\nTo begin the dataset creation flow, as shown in\u00a0Fig.\u00a04 (\u278a), it takes a document in PDF format. After that, it is subdivided into 2 parts: (I) Extraction/Summarization of text present in the document comprises of the description of schematic and Key Performance Indicators (KPI) like gain, linearity, bandwidth, noise, gain/phase margin, etc - This will be used as an additional context for fine-tuning LLMs,\u00a0Section\u00a0V-D, shown in\u00a0Fig.\u00a04 (\u278f - \u2790);\n(II) Automatic SPICE Netlist Generation - Auto-SPICE: This step starts with extraction of the schematics/image from the document, then goes through the three-step process: 1) Labeling Analog Circuit, 2) Prompt Tuning, and 3) SPICE netlist verification, to finally generate a SPICE netlist, shown in\u00a0Fig.\u00a04 (\u278b - \u278e). \nIV-A Labeling Analog Circuit\n Fig.\u00a04 (\u278b) step eases the job of the LLM in writing the SPICE netlist from schematics by detecting components and enabling net annotation. As discussed in Section\u00a0III, SPICE netlist generation is not straightforward, requiring the designer to feed information alongside the schematics. However, this manual process hinders generating large-scale datasets. Thus, we automate labeling all parts of the schematics. We train YoloV8\u00a0[25], a state-of-the-art object localization and classification model, specifically for detecting and classifying circuit components with high precision, similar to prior work\u00a0[21, 22].\nGiven an input circuit image, YoloV8 localizes each component by regressing its center coordinates, bounding box parameters, and classification label. The model\u2019s backbone, CSPDarknet53\u00a0[26], enhanced with a Cross Stage Partial bottleneck\u00a0[27], effectively integrates high-level semantic information with low-level spatial details, enabling accurate detection even for small-scale components \u2014 a critical requirement for high-performance circuit analysis. This process results in schematic diagrams annotated with bounding boxes for all detected components, as illustrated in\u00a0Fig.\u00a05 (\u278b).\nWe utilize YoloV8 on the open-sourced dataset\u00a0[28]. This dataset comprises approximately 4,300 circuit diagram snapshots annotated with bounding boxes across 12 component classes: AC Source, BJT, Battery, Capacitor, DC Source, Diode, Ground, Inductor, MOSFET, Resistor, Current Source, Voltage Source.\nWe resized images to 640x640 and trained for 1000 epochs with a learning rate of 0.01. We performed net detection in circuit schematics by leveraging pre-trained Deep Hough Transform Line priors\u00a0[29], a state-of-the-art approach for line detection in images. Unlike conventional methods, this model operates in the Hough domain, parametrizing line segments in polar coordinates to achieve precise line segmentation. We remove components from the schematic to address challenges posed by line segments within components such as capacitors and MOSFETs, leaving only the lines representing nets, as illustrated in Fig.\u00a05 (\u278c). This ensures a clean input for subsequent net analysis and eliminates potential sources of ambiguity.\nFollowing net detection, we propose a simple yet effective heuristic for clustering line segments into nets. Specifically, we group all line segments into a single cluster if their endpoints fall within a radius of 40 pixels, as demonstrated in Fig.\u00a05 (\u278d). To ensure reliability, we fine-tune the radius parameter and manually verify the consistency of the clustered nets. \nIV-B Prompt Tuning\n Prompts are crucial in guiding LLMs to generate precise, high-quality outputs. Crafting effective prompts involves meticulous design of their language, structure, and contextual framing, enabling models to respond accurately and meaningfully to user inputs. Through prompt refinement, designers can precisely control model behavior, ensuring the generation of coherent and domain-specific responses.\nIn this study, we systematically explore diverse prompt designs tailored to enhance understanding of schematics. Our approach demonstrates significant effectiveness, as illustrated in Fig.\u00a02 and Fig.\u00a06. Specifically, prompt in Fig.\u00a06 is used at the stage\u00a0Fig.\u00a04 (\u278c), facilitating insightful queries to the GPT model. \nIV-C SPICE netlist verification\n Our case study revealed that some generated netlists (Fig.\u00a04 (\u278d)) exhibit issues such as floating nets. Although these issues may pass a simulator check, they pose significant risks for large-scale datasets where manual inspection is infeasible. To mitigate this, we developed a Python-based feedback mechanism (Fig.\u00a04 (\u278e)) that enables LLMs to identify and self-correct such errors automatically, eliminating the need for manual intervention. This solution establishes a robust, closed-loop verification process, which can be further enhanced with additional netlist verification features. We are also releasing this as part of our open-source contribution\u00a0[16]. \nV Results\n \nV-A Dataset Creation\n We curated a comprehensive dataset by selecting schematics from textbooks\u00a0[24, 30]. Textbooks were chosen over research papers because they offer high-quality schematic images, clear context, and detailed explanations. This makes them more suitable for our purposes, as research papers often present high-level descriptions with advanced schematics that use abstract blocks for complex components.\nWe collected over 2,100 schematic images and applied our proposed Auto-SPICE flow (Fig.\u00a04) to generate the corresponding SPICE netlists. Alongside this, we collected captions and descriptions as detailed in Section\u00a0IV. Fig.\u00a07 demonstrates how Auto-SPICE translates a schematic into a SPICE netlist. Since our primary goal is to generate structurally correct netlists, each component is marked with default parameters. Our dataset is carefully characterized to showcase its diversity and complexity, as shown in Fig.\u00a08: (a) displays the variation in the number of components within the schematics, reflecting the complexity of the circuits - a higher number indicates a more complex design; (b) illustrates the distribution of nodes in the schematics, which corresponds to the connectivity between components. Notably, accurately identifying and mapping MOSFET terminals to the correct nets is a significant challenge for LLMs; (c) focuses on the number of MOSFETs present, indicating increased complexity for SPICE generation as the number rises; (d) presents the distribution of the number of lines in the generated SPICE netlists, further emphasizing the varying levels of complexity encompassed in our dataset.\nBy creating this diverse and challenging dataset, we provide a robust benchmark for evaluating and enhancing the capabilities of LLMs in automated SPICE netlist generation. \nV-B Evaluation\n To evaluate the performance of our proposed method, we leverage graph-matching concepts to compare the structural similarity between SPICE netlists. Unlike traditional approaches, we parse the netlists to extract components and their connections. We identify components by their types (e.g., Resistor, Capacitor, MOSFET) to ensure that our graph representations remain invariant to varying component labels used during netlist generation. This results in a constructed graph \u201cG\u201d, where nodes represent circuit components and edges represent their interconnections, as illustrated\u00a0Fig.\u00a09.\nWe use Graph Edit Distance (GED) as a quantitative metric to compare the representations of two SPICE netlists. GED (G1subscript\ud835\udc3a1G_{1}italic_G start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT,G2subscript\ud835\udc3a2G_{2}italic_G start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT) calculates the sum of the costs associated with transforming G1subscript\ud835\udc3a1G_{1}italic_G start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT into G2subscript\ud835\udc3a2G_{2}italic_G start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT through a series of edit operations on nodes and edges, such as insertions, deletions, and substitutions.\nTo enhance the interpretability of our results, we normalize the GED to yield a final metric ranging from 0 to 100. This normalization is achieved by calculating the maximum possible GED, obtained by summing the total number of nodes and edges in both graphs and scaling accordingly. This standardized metric allows for a consistent and meaningful comparison across different circuits.\nThus, the similarity score between the two netlists is: \nV-C Verifying against AMSNet\u00a0[13] netlists\n We demonstrated the effectiveness of our approach by evaluating it using the AMSNet\u00a0[13] dataset, which includes \u223csimilar-to\\sim\u223c800 circuit diagrams. Unlike AMSNet, which relies on an algorithmic method supplemented with some manual effort to generate SPICE netlists, our method introduces a fully automated flow (Fig.\u00a04). When comparing the SPICE netlists generated by our method to those produced by AMSNet, our approach achieved a 100% similarity score, as defined earlier. This validates the robustness of our flow and motivates us to extend the evaluation to additional circuit diagrams. Fig.\u00a09 illustrates an example\nof graphs representation of SPICE netlist generated by AMSNet (\u278a) and our Auto-SPICE framework (\u278b).\nThe results demonstrate that the number of components and their neighboring elements is identical across both methods, with differences in component and net naming conventions. These findings strengthen Auto-SPICE accuracy and scalability in generating accurate netlists for various circuit diagrams. \nV-D Finetuning\n Finetuning entails adapting a pre-trained model to a specific downstream task\u00a0[4, 3]. This process entails further training the model on a smaller task-specific dataset, enabling it to adjust its parameters to capture new patterns and subtle nuances relevant to the task without\ntraining from scratch. The advantages of fine-tuning are: 1) It substantially reduces training time and computational resources by leveraging the foundational knowledge already embedded in the model; 2) It enhances performance on specialized tasks, particularly when data is scarce; and 3) It facilitates the transfer of learned representations across different domains, thereby improving efficiency and effectiveness in model deployment. GPT: OpenAI\u00a0[31] released GPT models up to the state-of-the-art GPT-4o model, which is expensive. Fine-tuning\u00a0[32] reduces cost and latency by replacing the expensive GPT-4o, with a fine-tuned GPT-4o-mini model. The fine-tuned GPT-4o-mini can achieve quality results similar to those of GPT-4o. Table\u00a0I reports preliminary results of fine-tuning the GPT\u2019s models on the Auto-SPICE datasets generated. We picked 20 common design problems frequently used as a standalone circuit or sub-part of a bigger design. We prompted the LLM to generate (n = 10) samples of SPICE netlist for each circuit description and manually validated their correctness (the score reported is the number of correct SPICE netlists).\nWe subdivided the 20 benchmarks into 3 different categories: Easy (ID=1-7), Medium (ID=8-14), and Hard (ID=15-20).\n\u00a0Table\u00a0I provides a comprehensive evaluation of the performance of GPT-3.5-turbo and GPT-4o-mini, both in their \u2018base\u2019 and \u2018finetune\u2019 states, on a diverse set of circuit design tasks.\nFinetuning demonstrates significant performance gains across nearly all circuits, particularly for complex designs. For instance, in the (ID=8), GPT-3.5-turbo improves from a score of 4 to 10. At the same time, GPT-4o-mini achieves a perfect score of 10 after fine-tuning, demonstrating the impact of domain-specific training. This pattern is evident across other tasks such as the (ID=5), where fine-tuning boosts GPT-3.5-turbo from 5 to 9 and GPT-4o-mini from 6 to 10. The results reveal an important trend: simple circuits, such as the (ID=1) and (ID=3), are well-handled even by the base models. More advanced circuits like the (ID=14) and (ID=17) require fine-tuning to achieve meaningful results. Notably, finetuning enables GPT-4o-mini to score 8 on the (ID=10), doubling its base score of 4, illustrating the model\u2019s scalability for complex tasks. GPT-4o-mini consistently outperforms GPT-3.5-turbo in handling advanced designs, achieving superior scores across several tasks. Table\u00a0I reveals significant challenges, particularly with specialized circuits like the (ID=19) and (ID=20), where both models exhibit minimal gains even after finetuning.\n\u00a0Fig.\u00a010 showcases representative examples of both successful and failed netlists generated by LLMs, illustrating critical design challenges and the effectiveness of our approach. Notably, in Fig.\u00a010: (1) demonstrates the improper use of an NMOS as a current source rather than forming the intended cascode configuration; (2) highlights a fundamental error where the gate of a diode-connected transistor, expected to connect to the drain, is incorrectly connected to the source; (3) the complexity of a two-stage design further amplifies the difficulties in understanding and replicating the intended topology.\nThese examples highlight the inherent limitations of unoptimized LLMs. Through the \u2018Finetune\u2019 GPT-4o-mini model, we successfully resolved these issues, achieving accurate connections and eliminating floating nets.\nThis demonstrates the value of fine-tuning in enabling reliable and context-aware netlist generation. \nVI Conclusion and Future Work\n In this paper, we introduced Auto-SPICE, a novel framework to create large-scale datasets for SPICE netlists from documents comprising analog circuit schematics, addressing critical challenges in the design workflow. Through schematic preprocessing and prompt tuning, we developed a fully automated pipeline that leverages LLMs for scalable netlist generation. To the best of our knowledge, Auto-SPICE is the first LLM-based framework of its kind, and our released dataset and flow\u00a0[16] provide a valuable resource for the community for future advancements. Future work will focus on enhancing the framework with automated parameter tuning to ensure functionally validated netlists. Additionally, we aim to extend automation to the layout generation stage, addressing its inherent challenges by curating comprehensive datasets and fine-tuning LLMs for this task. These advancements will further streamline the analog design process, bridging the gap from schematic to GDSII and unlocking new efficiencies in IC design workflows. References"}
{"text": "CKTSO: High-Performance Parallel Sparse Linear Solver for General Circuit Simulations This paper introduces CKTSO (abbreviation of \u201ccircuit solver\u201d), a novel sparse linear solver specially designed for the simulation program with integrated circuit emphasis (SPICE). CKTSO is a parallel solver and can be run on a multi-core, shared-memory computer. The algorithms of CKTSO are designed by considering the features of matrices involved in SPICE simulations. CKTSO is superior to existing similar solvers mainly in the following three aspects. First, the matrix ordering step of CKTSO combines different types of ordering algorithms such that it can generally obtain the fewest fill-ins for a wide range of circuit matrices. Second, CKTSO provides a parallel fast LU factorization algorithm with pivot check, which behaves good performance, scalability, and numerical stability. Third, CKTSO provides a structure-adaptive hybrid parallel triangular solving algorithm, which can adapt to various circuit matrices. Experiments including both benchmark tests and SPICE simulations demonstrate the superior performance of CKTSO. The libraries of CKTSO are available at https://github.com/chenxm1986/cktso. \nI Introduction\n The simulation program with integrated circuit emphasis (SPICE)\u00a0[1] is a fundamental electronic design automation (EDA) kernel. SPICE utilizes numerical computation theories to find the detailed responses of integrated circuits. A SPICE simulation process involves solving a series of sparse linear systems. Fig.\u00a01 illustrates a typical transient simulation flow. There are two nested levels of loops, where the outer loop iterates the time and a Newton-Raphson (NR) iteration is performed in each inner loop.\nIn each NR iteration, a sparse linear system (\ud835\udc00\ud835\udc31=\ud835\udc1b\ud835\udc00\ud835\udc31\ud835\udc1b{\\bf Ax}={\\bf b}bold_Ax = bold_b where \ud835\udc00\ud835\udc00\\bf Abold_A is sparse) is solved. The linear solver is usually the performance bottleneck of SPICE simulators. In post-layout simulations, the linear solver can spend more than half of the total simulation time\u00a0[2]. SPICE simulators generally use LU factorization\u00a0[3] to solve sparse linear systems, as linear systems in SPICE simulations are usually not well conditioned. As shown in Fig.\u00a01, sparse LU factorization involves three main steps: pre-processing, numerical factorization, and triangular solving. Pre-processing reorders the matrix to minimize fill-ins that will be generated during numerical factorization. It is performed only once if the matrix structure is fixed, which usually holds in SPICE simulations. In each NR iteration, the matrix is factorized into LU factors: \ud835\udc00=\ud835\udc0b\ud835\udc14\ud835\udc00\ud835\udc0b\ud835\udc14{\\bf A}={\\bf LU}bold_A = bold_LU, where \ud835\udc0b\ud835\udc0b\\bf Lbold_L is a lower-triangular matrix and \ud835\udc14\ud835\udc14\\bf Ubold_U is an upper-triangular matrix. Finally, the solution \ud835\udc31\ud835\udc31\\bf xbold_x is obtained by solving two triangular systems, \ud835\udc0b\ud835\udc32=\ud835\udc1b\ud835\udc0b\ud835\udc32\ud835\udc1b{\\bf Ly}\\!=\\!\\bf bbold_Ly = bold_b and \ud835\udc14\ud835\udc31=\ud835\udc32\ud835\udc14\ud835\udc31\ud835\udc32{\\bf Ux}\\!=\\!\\bf ybold_Ux = bold_y. Numerical factorization and triangular solving are executed in every NR iteration. Typically, numerical factorization spends more time than triangular solving. People have developed several LU factorization-based sparse solvers to accelerate circuit simulations. KLU\u00a0[4], developed in 2006, is a sequential implementation of the sparse left-looking algorithm\u00a0[5]. NICSLU\u00a0[6, 7], developed in 2013, parallelizes the sparse left-looking algorithm at the column granularity. Later in 2016, Basker\u00a0[8] is developed, which partitions the matrix and parallelizes sparse LU factorization at the sub-matrix granularity. By exploring parallelism in the sparse left-looking algorithm, both NICSLU and Basker achieve speedups relative to KLU. PARDISO\u00a0[9], which is a general-purpose sparse direct solver, has also shown good performance in semiconductor device simulations\u00a0[10]. Despite the good performance of these solvers, it has been found that they have issues when facing some type of circuits. Various circuits put high demands for every step of LU factorization-based sparse linear solvers. Challenge 1: The popular matrix ordering methods, minimum degree and its variants\u00a0[11, 12, 13], which are adopted by KLU and NICSLU, usually do not perform well on post-layout, mesh-style, or very large circuits. Instead, nested dissection\u00a0[14] generally obtains better orderings on such matrices. However, nested dissection performs badly on pre-layout or small circuits. Challenge 2: The high sparsity of circuit matrices leads to low parallelism and a low computation-to-communication ratio in numerical LU factorization, which is difficult to be efficiently parallelized on multi-core processors. Challenge 3: For triangular solving, the parallelism and computation-to-communication ratio are both extremely low, and it is extremely difficult to get speedups by parallelism. NICSLU provides parallel factorization but the solving step is sequential. My previous work proposed a fast numerical factorization algorithm\u00a0[15] for circuit matrices, which addresses Challenge 2 and improves the performance and scalability of parallel LU factorization relative to NICSLU. This paper is extended from [15] and introduces a complete solver, CKTSO (abbreviation of \u201ccircuit solver\u201d), to accelerate circuit simulations. CKTSO is developed by redesigning the three steps of sparse solvers for SPICE simulators, to address all the above-mentioned challenges. CKTSO has the following three unique features. \u2460 The matrix ordering of CKTSO includes both minimum degree and nested dissection methods, and the best ordering is selected after trying them. \u2461 For numerical factorization, CKTSO exploits the unique features of matrices in circuit simulation iterations and proposes a novel pivoting reduction-based fast LU factorization algorithm, which behaves good scalability and numerical stability. \u2462 For triangular solving, CKTSO proposes a novel structure-adaptive hybrid parallelism strategy. Feature \u2461 inherits from [15] and features \u2460 and \u2462 are new relative to [15]. The three features are also the major technical advances of CKTSO relative to NICSLU. The contributions of this work are summarized as follows. A parallel fast LU factorization algorithm is proposed, which combines the advantages of both factorization with pivoting and re-factorization without pivoting, and behaves good performance and scalability, as well as good numerical stability. A structure-adaptive hybrid parallel triangular solving method is proposed. It adaptively splits the triangular matrices (i.e., \ud835\udc0b\ud835\udc0b\\bf Lbold_L and \ud835\udc14\ud835\udc14\\bf Ubold_U) and uses different parallel triangular solving strategies, according to the characteristics of the symbolic structures of the sub-matrices. Experimental results have revealed that CKTSO achieves better performance for a wide range of circuit matrices than stat-of-the-art solvers. In practical SPICE simulations, CKTSO also shows better performance in different simulation scenarios. \nII Background\n \nII-A Sparse Up-looking LU Factorization\n The sparse left-looking LU factorization\u00a0[5] is widely adopted by linear solvers for SPICE simulations (e.g., KLU\u00a0[4], NICSLU\u00a0[6, 7], and Basker\u00a0[8]). It computes the LU factors column by column. CKTSO uses the row-major order, so the left-looking algorithm is transposed and can be named as sparse up-looking LU factorization, as shown in Algorithm\u00a01. It computes the LU factors row by row (line 1). As its name implies, when computing a row, some rows on its upper side will be \u201clooked at\u201d (used). When computing a row, symbolic prediction (line 2), numerical update (lines 3-5), and pivoting (lines 6-7) are executed. Since pivoting may change the structure of the LU factors, symbolic prediction cannot be decoupled from numerical computation. Symbolic prediction (line 2) determines the symbolic structure of the current row, based on the already computed rows on the upper side\u00a0[5, 4]. The symbolic structure of \ud835\udc0b(i,1:i\u22121){\\bf L}(i,1:i-1)bold_L ( italic_i , 1 : italic_i - 1 ) also determines which rows on the upper side will update the current row. Numerical update (lines 3-5) uses these dependent rows to update the values of the current row. The core operation of numerical update is a series of floating-point multiply-and-accumulates (MACs) (line 5). After that, pivoting (lines 6-7) is performed to ensure large diagonal elements. When the diagonal element is smaller than the maximum element in the current row of \ud835\udc14\ud835\udc14\\bf Ubold_U times the pivoting threshold (\u03b5\ud835\udf00\\varepsilonitalic_\u03b5), the diagonal element and the largest element in the current row of \ud835\udc14\ud835\udc14\\bf Ubold_U are exchanged (line 7). The above described flow is a complete sparse up-looking LU factorization with pivoting. Before LU factorization, if the diagonal elements are assumed to be large enough, pivoting can be skipped. As a result, the symbolic structure of the LU factors will not change, and symbolic prediction can also be eliminated. In this case, the algorithm becomes re-factorization without pivoting, which only performs numerical update for each row. Re-factorization reuses the symbolic structure and pivoting order obtained in the last factorization with pivoting. If one or more small diagonal elements that do not meet the pivoting rule appear in re-factorization, they cannot be handled and may cause large errors in the solution. \nII-B Parallel Sparse Up-looking LU Factorization\n To perform parallel up-looking LU factorization, dependencies between rows should be first determined. The sparsity of the LU factors implies the inter-row parallelism. Lines 4-5 of Algorithm\u00a01 indicate that row i\ud835\udc56iitalic_i depends on row j\ud835\udc57jitalic_j (i.e., row i\ud835\udc56iitalic_i needs row j\ud835\udc57jitalic_j\u2019s values for numerical update) if and only if \ud835\udc0b\u2062(i,j)\ud835\udc0b\ud835\udc56\ud835\udc57{\\bf L}(i,j)bold_L ( italic_i , italic_j ) is a nonzero element. In short, the symbolic structure of \ud835\udc0b\ud835\udc0b\\bf Lbold_L determines the inter-row dependencies. Based on this principle, a dependency graph can be built based on the symbolic structure of \ud835\udc0b\ud835\udc0b\\bf Lbold_L. There is an edge j\u2192i\u2192\ud835\udc57\ud835\udc56j\\to iitalic_j \u2192 italic_i in the dependency graph if \ud835\udc0b\u2062(i,j)\ud835\udc0b\ud835\udc56\ud835\udc57{\\bf L}(i,j)bold_L ( italic_i , italic_j ) is a nonzero element. This dependency graph can be named as an elimination graph (EGraph). The EGraph describes exact inter-row dependencies corresponding to a specific symbolic structure of the LU factors, and can be used to schedule parallel LU re-factorization without pivoting\u00a0[7]. For parallel factorization with pivoting, however, it is impossible to determine the exact inter-row dependencies prior to factorization, since the symbolic structure of \ud835\udc0b\ud835\udc0b\\bf Lbold_L depends on pivoting and is known only after factorization is completed. To solve this dilemma, people have proposed a concept of elimination tree (ETree)\u00a0[16], which describes an upper bound of the inter-row dependencies when considering pivoting. The meaning of the upper bound is that, the dependencies generated by any pivoting order of up-looking factorization are contained in the ETree. The ETree is constructed from the symbolic structure of matrix \ud835\udc00\ud835\udc00\\bf Abold_A (if \ud835\udc00\ud835\udc00\\bf Abold_A is unsymmetric, the ETree is built from \ud835\udc00\ud835\uddb3\u2062\ud835\udc00superscript\ud835\udc00\ud835\uddb3\ud835\udc00{\\bf A}^{\\mathsf{T}}{\\bf A}bold_A start_POSTSUPERSCRIPT sansserif_T end_POSTSUPERSCRIPT bold_A without explicitly forming \ud835\udc00\ud835\uddb3\u2062\ud835\udc00superscript\ud835\udc00\ud835\uddb3\ud835\udc00{\\bf A}^{\\mathsf{T}}{\\bf A}bold_A start_POSTSUPERSCRIPT sansserif_T end_POSTSUPERSCRIPT bold_A\u00a0[16]), so it can be built in the pre-processing step. The EGraph and ETree are both directed acyclic graphs, as illustrated in Fig.\u00a02. The parallelism implied in EGraph and ETree can be explored by levelizing them, so that parallel factorization and re-factorization can both be scheduled by a unified method\u00a0[6, 7]. As shown in Fig.\u00a03, the EGraph or ETree can be levelized (Fig.\u00a03(a)) so that nodes (a node corresponds to a row) in the same level have no dependency. The level of a node is the maximum path length from any source node to itself. A dual-mode scheduling method has been proposed\u00a0[6, 7]. For front levels that have a large number of nodes in each level, they are computed level by level. Nodes in a level are evenly assigned to threads and computed in parallel. This method is called a cluster mode. For the remaining levels that have very few nodes in each level, a pipeline mode is proposed which explores finer-grained parallelism between dependent rows. Fig.\u00a03(d) illustrates an example of computing node 10 using the pipeline mode, corresponding to the dependencies described in Fig.\u00a03(a). Nodes 9 and 10 are computed in parallel in the pipeline mode. Since node 10 depends on node 9, node 10 can first use already-finished nodes (i.e., nodes 7 and 8) to perform partial updates. These partial updates are performed in parallel with the computation of node 9. This is the key to explore parallelism between dependent rows. After node 9 is finished, node 10 can use node 9\u2019s results to perform a partial update. The levelization-based dual-mode scheduling method has been adopted by NICSLU. The height and width of the ETree/EGraph can be an intuitive estimation of the scalability of parallel LU factorization/re-factorization. The height (maximum level) is the critical path length and the width (maximum number of nodes in each level) is the maximum parallelism. As illustrated in Fig.\u00a02, the ETree is tall and narrow, while the EGraph is short and wide. As the ETree describes an inter-row dependency upper bound, it implies many redundant dependencies. We have tested more than 50 circuit matrices from the SuiteSparse Matrix Collection\u00a0[17]. On average, the ETree is nearly 80\u00d7\\times\u00d7 taller than the EGraph, while the EGraph is 10\u00d7\\times\u00d7 wider. Therefore, the scalability of parallel factorization with pivoting (scheduled by the ETree) is much poorer than that of re-factorization without pivoting (scheduled by the EGraph). According to the results in Ref.\u00a0[18], for NICSLU, 16-threaded factorization with pivoting achieves only 2\u00d7\\times\u00d7 speedup on average, compared with sequential factorization. \nII-C Triangular Solving"}
{"text": "Dissecting Conditional Branch Predictors of Apple Firestorm and Qualcomm Oryon for Software Optimization and Architectural Analysis Branch predictor (BP) is a critical component of modern processors, and its accurate modeling is essential for compilers and applications. However, processor vendors have disclosed limited details about their BP implementations. Recent advancements in reverse engineering the BP of general-purpose processors have enabled the creation of more accurate BP models. Nonetheless, we have identified critical deficiencies in the existing methods. For instance, they impose strong assumptions on the branch history update function and the index/tag functions of key BP components, limiting their applicability to a broader range of processors, including those from Apple and Qualcomm. In this paper, we design a more general branch prediction reverse engineering pipeline that can additionally recover the conditional branch predictors (CBPs) of Apple Firestorm and Qualcomm Oryon microarchitectures, and subsequently build accurate CBP models. Leveraging these models, we uncover two previously undisclosed effects that impair branch prediction accuracy and propose related solutions, resulting in up to 14% MPKI reduction and 7% performance improvement in representative applications. Furthermore, we conduct a comprehensive comparison of the known Intel/Apple/Qualcomm CBPs using a unified standalone branch predictor simulator, which facilitates a deeper understanding of CBP behavior. \n1. Introduction Modern processors heavily rely on branch prediction to sustain high instruction throughput. To achieve this, the branch predictor (BP) records recent branches and predicts the direction and target address of the current one well before instruction decoding and execution (combining, ; study, ). Given that branch instructions constitute approximately 15% to 30% of the total number of executed instructions (reduction, ), the design of accurate BPs and software optimization based on accurate BP modeling are of significant importance. Although the general structure of the branch predictor (BP) is well-known and has been published by some CPU vendors (zen2analysis, ), the detailed behavior, including the characteristics of branch history recording, the algorithm for conditional branch prediction, and the capacity of prediction tables (if any), remains largely undocumented. While limited performance monitoring counters (PMCs) can indicate which branches are most frequently mispredicted, programmers may still struggle with optimization, as the BP operates as a black box. In contrast, with an accurate model of the BP of commercial processors, we can simulate the application using the model and pinpoint the actual sources of mispredictions. Recently, several studies (spectre, ; halfhalf, ; trustzonetunnel, ) have successfully reverse-engineered the conditional branch predictor (CBP) of Intel and ARM Cortex processors. However, our analysis reveals that their reverse engineering pipelines rely on strong assumptions that do not hold for other processors. For instance, the existing method is tailored to the relatively simple PHT index and tag functions of Intel and assumes that the path history register (PHR) can be easily cleared to zero. In contrast, Apple Firestorm (Apple M1 P-core) and Qualcomm Oryon (in Qualcomm X Elite) exhibit much greater complexity in these aspects, rendering the existing method inapplicable. Therefore, we propose a new and more general pipeline to additionally reverse-engineer the CBPs of Apple Firestorm and Qualcomm Oryon. Specifically, we design new microbenchmarks with minimal assumptions about CBP internals and develop a comprehensive approach to recover all possible combinations of bits in the index/tag functions of all pattern history tables (PHT). Based on this proposal, we recover the novel branch history organization used in these two microarchitectures, as well as the six pattern history tables in their TAGE branch predictor. Furthermore, we reveal the index and tag functions of the PHTs and the set-associative structure of each table. Notably, we identify some hidden PMCs that record conditional branch mispredictions in Oryon, enabling the isolation of CBP from other BP components, which is a prerequisite for the viability of the aforementioned microbenchmarks. Utilizing the reverse engineered results, we construct an accurate CBP model and use it to locate the root cause of two previously undisclosed effects, named Scatter and Annihilation, which contribute to increased branch misprediction rates. Scatter occurs when one branch maps to an excessive number of PHT entries, while Annihilation refers to the scenario where two or more branches leave identical traces in the branch history. We also devise solutions to mitigate these effects. It is important to note that these phenomena also exist in other general-purpose processors, such as Intel. Finally, we compare the CBPs of several microarchitectures from Intel, Apple, and Qualcomm by porting them to an in-house standalone branch predictor model and evaluating their branch prediction accuracy on SPEC INT 2017 (spec2017, ) and Geekbench 5 (geekbench5, ) benchmarks. Our analysis reveals that capacity significantly impacts MPKI, far more than other microarchitecture-specific functions. We also provide microarchitectural insights on optimizing CBP accuracy and bandwidth that may guide future iterations of branch predictors for both open-source and commercial processors: incorporating additional PC bits into the PHR footprint, increasing the CBP capacity, and partitioning PHTs by one PC bit to enable the prediction of two conditional branches per cycle without a second SRAM read port.\nThe contributions of this paper are as follows: For the first time, we recover the CBPs of Apple Firestorm and Qualcomm Oryon. We design a new reverse engineering pipeline that is less restrictive and more general than existing methods. We construct a CBP model and identify two previously undisclosed effects that impair branch prediction accuracy. Using this model, we identify the root causes of these effects and enhance performance through minimal software/hardware modifications. We compare the CBPs of commercial processors in terms of their design and MPKI on SPEC INT 2017 and Geekbench 5 benchmarks, providing insights that support future branch prediction development. \n2. Background and Related Work In this section, we provide an overview of the structure of modern branch predictors and discuss recent advancements in reverse engineering conditional branch predictors. \n2.1. Branch Prediction Modern out-of-order processors typically comprise a frontend and a backend. The frontend is responsible for fetching and decoding instructions, while the backend executes and commits them. Branches are prevalent in applications (reduction, ), and accurate branch prediction is crucial for ensuring correct instruction fetching and maintaining the continuity of execution flow, which is essential for enhancing program performance. As a key functional unit, the branch predictor (BP) often includes the following components to predict branch outcomes (direction and target address) early in the frontend: The Branch Target Buffer (BTB) records branch information, including branch type and target address, for branches encountered in recent executions.\nThe Conditional Branch Predictor (CBP) predicts the direction of conditional branches, which can be either taken (T) or not-taken (NT).\nThe Indirect Branch Predictor (IBP) predicts the target addresses of indirect branches, which may jump to various addresses.\nThe Return Address Stack (RAS) predicts the return address of functions. This paper primarily focuses on the CBP. For the CBP to function effectively, the predictor may utilize local history or global history as input. Local history records the previous directions of the same branch, while global history records the directions or addresses of recent branches. The former correlates a branch with itself, whereas the latter identifies correlations between different branches. Global history is typically maintained in a Global History Register (GHR) (ghr, ), which is a shift register storing the directions of recent branches. For each executed conditional branch, the GHR is shifted by one and updated with the direction (0 for not-taken, 1 for taken) of the last branch. While the GHR scheme is simpler and less costly to implement, it cannot distinguish between different conditional branches or capture correlations between conditional and unconditional branches. An alternative global history scheme is the Path History Register (PHR) (phr, ; ogehl, ; ogehl2, ), which is also a shift register. However, instead of storing directions, it records the branch and target address of taken branches. Taken branches typically include conditional branches that are taken, unconditional jumps, indirect jumps, calls and returns. Not-taken conditional branches are not recorded in the PHR. Since storing the entire branch and target address would require significant space, the address bits are typically XOR-ed into a fixed-width footprint, which is then XOR-ed into the shifted PHR. The PHR update function is as follows: The definition of branch address varies: in Intel processors, it points to the last byte of the branch instruction, whereas in ARMv8 processors, it points to the first byte. The PHR scheme provides more detailed history but is more expensive to update and recover. For both global history schemes, the history register and the branch address are used together to predict the direction of conditional branches.\nA state-of-the-art conditional branch predictor, TAGE, was proposed by Seznec (tage, ) in 2006 and has won several branch predictor championships (cbp4, ; cbp5, ). TAGE is widely used in commercial processors, including AMD Zen 2 (zen2analysis, ), ARM Cortex-A53 (trustzonetunnel, ), and Intel Alder Lake (halfhalf, ).\nTAGE consists of multiple pattern history tables (PHTs) using different history lengths of the history register and program counter (PC) as inputs. During prediction, the PC and history register are sent to the base predictor and multiple PHTs, and the predicted direction is computed from the matching entries.\nEach PHT is a set-associative structure where each entry contains tag, counter, and useful bits. The tag and index are computed from the PC and truncated history register. If there is a tag match, the counter value predicts the direction. The history lengths of the PHT inputs L1,L2,\u22ef,Lnsubscript\ud835\udc3f1subscript\ud835\udc3f2\u22efsubscript\ud835\udc3f\ud835\udc5bL_{1},L_{2},\\cdots,L_{n}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u22ef , italic_L start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT form a geometric series. The structure of the TAGE conditional branch predictor is illustrated in Figure 1. In addition to prediction accuracy, prediction bandwidth is also critical. A simple implementation predicts at most one branch per cycle, making the branch prediction structures easier to implement with low area and high clock frequency. However, many applications execute a large fraction of branches, such as loops with small bodies, where the frontend can deliver at most one loop body\u2019s worth of instructions per cycle. Therefore, predicting more branches per cycle is necessary for better performance. A common method to improve branch prediction bandwidth is to predict two adjacent conditional branches in a cycle. If the first conditional branch is predicted as taken, the branch prediction unit redirects to the target address of the branch. Otherwise, the prediction result of the second conditional branch is examined. Since the two branches are adjacent, it is possible to save these two branches in one BTB entry to be accessed in one cycle (btb, ; exynos, ; zen1, ; zen2, ; zen3, ; zen4, ; zen5, ). However, some implementations only support predicting at most one taken branch per cycle (btb, ; exynos, ; zen1, ; zen2, ; zen3, ). AMD Zen 4 and Zen 5 support predicting two taken branches per cycle, further enhancing branch prediction bandwidth (zen4, ; zen5, ). \n2.2. Reverse Engineering Branch Prediction In 2019 Google Zero identified the Spectre and Meltdown vulnerabilities in commercial processors which exploit branch target injection (spectre, ). To inject arbitrary addresses into the branch predictor, they reverse-engineered the branch predictors in the Intel Haswell microarchitecture. By leveraging the branch prediction internals, it is possible to attack a victim process via branch target injection. Half&Half (halfhalf, ) extended the PHR reverse engineering to more recent Intel microarchitectures, from Haswell to Alder Lake. It revealed that Intel has maintained the same CBP design for many years, primarily increasing history lengths to capture longer branch histories. In addition to the PHR, Half&Half provided the first detailed implementation of the TAGE CBP in Intel processors, including the set-associative structure of pattern history tables and the index and tag functions used to access the tables. Building on Half&Half, TrustZoneTunnel (trustzonetunnel, ) reverse-engineered the CBP of the ARM Cortex-A53 CPU. The aforementioned work forms the basis for the microbenchmarks used in this paper. However, there are significant differences. For instance, Half&Half (halfhalf, ) is designed for the simple design of Intel CBP, which combines branch and target addresses into one footprint and uses only one PC bit in the index function of the PHT, along with typical PHR folding schemes in both index and tag functions.\nAdditionally, the microbenchmarks in Half&Half heavily rely on setting the PHR to known values, which is not applicable to Firestorm and Oryon. Notably, no prior work has revealed the CBPs of the Firestorm and Oryon microarchitectures. Our work addresses this gap by reverse-engineering their CBPs, including the special design of the PHR and the six pattern history tables. \n3. Reverse Engineering CBPs of Apple Firestorm and Qualcomm Oryon In this section, we introduce a novel reverse engineering pipeline for conditional branch predictors (CBPs) and apply it to recover the CBPs of Apple Firestorm and Qualcomm Oryon microarchitectures. \n3.1. Reverse Engineering Pipeline Before delving into the reverse engineering pipeline, it is essential to revisit how the CBP operates, as illustrated in Figure 2. The CBP records the branch history of recent branches, computes the tag and index based on the branch history and program counter (PC), and finally uses the index to access the pattern history table (PHT) sets and find the matching way. Accordingly, to reverse engineer the CBP, we need to recover the branch history first, then narrow the input range of the PHTs, and eventually recover each index and tag bit of the PHT. Based on this high-level procedure, existing branch prediction reverse engineering works (halfhalf, ; indirector, ) construct a reverse engineering pipeline as shown in Figure 3. Assuming the Path History Register (PHR) is used to record branch history, the pipeline breaks down branch history recovery into three steps: first, it measures the PHR length; then, it identifies the range of input bits of the PHR; and lastly, it enumerates the location of each input bit. Subsequently, it narrows the input range of the PHT by guessing the PHT associativity using low PC bits and probing PHR bits in the index and tag functions. However, the existing pipeline has the following major issues: Interference of mispredictions between different branches: Existing methods primarily use conditional branches to set the PHR to known values. However, these branches also contribute to branch mispredictions and introduce noise to the measured performance monitoring counters (PMC). This makes it hard for these methods to deal with complex CBP designs. To overcome this, we adopt a novel approach of utilizing indirect branches to inject values into the PHR and then record conditional branch mispredictions using PMC. Since PMC recording conditional branch mispredictions is not always publicly available (e.g., in some AMD, ARM, and Qualcomm microarchitectures), we add an additional step to recover hidden counters. Clearing and setting PHR: Existing methods rely on clearing the PHR to zero and setting it to arbitrary values. However, this is challenging when the PHR uses many PC bits, as in Firestorm and Oryon. We rewrite existing microbenchmarks to inject PHR in a differential way, where two PHRs differ by only one bit. This eliminates the need to find PC inputs to clear the PHR. Simple assumptions about PHT index and tag functions: The existing pipeline makes assumptions about the simpler PHT index and tag functions of Intel, which do not hold for Firestorm and Oryon. For instance, Intel uses only one PC bit in the index function, whereas Firestorm and Oryon use multiple. Therefore, we propose a comprehensive approach to find the associativity and all PC bits in the index function together. The order of index and tag function recovery: The tag function should be recovered before the index function because creating set conflicts requires prior knowledge of the tag function. The existing pipeline uses assumptions about the index function organization, which only applies to Intel CBP. Instead of making assumptions, we recover them in the correct order. To address these deficiencies, we propose a new pipeline in the right part of Figure 3 to successfully recover the CBPs of Firestorm and Oryon. This pipeline is more general with fewer assumptions, making it applicable to ARM and Intel CBPs as well. \n3.2. Experimental Setup To reverse engineer the conditional branch predictors, we utilize the Apple M1 processor for the Apple Firestorm microarchitecture and the Qualcomm X1E-80-100 processor for the Qualcomm Oryon microarchitecture. We conduct our experiments in bare-metal Linux environments (e.g. Asahi Linux on Apple M1) to access performance monitoring counters directly. \n3.3. Discover Hidden PMCs As previously mentioned, this is one of the prerequisites for the subsequent reverse engineering of the more complex CBPs.\nThere are three involved counters that record different types of branch mispredictions: the all branch counter, the conditional branch counter, and the indirect branch counter. According to open literature, Apple provides all the three counters, and the numbers are respectively 0xcb, 0xc5, and 0xc6. However, Qualcomm only publishes the first one, 0x10 (or 0x22) (armv8, ). Therefore, we design three microbenchmarks that have a known number of mispredictions for each type of branch. We then enumerate the hidden performance counters to determine if the actual values match our expectations. The first microbenchmark contains conditional branches whose directions are based on random numbers, resulting in a conditional branch misprediction rate of 50%. The second microbenchmark contains indirect branches that jump to two target addresses randomly, leading to an indirect branch misprediction rate of 50%. The third microbenchmark combines the first two microbenchmarks by running both conditional and indirect branches simultaneously. When executing the loop body n\ud835\udc5bnitalic_n times, we anticipate that three counters recording different types of branch mispredictions will yield the values presented in Table 1. We have enumerated the PMC space and identified the corresponding counters as listed in Table 2. Our findings are: for Qualcomm Oryon, the counters 0x10 and 0x22 correspond to the standard counters defined in ARMv8 (armv8, ), but the counters 0x400, 0x80d, and 0x80e remain undisclosed.\nIn the subsequent steps, we utilize the 0xc5 counter for Apple Firestorm and the 0x400 counter for Qualcomm Oryon to measure conditional branch misprediction rates. This approach allowed us to inject branch histories via indirect branches without introducing noise to the performance counters, which is required for subsequent microbenchmarks. \n3.4. Reverse Engineering The Path History Register The PHR records recent taken branches by shifting a certain number of bits and then XOR-ing the footprint generated by the branch and the target address of the taken branch. To reverse engineer the PHR, we need to recover the length of the register, the shift amount upon each taken branch, and the method by which the footprint is generated. First, we measure the number of taken branches that can be tracked in the PHR using an approach improved from that described in (halfhalf, ) with the newly discovered hidden PMC. We inject a variable number of unconditional branches, referred to as dummy branches, to shift the PHR. We create two random but correlated branches with various dummy branches in between. If the CBP sees the history of the first correlated branch when predicting the direction of the second one, it can predict correctly; otherwise, it has a 50% chance of mispredicting. Since we have specific PMCs for conditional branches alone, we use an indirect branch for the first correlated branch and a conditional branch for the second one. The pseudo-code is shown in Listing 1, which is similar to Listing 3 of (halfhalf, ), but with the first conditional branch replaced by an indirect branch. We run the microbenchmark on both microarchitectures and obtain identical results, as depicted in Figure 4. The misprediction rate increases from 0% at 100 branches to 50% at 101 branches. This indicates that the PHR in both microarchitectures can only record information from the most recent 100 taken branches. The result also confirms that the global history register (GHR) is not used in place of the PHR: if the GHR were used solely for prediction, it would only contain the direction history of the conditional branch, preventing the conditional branch predictor from achieving a 0% misprediction rate. Next, we reverse engineer the update mechanism and bit width of the PHR. We begin by determining the location of the branch address within the PHR. Given that the width of the branch footprint may exceed the shift amount, some footprint bits may be shifted out of the PHR earlier than others. To investigate this, we inject the branch direction into different bits of the branch address to observe how many branches are required to shift these bits out of the PHR. Unlike the approach in (halfhalf, ), which involves finding a method to clear the PHR, we employ a differential approach. This involves using two PHRs that differ by only one bit while the other bits can be non-zero. We use a pair of branches with a single bit difference in their branch addresses but identical target addresses to inject a single bit, as illustrated in Listing 2. In this scenario, the first conditional branch will contribute approximately 25% to the overall conditional branch misprediction rate. We run the microbenchmark on both microarchitectures, and the results are presented in Figure 5. The findings indicate that, for both Firestorm and Oryon, only the bits 5:2 of the branch address (abbreviated as B below) are utilized in the computation of the footprint. The remaining bits of the branch address do not contribute to the PHR footprint computation. Each bit corresponds to a single number of dummy branches, which strongly suggests that the shift amount of the PHR is one. Subsequently, we employ a similar differential approach to identify the target address bits, utilizing the newly discovered hidden PMC. Instead of using a branch pair to inject the branch address, we utilize an indirect branch with variable target addresses. To bypass the numerous NOP instructions between the two target addresses, we copy the first dummy branch to the first target address to jump over the NOP instructions. We maintain the same B[5:2] for both branches to avoid altering the PHR. To circumvent the limitations of assemblers and linkers, we utilize just-in-time (JIT) compilation to place instructions directly into memory. The results are illustrated in Figure 6. Both Firestorm and Oryon exhibit identical behavior in the target address injection test: the bits 31:2 of the target address (abbreviated as T below) are utilized in the PHR footprint computation. Similar to the branch address, the target address is shifted into the PHR by one bit for each taken branch. Therefore, we conclude that the shift amount of the PHR is one bit. Given that the PHR can track 100 taken branches, we infer that the PHR has a length of 100 bits. Finally, we recover the footprint computation function of the PHR. Given that we already know the PHR shift amount and bit width, we can convert the maximum number of dummy branches that allow perfect prediction of each B/T bits to their respective locations within the footprint. Based on Figure 5 and Figure 6, if both the branch and target addresses are combined into a single PHR register, it will result in a large gap within the footprint. Therefore, it is plausible that the PHR is split into two registers: one called PHRT (Path History Register for Target address), which exclusively records the target address, and another called PHRB (Path History Register for Branch address). The update functions for both PHRT and PHRB are as follows for Firestorm and Oryon: Subsequently, Firestorm and Oryon differ only in the width of PHRB: Oryon features a 32-bit PHRB, whereas Firestorm has a 28-bit PHRB. The PHRT remains consistent at 100 bits for both microarchitectures. In the following section, we will demonstrate that the split PHRT and PHRB approach is indeed employed in the actual hardware. \n3.5. Recover the 1st PHT In this section, we reverse-engineer the PHTs used in the TAGE algorithm. Currently, we do not know the number of PHTs and the history lengths of each table, so we reverse-engineer the PHT with the longest history (referred to as the 1st PHT below) first. To force the 1st PHT to be used, we inject a random direction k\ud835\udc58kitalic_k into PHR[99], which is the highest bit of the PHR, and correlate it with the conditional branches in microbenchmarks. Since Firestorm and Oryon differ in PHT details, but the same reverse-engineering techniques can be applied, we primarily present Firestorm results in the following paragraphs and we will publish the full results online. First, we need to determine the inputs to the PHT. As previously mentioned, the program counter (PC) of the conditional branch and the PHR are input values to the PHT, but we do not yet know which bits of the PC are utilized. Therefore, we design a microbenchmark to observe whether the conditional branch predictor can distinguish between two conditional branches whose addresses differ by only one bit. Instead of clearing the PHR as in (halfhalf, ), we use a long chain of dummy branches to reset the PHR to an unknown but constant value. Subsequently, two conditional branches are predicted by the CBP using the same PHR. If the bit difference between the two conditional branches does not affect the PHT index or tag computation, the CBP will treat them as a single branch and fail to predict correctly. We run the microbenchmark on both microarchitectures and conclude that the Firestorm CBP uses PC[18:2] as input, while the Oryon CBP uses PC[12:2]. Next, we need to determine whether the PC bits are used in the index or tag function of the PHTs. In Intel Alder Lake, only one PC bit was used in the index, while the others were used in the tag (halfhalf, ). This is no longer the case in Firestorm and Oryon CBP. To identify the PC bits that belong to the index function, we use one PHR to predict several branches with different PC values. We place the branches at multiples of a power-of-two base address. These addresses may differ in several PC bits. If all these bits only reside in the tag function, then the branches will belong to one set of PHT, and the maximum number of branches that can be predicted accurately will equal the associativity. If some bits appear in the index function, then the branches will be scattered across multiple sets, and we can observe a power-of-two factor multiplied to the maximum number of branches predicted accurately. For example, if only PC[6] and PC[9] are included in the index function, while others are in the tag function, when we use different base addresses, we should be able to observe the following results, assuming a 4-way set associative PHT on Firestorm: Case 1. All PC bits reside in tag: Using a base address of 23superscript232^{3}2 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT bytes, the first five branches are placed at 0x8, 0x10, 0x18, 0x20, 0x28 addresses respectively. They are mapped to the same index, so adding the fifth branch will lead to branch misprediction. Thus, the maximum number of branches without mispredictions would be four. For base address of 210superscript2102^{10}2 start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT to 217superscript2172^{17}2 start_POSTSUPERSCRIPT 17 end_POSTSUPERSCRIPT bytes, the result is the same. Case 2. Only one PC bit reside in index: Using base address of 24superscript242^{4}2 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT bytes, the first four branches are mapped to the same index, but the fifth branch goes to a new set. So another set of four branches can be predicted without mispredictions. The ninth branch will lead to set conflict. Using base address of 25superscript252^{5}2 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT or 27superscript272^{7}2 start_POSTSUPERSCRIPT 7 end_POSTSUPERSCRIPT to 29superscript292^{9}2 start_POSTSUPERSCRIPT 9 end_POSTSUPERSCRIPT bytes, the result is the same. Case 3. Two PC bits reside in index: Using base address of 26superscript262^{6}2 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT bytes, now the branches can span both PC[6] and PC[9] index bits to achieve sixteen branches without mispredictions using four sets of branches. Case 4. PC bits go beyond the input range: Using base address of 218superscript2182^{18}2 start_POSTSUPERSCRIPT 18 end_POSTSUPERSCRIPT and 219superscript2192^{19}2 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT, the result is two or one branches because higher bits are no longer computed in index or tag function. We design a microbenchmark to observe the actual numbers, utilizing the newly discovered hidden PMC. Then, we need to let the CBP use the same PHR to predict multiple branches. If we use an indirect branch to jump to these conditional branches, the indirect branch itself will introduce PHR differences. Therefore, we use two indirect branches to maintain the same PHR while landing at different conditional branches. For example, if we want to jump to a 2nsuperscript2\ud835\udc5b2^{n}2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT address, assuming n\u22121>5\ud835\udc5b15n-1>5italic_n - 1 > 5: Branch #1: One indirect branch to jump to a 2n\u22121superscript2\ud835\udc5b12^{n-1}2 start_POSTSUPERSCRIPT italic_n - 1 end_POSTSUPERSCRIPT address: B=0, T[n-1]=1, then P\u2062H\u2062R1=(P\u2062H\u2062R0\u226a1)\u2295(1\u226a(n\u22123))\ud835\udc43\ud835\udc3bsubscript\ud835\udc451direct-summuch-less-than\ud835\udc43\ud835\udc3bsubscript\ud835\udc4501much-less-than1\ud835\udc5b3PHR_{1}=(PHR_{0}\\ll 1)\\oplus(1\\ll(n-3))italic_P italic_H italic_R start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = ( italic_P italic_H italic_R start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u226a 1 ) \u2295 ( 1 \u226a ( italic_n - 3 ) ) Branch #2: For every n\ud835\udc5bnitalic_n, one indirect branch at 2n\u22121superscript2\ud835\udc5b12^{n-1}2 start_POSTSUPERSCRIPT italic_n - 1 end_POSTSUPERSCRIPT to jump to 2nsuperscript2\ud835\udc5b2^{n}2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT: B[n-1]=1, T[n]=1, since n\u22121>5\ud835\udc5b15n-1>5italic_n - 1 > 5, then P\u2062H\u2062R2=(P\u2062H\u2062R1\u226a1)\u2295(1\u226a(n\u22122))=P\u2062H\u2062R0\u226a2\ud835\udc43\ud835\udc3bsubscript\ud835\udc452direct-summuch-less-than\ud835\udc43\ud835\udc3bsubscript\ud835\udc4511much-less-than1\ud835\udc5b2\ud835\udc43\ud835\udc3bsubscript\ud835\udc450much-less-than2PHR_{2}=(PHR_{1}\\ll 1)\\oplus(1\\ll(n-2))=PHR_{0}\\ll 2italic_P italic_H italic_R start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = ( italic_P italic_H italic_R start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u226a 1 ) \u2295 ( 1 \u226a ( italic_n - 2 ) ) = italic_P italic_H italic_R start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u226a 2 The PHR changes of the two indirect branches via target branch cancel out. If n\u22121\u22645\ud835\udc5b15n-1\\leq 5italic_n - 1 \u2264 5, we fill the space between the targets of the first indirect branch with NOPs, and then use a second indirect branch to cancel out the contribution of T bits of the first indirect branch. Either way, we can let the CBP predict different branches using the same PHR. This technique can be extended to use three or more branches to inject PHR via branch address in other microbenchmarks. The results of the microbenchmark on Firestorm are shown in Figure 7. The results matched our expected outcomes based on the assumption that only PC[6] and PC[9] are used in the index function and the PHT is 4-way set associative. Therefore, we can conclude that the 1st PHT of the Firestorm CBP is 4-way set associative and uses PC[6] and PC[9] in its index function. Similarly, we find that the 1st PHT of the Oryon CBP is also 4-way associative but uses PC[6] and PC[7] in its index function instead. Next, we recover the tag function. To identify which bits are XOR-ed in the tag function, we use two independent random variables: k\ud835\udc58kitalic_k and l\ud835\udc59litalic_l. We inject k\ud835\udc58kitalic_k and l\ud835\udc59litalic_l into different bits of the PHR or PC and let the CBP predict a conditional branch conditioned by k\u2295ldirect-sum\ud835\udc58\ud835\udc59k\\oplus litalic_k \u2295 italic_l. If the two bits are XOR-ed in the tag function and do not appear in the index function, misprediction will occur. We enumerate all pairs among the PHR bits (using the target address and branch address separately) and PC bits. On Firestorm, 12 groups of XOR-ed bits can be observed (PHRT denotes injecting PHR via the target address, PHRB for injecting with the branch address, PHRT[a,b,\u2026,d] means PHRT[a] \u2295direct-sum\\oplus\u2295 PHRT[b] \u2295\u22ef\u2295limit-fromdirect-sum\u22efdirect-sum\\oplus\\cdots\\oplus\u2295 \u22ef \u2295 PHRT[d], PHRB likewise): PC[7] \u2295direct-sum\\oplus\u2295 PHRT[0,12,\u2026,96] \u2295direct-sum\\oplus\u2295 PHRB[8,21] PC[8] \u2295direct-sum\\oplus\u2295 PHRT[1,13,\u2026,97] \u2295direct-sum\\oplus\u2295 PHRB[9,22] PC[9] \u2295direct-sum\\oplus\u2295 PHRT[2,14,\u2026,98] \u2295direct-sum\\oplus\u2295 PHRB[10,23] PC[10] \u2295direct-sum\\oplus\u2295 PHRT[3,15,\u2026,87] \u2295direct-sum\\oplus\u2295 PHRB[11,12,24,25] PC[11] \u2295direct-sum\\oplus\u2295 PHRT[4,16,\u2026,88] \u2295direct-sum\\oplus\u2295 PHRB[0,13,26] PC[12] \u2295direct-sum\\oplus\u2295 PHRT[5,17,\u2026,89] \u2295direct-sum\\oplus\u2295 PHRB[1,14,27] PC[13] \u2295direct-sum\\oplus\u2295 PHRT[6,18,\u2026,90] \u2295direct-sum\\oplus\u2295 PHRB[2,15] PC[14] \u2295direct-sum\\oplus\u2295 PHRT[7,19,\u2026,91] \u2295direct-sum\\oplus\u2295 PHRB[3,16] PC[15] \u2295direct-sum\\oplus\u2295 PHRT[8,20,\u2026,92] \u2295direct-sum\\oplus\u2295 PHRB[4,17] PC[16] \u2295direct-sum\\oplus\u2295 PHRT[9,21,\u2026,93] \u2295direct-sum\\oplus\u2295 PHRB[5,18] PC[17] \u2295direct-sum\\oplus\u2295 PHRT[10,22,\u2026,94] \u2295direct-sum\\oplus\u2295 PHRB[6,19] PC[18] \u2295direct-sum\\oplus\u2295 PHRT[11,23,\u2026,95] \u2295direct-sum\\oplus\u2295 PHRB[7,20] The results are consistent with our previous observations. Since PHRB and PHRT participate in the tag function in different ways, we can deduce that the PHR is split into two registers: PHRT and PHRB. Because PC[5:2] are also included in the index or tag function and do not reside in the index function, they appear in the tag function without being XOR-ed. We conclude that the tag function of the 1st PHT contains 16 bits: 12 bits from the XOR group above and 4 bits from PC[5:2]. Lastly, we recover the index function of the 1st PHT. Since we already know the tag function, we can create multiple branches with different PC[5:2] so that they will never collide in the tag. We also know the associativity, so we use two groups of conditional branches with 4 branches each. Again, we inject two random variables k\ud835\udc58kitalic_k and l\ud835\udc59litalic_l into bits of PHRB, PHRT or PC, and let the conditional branches be conditioned by k\u2295ldirect-sum\ud835\udc58\ud835\udc59k\\oplus litalic_k \u2295 italic_l. Consider the different cases where the bits reside: If k\ud835\udc58kitalic_k and l\ud835\udc59litalic_l are both injected into bits in the index function but not XOR-ed, the CBP can predict all eight branches without misprediction since they are mapped to different sets. If k\ud835\udc58kitalic_k and l\ud835\udc59litalic_l are both injected into bits in the index function and XOR-ed, the CBP can predict at most four branches without misprediction because they are mapped to the same set. Otherwise, the CBP can predict at most two branches without misprediction because both directions of the same branch (k\u2295l=0direct-sum\ud835\udc58\ud835\udc590k\\oplus l=0italic_k \u2295 italic_l = 0 and k\u2295l=1direct-sum\ud835\udc58\ud835\udc591k\\oplus l=1italic_k \u2295 italic_l = 1) will occupy an entry in one set. Thus, we are able to identify all index bits and their XOR relationships. On Firestorm, 10 index bits are discovered for the 1st PHT: PHRT[2] \u2295direct-sum\\oplus\u2295 PHRT[43] \u2295direct-sum\\oplus\u2295 PHRT[93] PHRT[7] \u2295direct-sum\\oplus\u2295 PHRT[48] \u2295direct-sum\\oplus\u2295 PHRT[99] PHRT[12] \u2295direct-sum\\oplus\u2295 PHRT[63] \u2295direct-sum\\oplus\u2295 PHRB[5] PHRT[17] \u2295direct-sum\\oplus\u2295 PHRT[68] \u2295direct-sum\\oplus\u2295 PHRB[10] PHRT[22] \u2295direct-sum\\oplus\u2295 PHRT[73] \u2295direct-sum\\oplus\u2295 PHRB[15] PHRT[27] \u2295direct-sum\\oplus\u2295 PHRT[78] \u2295direct-sum\\oplus\u2295 PHRB[20] PHRT[33] \u2295direct-sum\\oplus\u2295 PHRT[83] \u2295direct-sum\\oplus\u2295 PHRB[25] PHRT[38] \u2295direct-sum\\oplus\u2295 PHRT[88] \u2295direct-sum\\oplus\u2295 PC[9] PHRT[53] \u2295direct-sum\\oplus\u2295 PHRT[58] \u2295direct-sum\\oplus\u2295 PHRB[0] PC[6] We have recovered the index and tag functions of the 1st PHT of Firestorm, which is 4-way 4096-entry in capacity. We have also demonstrated that the Firestorm CBP uses two separate registers, PHRB and PHRT, to record branch history. \n3.6. Recover the rest PHTs After recovering the 1st PHT, which has the longest history length, we need to recover the remaining PHTs with shorter history lengths. Due to the nature of how TAGE works, upon mispredictions in a PHT with a shorter history length, new entries will be allocated in a PHT with a longer history length (tage, ). Therefore, we need to remove the effect of PHTs with longer history lengths to recover the rest of the PHTs. In (halfhalf, ), to recover the base predictor, they zeroed out the PHR so that the index is cleared to zero, and then pre-filled the PHT with branches. This technique can also be applied to recovering PHTs: for example, if we inject k\ud835\udc58kitalic_k into PHRT[43] in the 2nd PHT (the PHT with the second longest history length), we also inject k\ud835\udc58kitalic_k into PHRT[93] so that they get XOR-ed in the index function of the 1st PHT. This method works well for the first few PHTs but has some limitations: clearing out index bits may not always be applicable. For example, if an index bit only uses very low PHR bits, such as PHRT[0] \u2295direct-sum\\oplus\u2295 PHRT[1] \u2295direct-sum\\oplus\u2295 PHRB[2], then PHTs with shorter history lengths will be involved. This was actually observed while recovering the 3rd PHT of Firestorm. To make matters worse, zeroing out an index for one bit introduces another, while the new bit may require a third bit to clear out in another PHT, essentially creating a chain reaction. To overcome these limitations, we introduce a new approach that inserts independent random variables into history bits that do not belong to the current PHT. For example, to recover the 2nd PHT of Firestorm, which uses 57 bits of PHRT, we inject random values into PHRT[99:57]. Even if new entries are allocated in the 1st PHT, there is little possibility of having a tag hit later. If without the hidden PMC, this approach will no longer work due to interference. Combining these techniques, we recover the remaining PHTs of Firestorm and Oryon. From Table 3, the size of the Oryon CBP, considering tag bits, is computed as (4\u00d7210\u00d73+4\u00d7211\u00d72+6\u00d7211)\u00d716=6553604superscript21034superscript21126superscript21116655360(4\\times 2^{10}\\times 3+4\\times 2^{11}\\times 2+6\\times 2^{11})\\times 16=655360( 4 \u00d7 2 start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT \u00d7 3 + 4 \u00d7 2 start_POSTSUPERSCRIPT 11 end_POSTSUPERSCRIPT \u00d7 2 + 6 \u00d7 2 start_POSTSUPERSCRIPT 11 end_POSTSUPERSCRIPT ) \u00d7 16 = 655360 bits, which equals the 80KB capacity announced by Qualcomm in (hotchips, ). \n4. Optimization with CBP Model In this section, we observe two previously undisclosed effects that may impair branch prediction accuracy: Scatter and Annihilation. Utilizing the findings from reverse engineering, we develop models of the CBPs for Firestorm, Oryon and Intel. These models enable us to explain the underlying mechanisms of the two effects and subsequently enhance performance. \n4.1. Scatter Effect We observe a scenario where the insertion of a single NOP instruction can simultaneously reduce both the branch misprediction rate and the Mispredictions Per Kilo Instructions (MPKI), thereby enhancing performance. This scenario can be demonstrated through an example, that is, the execution of a typical binary search program using keys that adhere to a Zipf distribution with an exponent of, say, 0.9. The corresponding code and generated assembly are presented in Listings 3 and 4, respectively. By inserting a single NOP instruction into the assembly, we observe a significant reduction in the branch misprediction rate, dropping from 10.92% to 9.63%, and a corresponding decrease in MPKI from 34.5 to 29.5 on the Oryon microarchitecture. This optimization finally results in a 7% speedup on Oryon. The placement of the NOP instruction is crucial, as illustrated in Figure 8. The minimum misprediction rate and MPKI are attained when the NOP instruction is inserted between the labels .L2 and .L3. .L2 .L3 Moreover, this effect is universal, that is, it also exists in Intel and Apple processors, and the same optimization results in a 6% speedup on Intel Cascade Lake and a 2% speedup on Firestorm. .L2 .L3 .L3 b.lt .L2 With the branch predictor model at our disposal, we can now explain the underlying reasons for these observations. The performance-critical conditional branches within the loop are b.lt .L2 and b.le .L3. These branches are data-dependent and correlated due to the Zipf distribution. According to the model, the tag function of the Oryon CBP XORs the PHRB and PHRT bits with zero offset. When a NOP instruction is inserted between the labels .L2 and .L3, as shown in Table 4, the contributions of the two branches to the PHT tag differ by only one bit. In contrast, other NOP locations or the absence of a NOP result in more significant differences in the PHT tag. Additionally, the two conditional branches differ in PC[5:2], thus avoiding any aliasing. Given the different TAGE entries for the same branch due to varying PHRs, fewer entries are occupied because the tag function exhibits less randomness, allowing TAGE to be trained more rapidly. Without the additional NOP or if the NOP is not properly placed, one branch may be mapped to more TAGE entries, leading to increased capacity conflicts. b.lt .L2 b.le .L3 .L2 .L3 In conclusion, the Scatter effect refers to the phenomenon where a single conditional branch is distributed across a large number of TAGE PHT entries, resulting in higher MPKI and degraded performance. Our analysis indicates that this effect can be mitigated by examining the PHT tag bits contributed by the critical branches and strategically inserting NOPs. \n4.2. Annihilation Effect Based on the CBP model, we observe that numerous taken branches share identical footprints, implying that the CBP can no longer differentiate between these branches, thereby leading to increased mispredictions. We term this phenomenon the Annihilation effect. To delve deeper into this issue, we examine the leela_r benchmark from SPEC INT 2017 using distinct PHR update functions from Intel, Apple and Qualcomm CBPs. We conduct a static analysis of all branch pairs within the same function, and count the number of branch pairs whose PHR footprints collide for each CBP. The results are presented in Table 5, indicating that the Alder Lake CBP generates a smaller number of collisions than Firestorm, Oryon and Skylake whereas Haswell performs the worst. leela_r Regarding the PHR update function, Intel and ARM adopt the more traditional approach of using a single PHR for both branch and target addresses (halfhalf, ; trustzonetunnel, ; phr, ). In contrast, Apple and Qualcomm employ two separate registers with a predominant width of footprint in T[31:2]. This approach may present challenges in speculative PHR update and restoration but can mitigate the occurrence of the Annihilation effect with regard to target address. However, Apple and Qualcomm only use a small portion of the branch address bits in the PHR footprint. Therefore it becomes relatively easy for collisions to occur in typical programming patterns such as goto end for cleanups and continue statements within loop bodies, as in these patterns, branches often jump to the same target address. Conversely, if lower branch address bits are not used, as in Intel Haswell and Skylake, it is easy for two adjacent branches to generate the same footprint. Intel improved this in Alder Lake by utilizing lower branch address bits. goto end continue In conclusion, the Annihilation effect of colliding PHR footprints can be mitigated through redesigning the PHR update function to include lower bits of both the branch and target addresses. \n5. Comparing CBPs of Different CPU Vendors In this section, we conduct a comparative analysis of the conditional branch predictors (CBPs) from various CPU vendors, employing both quantitative and qualitative approaches. We implement these CBPs on a unified branch predictor model and evaluate their branch prediction accuracy using SPEC INT 2017 and Geekbench 5 benchmarks. Subsequently, we analyze the design considerations underlying these implementations. \n5.1. Branch Prediction Accuracy of CBPs Given that the CBPs of Intel (halfhalf, ), Apple, and Qualcomm cores have been recovered (as shown in Table 6), we implement them on an in-house ChampSim-like (champsim, ) standalone branch predictor model that aligns with a commercial processor. We simulate these CBPs using SPEC INT 2017 and Geekbench 5 benchmarks compiled for ARM64, employing the SimPoint (simpoint, ) methodology. The results, depicted in Figure 9, indicate that the Firestorm CBP performs the best, narrowly outperforming the Oryon CBP by 1%, while the Skylake CBP significantly lags behind by more than 20%. Given their differing capacities, for each CBP, we create a clone with 24K entries, comprising 6 PHTs with 4K entries each, while maintaining similar PHR update and PHT index/tag functions. This allows us to compare the performance solely attributable to the hash functions. We conduct the same experiments using the 24K CBPs. Results show that 24K Firestorm CBP produces only 1% less MPKI then 24K Oryon and 24K Skylake. We can perceive that the capacity of the CBP significantly contributes to branch prediction accuracy. The CBP of Intel Skylake has the smallest capacity and the highest MPKI. The capacities of Firestorm and Oryon are nearly identical, and their accuracies are also similar. When these CBPs are aligned to the same capacity of 24K entries, the MPKI values become much closer. \n5.2. Comparing PHT Organizations In addition to capacity, we compare the design differences in the organization of PHT. For the PHT tag function, Intel, Apple, and Qualcomm appear to agree on PHR folding and XOR-ing PC bits. However, for the PHT index function, Apple and Qualcomm diverge again by using two or three bits XOR-ed instead of folding PHR. Previously we demonstrate that the hash functions contribute less to branch prediction accuracy than capacity, indicating that the differences are primarily due to distinct methods of improving timing. Regarding PHT partitioning, Half & Half reveals that Intel CBP is partitioned by one PC bit: PC[5] since Skylake and PC[4] on older microarchitectures (halfhalf, ). Similarly, Apple and Qualcomm CBPs are also partitioned by one PC bit: PC[6], according to our CBP models. This is not coincidental, as it enables a significant improvement: the CBP can easily predict the direction of two adjacent conditional branches using the same PHR. They reside in the same prediction block, ensuring that their PC[6] bit is always equal on Firestorm and Qualcomm (likewise on Intel). Consequently, their index bits are identical, allowing the CBP to extract all ways from one set and compare against two tags simultaneously. This approach is both easier and more cost-effective to implement compared to having two read ports for the backing SRAM arrays. \n6. Conclusions In this paper, we present, for the first time, the CBP structure of Apple Firestorm and Qualcomm Oryon processors, enabling the construction of a branch predictor model. We introduce a novel reverse engineering pipeline with fewer assumptions, making it more applicable to a broader range of microarchitectures. Subsequently, we identify the Scatter and Annihilation effects and discuss how to utilize the CBP model to guide optimization. Finally, we compare the MPKI of different CBP designs running SPEC CPU 2017 and Geekbench 5 benchmarks, and offer the following insights on CBP design: The PHR footprint should include more bits of both branch and target addresses, especially low address bits, to avoid Annihilation effect. In CBP design, capacity is a critical factor for prediction accuracy. The PHR update and PHT index/tag functions have a smaller impact and allow for more timing optimization. Predicting two adjacent conditional branches per cycle can be implemented by partitioning PHT by one PC bit and matching two ways simultaneously, without requiring an additional read port for SRAM. References"}
{"text": "Copyright for this paper by its authors.\nUse permitted under Creative Commons License Attribution 4.0\nInternational (CC BY 4.0). EAmSI24: Edge AI meets swarm intelligence,\nSeptember 18, 2024, Dubrovnik, Croatia [orcid=0009-0000-7543-2391,\nemail=ali.ganbarov@tu-berlin.de,\n] [orcid=0009-0002-4448-2809,\nemail=jicheng.yuan@tu-berlin.de,\n] [orcid=0000-0003-2458-607X,\nemail=anh.letuan@tu-berlin.de,\n] [orcid=0000-0002-1839-0372,\nemail=manfred.hauswirth@tu-berlin.de,\n] [orcid=0000-0003-2480-9261,\nemail=danh.lephuoc@tu-berlin.de,\n] Experimental comparison of graph-based approximate nearest neighbor search algorithms on edge devices In this paper, we present an experimental comparison of various graph-based approximate nearest neighbor (ANN) search algorithms deployed on edge devices for real-time nearest neighbor search applications, such as smart city infrastructure and autonomous vehicles. To the best of our knowledge, this specific comparative analysis has not been previously conducted. While existing research has explored graph-based ANN algorithms, it has often been limited to single-threaded implementations on standard commodity hardware. Our study leverages the full computational and storage capabilities of edge devices, incorporating additional metrics such as insertion and deletion latency of new vectors and power consumption. This comprehensive evaluation aims to provide valuable insights into the performance and suitability of these algorithms for edge-based real-time tracking systems enhanced by nearest-neighbor search algorithms. \n1 Motivation and contribution Approximate Nearest Neighbor Search (ANNS) has become more crucial as the amount of data we have to handle keeps increasing rapidly. ANNS plays a key role in addressing the k\ud835\udc58kitalic_k-Nearest Neighbor Search (k\ud835\udc58kitalic_k-NNS) issue, where the task is to identify the k\ud835\udc58kitalic_k most similar vectors to a given query vector within a dataset. The easiest exact approach to k\ud835\udc58kitalic_k-NNS involves measuring distances between the query vector and all vectors in the dataset, followed by selecting the k\ud835\udc58kitalic_k vectors with the shortest distances. However, this approach is not feasible for large datasets due to the significant computational burden, which poses challenges for scalability. As datasets expand exponentially and the curse of dimensionality becomes a critical concern, precise NNS methods become inefficient and costly. Consequently, research has pivoted towards ANNS algorithms that substantially enhance efficiency while allowing for a slight decrease in accuracy. ANNS achieves a balance between speed and precision, minimizing computational requirements and facilitating scalability for extensive datasets. These algorithms utilize indexing to identify approximate nearest neighbors in high-dimensional spaces, providing a practical solution for numerous applications where exact results are not essential. Nearest Neighbor Search (NNS) is fundamental in various sectors such as recommendation systems [1], data retrieval [2], information matching [3]. Although the brute force method is straightforward and intuitive, its computational cost increases significantly as data volume grows. ANNS approaches address this challenge by significantly reducing search times with only a minor sacrifice in accuracy, thereby balancing precision and latency to suit different application needs. This paper presents an experimental evaluation of approximate nearest neighbor (ANNS) algorithms on edge devices for smart city applications, using data from street cameras provided by Conveqs and Aalto University in Helsinki. The captured data was preprocessed to extract object bounding boxes, which were then embedded into vectors for constructing an index graph for ANNS classification tasks. Our study reveals several key insights: the performance patterns of algorithms differ significantly between edge devices and servers, more powerful edge devices do not always yield more efficient algorithm execution, and cost-effective devices can achieve performance comparable to much more expensive counterparts. This paper is structured as follows: Section 1 discusses the motivation and contributions of the study. Section 2 provides background information and related work done in the field. Section 3 outlines the methodology, including the metrics used and their justifications, the process for determining accuracy, the algorithm implementations, and the data utilized. Section 4 presents the experimental results and discussions. Finally, Section 5 concludes the paper and suggests directions for future work. \n2 Background and Related Work In this section, we will discuss Product Quantization for data compression to fit models into edge devices with smaller memory capacity and Approximate Nearest Neighbor Search algorithms for vector search to retrieve the closest items in the dataset to a given query. Additionally, we will give an overview of existing work. \n2.1 Background Product Quantization (PQ) is an effective technique for approximate nearest neighbor (ANN) search, ideal for edge devices [4]. It reduces storage and computational demands, making it suitable for resource-constrained environments. PQ splits high-dimensional vectors into lower-dimensional sub-vectors, which are independently quantized using codebooks, thus approximating the original data with reduced storage needs. ANNS techniques are crucial in fields like data mining, AI, NLP [5], computer vision [6], information retrieval [7], and advertising [8]. They are vital for clustering, classification, and dimensionality reduction. ANNS algorithms are classified into hashing-based [9], tree-based [10], quantization-based [11], and graph-based methods [12]. Graph-based methods, in particular, offer high recall rates and reduced search times, making them valuable for many practical applications. Hierarchical Navigable Small World (HNSW)\n[13] is an approximate nearest neighbor algorithm that builds a multi-layer graph. It organizes data into hierarchical layers, with each layer containing fewer points. The algorithm uses a greedy search from the top layer down, efficiently narrowing the search space. This structure allows HNSW to achieve high recall rates and fast search times, making it ideal for large-scale, high-dimensional data retrieval. Vamana\n[14] is an approximate nearest neighbor algorithm that builds a navigable small-world graph. Unlike HNSW, Vamana focuses on balanced connectivity, ensuring each node links to both close and distant neighbors. This approach enables efficient graph traversal and rapid identification of nearest neighbors, making it well-suited for large-scale, high-dimensional datasets while keeping computational costs low. Inverted-file search (IVF)\n[15] is a two-stage search algorithm that combines a coarse quantizer with Product Quantization (PQ). First, IVF partitions the database into k0subscript\ud835\udc580k_{0}italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT Voronoi cells and selects the w\ud835\udc64witalic_w nearest cells for a query vector. In the second stage, PQ is applied only to vectors in these selected cells, which reduces the search space and computational costs, balancing efficiency and accuracy. Locality-Sensitive Hashing (LSH)\n[16] improves approximate nearest neighbor search by mapping high-dimensional data into low-dimensional buckets using hash functions. Similar points are likely to fall into the same bucket, reducing the search space. LSH computes hash values for the query vector and retrieves candidates from the relevant buckets, thus enhancing efficiency and minimizing computational costs. This method balances search accuracy and speed, making it suitable for large-scale and real-time applications. \n2.2 Related Work In a comprehensive study, [17] provides a detailed comparison of graph-based approximate nearest neighbor search (ANNS) algorithms. The authors implemented these algorithms from scratch in C++ and evaluated them theoretically without using parallel programming or GPU techniques, which are essential for practical optimization. The study compares 13 algorithms across various datasets, offering insights into their performance, strengths, and weaknesses. [18] assesses graph-based ANNS algorithms with quantization techniques for real-time streaming data in fog computing. Their focus is on algorithmic redesign for FPGA architectures to reduce network congestion. In contrast, our research targets the deployment of ANNS algorithms on edge devices, utilizing CPU and GPU resources. We evaluate algorithms in their native state, contrasting with FPGA-focused modifications in the referenced study [18]. \n3 Methodology In this section, we will discuss the crucial metrics that are employed in real-time performance-critical scenarios such as smart city applications. Real-time object detection systems, essential for smart traffic management, public safety, and environmental monitoring, demand efficient algorithms to be able to process incoming data swiftly. Our study incorporates state-of-the-art ANNS algorithms to process large amounts of data collected from street cameras and evaluates them based on metrics aimed at optimizing real-time performance. The following subsections will discuss the accuracy measurement technique, evaluation metrics, algorithm implementations, and data. \n3.1 Accuracy Measure To evaluate the accuracy of graph-based nearest neighbor search algorithms, we employ a process where the K\ud835\udc3eKitalic_K nearest neighbors for each query are retrieved, and their class labels are analyzed. Given a query point q\ud835\udc5eqitalic_q, the algorithm identifies its K\ud835\udc3eKitalic_K nearest neighbors in the dataset\n{x1,x2,\u2026,xN}subscript\ud835\udc651subscript\ud835\udc652\u2026subscript\ud835\udc65\ud835\udc41\\{x_{1},x_{2},\\ldots,x_{N}\\}{ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT }\n. Let\n\ud835\udca9K\u2062(q)subscript\ud835\udca9\ud835\udc3e\ud835\udc5e\\mathcal{N}_{K}(q)caligraphic_N start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ( italic_q )\ndenote the set of these K\ud835\udc3eKitalic_K nearest neighbors. The class labels of these neighbors are then counted to determine the most frequent class label, which is assigned as the predicted class label for q\ud835\udc5eqitalic_q. Formally, let yisubscript\ud835\udc66\ud835\udc56y_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT represent the class label of the i\ud835\udc56iitalic_i-th nearest neighbor. The predicted class label y^\u2062(q)^\ud835\udc66\ud835\udc5e\\hat{y}(q)over^ start_ARG italic_y end_ARG ( italic_q ) is given by: where\n\ud835\udc9e\ud835\udc9e\\mathcal{C}caligraphic_C\nis the set of all possible class labels and \ud835\udd40\u2062(\u22c5)\ud835\udd40\u22c5\\mathbb{I}(\\cdot)blackboard_I ( \u22c5 ) is the indicator function, which equals 1 if the argument is true and 0 otherwise. The overall accuracy A\ud835\udc34Aitalic_A of the algorithm is calculated as the proportion of correctly classified query points out of the total number of query points Q\ud835\udc44Qitalic_Q Here, y\u2062(qj)\ud835\udc66subscript\ud835\udc5e\ud835\udc57y(q_{j})italic_y ( italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) is the true class label of the j\ud835\udc57jitalic_j-th query point qjsubscript\ud835\udc5e\ud835\udc57q_{j}italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT. This method provides a measure of how well the graph-based nearest neighbor search algorithm can classify points based on the labels of their nearest neighbors. \n3.2 Real-time Performance Metrics In evaluating graph-based approximate nearest neighbor search (ANNS) algorithms for real-time object detection on edge devices in smart city applications, the performance metrics chosen are Queries per Second (QPS), insertion speed, deletion speed, and power consumption. These metrics are essential for assessing the algorithms\u2019 effectiveness and efficiency in practical scenarios. Queries per Second (QPS)\nmeasures how many search queries the algorithm can handle per second. High QPS is crucial for real-time object detection, ensuring timely responses in dynamic environments such as traffic monitoring and public safety. Insertion Speed\nrefers to the time taken to add new data points into the ANNS structure. Efficient insertion is important for dynamic datasets, like real-time video feeds and sensor data, allowing the system to adapt quickly to new information. Deletion Speed\nmeasures how quickly outdated or irrelevant data points can be removed. Fast deletion helps manage storage and keeps the system responsive by preventing data buildup, which is crucial in environments with limited resources. Power Consumption\nevaluates the energy efficiency of the algorithm. Low power consumption is important for edge devices in resource-constrained settings to reduce operational costs and extend device lifespan. The selected performance metrics - queries per second, insertion speed, deletion speed, and power consumption - provide a comprehensive evaluation framework for assessing the suitability of graph-based ANNS algorithms in real-time object detection for smart city applications. These metrics address both the computational performance and practical deployment considerations, ensuring that the chosen algorithms can deliver accurate, timely, and energy-efficient solutions in dynamic urban environments. \n3.3 Algorithm Implementation FAISS (Facebook AI Similarity Search) [19] and DiskANN [14] are key tools for efficient approximate nearest neighbor search (ANNS). FAISS, from Facebook AI Research, offers various indexing methods optimized for CPU and GPU, making it suitable for real-time applications and large datasets. DiskANN focuses on disk-based indexing, using methods like Vamana to efficiently organize and query large datasets with limited memory. DiskANN\u2019s approach allows for fast searches by constructing a structured graph representation of data on disk, ideal for memory-constrained environments. Both FAISS and DiskANN provide effective solutions for diverse ANNS needs, supporting both research and industrial applications. \n3.4 Data In this study, we collected data from the test platform set up by Conveqs and Aalto University in Helsinki, near L\u00e4nsisatama (Western Harbor), J\u00e4tk\u00e4saari\u00a0111https://www.smart-edge.eu/wp-content/uploads/2023/10/SmartEdge-D2.1-First-definition-or-requirements-architecture-and-use-cases.pdf. This platform consists of 17 roadside cameras along with radars. For this experiment, four cameras (No. 2692subscript2692269_{2}269 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, 2693subscript2693269_{3}269 start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, 2702subscript2702270_{2}270 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, 2703subscript2703270_{3}270 start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT) were selected based on their strategic placement, capturing diverse traffic patterns and environmental conditions. These cameras were positioned at high-traffic intersections to ensure a comprehensive dataset covering various vehicle types and pedestrian activities. Due to the large volume of data, we extracted one frame for every ten frames. In total, we have 12,451 frames across ten classes. The box predictions are from the SOTA 2D detection model Co-DETR\u00a0[20] (pre-trained on MS-COCO\u00a0[21]), and we filtered the predictions from the classification head with a score greater than 0.5 as our pseudo-label. This method resulted in a total of 161,805 valid bounding boxes, which we split into 85% for training and 15% for evaluation. To ensure the quality and consistency of the dataset, we established a preprocessing pipeline. This included steps such as removing corrupted frames, correcting for lens distortion, and normalizing image dimensions to 224\u00d7224224224224\\times 224224 \u00d7 224 pixels to match the input requirements of the classification models which were pre-trained on ImageNet\u00a0[22]. In addition, data augmentation such as random cropping, horizontal flipping, and brightness adjustments were applied to increase the variability and robustness of the dataset. For embedding generation, we employed the ResNet50\u00a0[23], a convolutional neural network pre-trained on the ImageNet dataset, as our feature extractor due to its proven performance in various image recognition tasks and its effective performance in capturing hierarchical features through residual learning. To serve as a feature extractor, we removed the final fully connected layer and replaced it with a global average pooling layer, followed by a dense layer producing a 2048-dimensional embedding vector for each image. This configuration ensures that the generated embeddings encapsulate the consistent features of the images. \n4 Experimental Results \n4.1 Setup In our experimental setup to compare the performance of graph-based nearest-neighbor search algorithms, we utilized several edge devices, including four from the NVIDIA Jetson family: Nano, TX2, Xavier AGX, and Orin. Additionally, we incorporated other edge devices such as the Raspberry Pi 4, Raspberry Pi 3, and Raspberry Pi Zero. Tables 1 and 2 provide a comparative analysis of these devices and their capabilities. As observed, devices with higher computational power, such as the Jetson Xavier AGX and Jetson Orin, exhibit increased power consumption, underscoring the trade-off between performance and energy efficiency. These devices are widely used in edge AI application processing scenarios. For instance, in graph-based approximate nearest neighbor search tasks for embedded images with bounding boxes, the computational demands necessitate efficient processing power balanced with manageable energy consumption. This is particularly relevant in applications such as real-time object detection and tracking in surveillance systems, where quick and accurate identification of objects is crucial. In autonomous vehicles, Jetson devices like the Jetson Xavier AGX and Orin process data from sensors such as cameras and LIDAR to identify and track objects, navigate environments, and make real-time decisions. Their high-performance GPUs and AI capabilities enable sophisticated algorithms for tasks such as object detection and lane recognition in advanced driver assistance systems (ADAS). Utilizing a single Jetson AGX Xavier board integrates computing resources\u2014CPUs, GPUs, and DLA cores\u2014reducing costs, power consumption, and cross-device communication issues, ensuring effective ADAS operation [24]. In healthcare, Jetson devices facilitate portable medical imaging and diagnostics [25]. They enable real-time processing of ultrasound or MRI images on-site, reducing reliance on centralized servers. The Raspberry Pi 3, noted for its performance and cost-effectiveness, is also used in healthcare for flexible, robust solutions that minimize data transmission and processing time. For smart city applications, Raspberry Pi devices can be effectively utilized in video surveillance systems. Their low power consumption and cost-effectiveness make them ideal for extensive deployment in both public and private areas, enabling real-time data collection and analysis. By integrating Raspberry Pi into these systems, the devices can perform complex recognition tasks independently, thereby reducing the need for centralized server resources [26]. These examples illustrate the versatility and importance of selecting the appropriate edge computing hardware for various AI and machine learning applications, balancing computational power with energy efficiency to meet the specific needs of different use cases. \n4.2 Results and Discussions In our experimental setup, we utilized FAISS implementations of IVF, LSH, HNSW, and PQ algorithms. For Vamana, DiskANN was used. Construction and parameter optimization were conducted on a server equipped with 28 CPU cores and 2TB of RAM. PQ was applied across all algorithms to ensure compatibility with memory constraints of edge devices. However, integrating PQ with DiskANN using standard tools posed challenges, and we were unable to deploy it on edge devices. Therefore, we will first present comparative ANNS performance results on our server and subsequently evaluate FAISS algorithms on edge devices. Table 3 provides details on the build timew and index size of the most accurate configurations achieved on the server. Following construction, these indices were stored and transferred to edge devices for conducting nearest-neighbor searches on the test dataset. Figure 1 illustrates a comparison of these algorithms on a server running on CPU, depicting the tradeoff between Queries Per Second (QPS) and accuracy. The graph highlights the balance between QPS and accuracy for each algorithm. Notably, DiskANN, which does not use any data compression and operates on an index built from the original data vectors, shows competitive performance relative to other approximate nearest neighbor search (ANNS) methods that utilize Product Quantization (PQ) for data compression. Furthermore, it is evident that FAISS algorithms, even with compression, can achieve high accuracy while maintaining high QPS. Figure 2 shows the performance of various algorithms on edge devices using CPU. FAISS was not supported on the Raspberry Pi 3 and Pi Zero, but the Raspberry Pi 4 proved to be a strong competitor in CPU-based approximate nearest-neighbor search. Despite similar CPU clock speeds and double the memory of the Pi 4, it performed over three times better than the Jetson Nano while being about one-fourth the price. The Jetson Xavier also outperformed the Jetson Orin by 1.5 times in CPU utilization for HNSW-PQ, despite having less memory and fewer CPU cores (8 versus 12). This discrepancy may be due to Xavier\u2019s larger L2 cache (8MB vs. Orin\u2019s 3MB). On the Jetson TX2, the IVF-PQ algorithm achieved higher QPS than HNSW-PQ at lower accuracy levels. Generally, HNSW-PQ outperformed IVF-PQ across all devices, unlike the server results. Figure 3 (left) shows that the Jetson Xavier had the highest throughput for both HNSW-PQ and IVF-PQ. The Raspberry Pi 4\u2019s CPU performance is comparable to the Jetson TX2 and Orin. Additionally, PQ performed better than LSH on lower-power devices like the Raspberry Pi 4 and Jetson Nano. For GPU comparison, we utilized the HNSW-PQ algorithm implementation of ANNS with the FAISS library to conduct inference on GPUs. The outcomes are illustrated in Figure 3 (right). As anticipated, the Queries Per Second (QPS) metric demonstrated higher performance on more powerful devices. It is intriguing that GPU acceleration provided a threefold improvement in throughput compared to CPU implementations. However, it\u2019s important to consider that GPU-based inference consumes double the power of CPU-based inference on these devices. DiskANN\u2019s lack of support for compression prevented us from fitting its index onto edge devices, so we evaluated it on our server. DiskANN allows true vector deletion, unlike FAISS, which uses \"soft\" deletion\u2014vectors are marked as deleted but remain in the index structure. We developed a test pipeline for insertion and deletion with DiskANN, simulating real-world data scenarios. Figure 4 shows QPS results for search, insertions, deletions, and overall performance across various batch sizes. Deletion throughput significantly affects pipeline performance, similar to insertion operations, due to the costly re-indexing process required. Larger batch sizes generally improved throughput, with search and insertion operations effectively managing batches up to 128 before plateauing. The diminishing returns in search and insertion had a greater impact compared to the slowdown in deletions. \n5 Conclusion and Future Work This study presents an experimental evaluation of various approximate nearest neighbor search (ANNS) algorithms across diverse hardware configurations, with a primary focus on edge devices. Our findings underscore several key insights: the performance superiority of specific algorithms tailored to particular devices and the suitability of different devices for distinct types of algorithms. Notably, our research demonstrates that cost-effective devices like the Raspberry Pi 4 can achieve competitive performance compared to more expensive and power-intensive Jetson devices in CPU-based tasks. Furthermore, we showcased the end-to-end workflow capabilities of DiskANN, particularly its support for true deletion of vectors. This feature proves invaluable for maintaining model relevance for potential shifts in data distribution over time and supports continuous on-device learning. Looking ahead, our findings highlight several promising research directions. First, optimizing FAISS algorithms for resource-constrained edge devices, such as the Raspberry Pi 3 and Zero, is crucial. Enhancing these algorithms to function efficiently with limited memory and processing power could improve their practical use and reduce power consumption. Additionally, our study identifies performance differences among devices with varying CPU capabilities. Future research should investigate why some algorithms perform better on devices with lower CPU power and explore which hardware specifications most impact performance. In conclusion, our study contributes to advancing the understanding of ANNS algorithm performance across edge devices, providing a foundation for optimizing these algorithms across diverse applications in machine learning and data-intensive tasks. \n6 Acknowledgements This work is supported by the German Research Foundation (DFG) under the COSMO project (grant No. 453130567), and by the European Union\u2019s Horizon WIDERA under the grant agreement No. 101079214 (AIoTwin), by the Federal Ministry for Education and Research Germany under grant number 01IS18037A (BIFOLD), and RIA research and innovation program under the grant agreement No. 101092908 (SmartEdge). References"}
{"text": "SimPhony: A Heterogeneous Electronic-Photonic AI System Cross-Layer Simulation Framework with Accurate, Versatile Device, Circuit, Layout Representation and Modeling \nSimPhony: A Device-Circuit-Architecture Cross-Layer Modeling and Simulation Framework for Heterogeneous Electronic-Photonic AI System\n\n Electronic-photonic integrated circuits (EPICs) offer transformative potential for next-generation high-performance AI but require interdisciplinary advances across devices, circuits, architecture, and design automation.\nThe complexity of hybrid systems makes it challenging even for domain experts to understand distinct behaviors and interactions across design stack.\nThe lack of a flexible, accurate, fast, and easy-to-use EPIC AI system simulation framework significantly limits the exploration of hardware innovations and system evaluations on common benchmarks.\nTo address this gap, we propose SimPhony 111We open-source our codes at https://github.com/ScopeX-ASU/SimPhony, a\ncross-layer modeling and simulation framework for heterogeneous electronic-photonic AI systems.\nSimPhony offers a platform that enables\n(1) generic, extensible hardware topology representation that supports heterogeneous multi-core architectures with diverse photonic tensor core designs;\n(2) optics-specific dataflow modeling with unique multi-dimensional parallelism and reuse beyond spatial/temporal dimensions;\n(3) data-aware energy modeling with realistic device responses, layout-aware area estimation, link budget analysis, and bandwidth-adaptive memory modeling; and\n(4) seamless integration with model training framework for hardware/software co-simulation.\nBy providing a unified, versatile, and high-fidelity simulation platform, SimPhony enables researchers to innovate and evaluate EPIC AI hardware across multiple domains, facilitating the next leap in emerging AI hardware. \nI Introduction\n Heterogeneous electronic-photonic integrated circuits (EPICs) are emerging as a next-generation platform for high-performance artificial intelligence (AI) computing.\nDemonstrated optical neural networks (ONNs) showcase breakthroughs in their performance and efficiency\u00a0[1, 2, 3, 4, 5, 6, 7, 8, 9].\nMany research focuses on materials, devices, and circuits to overcome technological barriers in photonic AI.\nSome cross-layer co-design\u00a0[10, 11, 12, 13, 14] and architecture optimization\u00a0[5, 4, 15, 16, 17, 18, 19] research have scaled these systems for real-world AI tasks.\nExploring the full stack of EPIC AI systems demands expertise across physics, device design, analog circuits, system architecture, electronic-photonic design automation (EPDA), and AI algorithms, as system-level evaluation is critical to understanding innovations at each individual design layer. However, the complexity of such hybrid system makes it challenging even for experts to grasp the behavior of each component and its interactions across software and hardware.\nKey challenges include:\n\u278a\u00a0Lack of Unified Representation of Distinct PTC Circuit Topology:\u00a0\nPhotonic tensor cores (PTCs) employ diverse optical principles for matrix computation, e.g., magnitude/phase modulation, wavelength/mode/time-division multiplexing (WDM, MDM, TDM), interference, diffraction, etc.\nSuch abundant design flexibility creates various PTC circuit topologies, e.g., weight bank\u00a0[20], triangular mesh\u00a0[21, 1], rectangular mesh\u00a0[22], butterfly mesh\u00a0[3], crossbar\u00a0[2, 4, 17], single WDM link\u00a0[23], etc.\nPrior simulators are modified from digital tools\u00a0[24] that only support array-like computing architectures and cannot represent diverse PTC topologies.\n\u278b\u00a0Lack of Support for Optics-Specific Dataflow and Parallelism:\nEPIC AI systems involve parallelism and resource sharing beyond temporal/spatial dimensions.\nThe combination of optical broadcasting, hierarchical accumulation, and multi-dimensional reuse patterns results in a complex dataflow.\nThe additional optical dimensions, like magnitude/phase, wavelength, polarization, and modes, further complicate the design space.\nThus, a flexible framework tailored to optics-specific needs is required.\n\u278c\u00a0Lack of Accurate Model-Circuit-Layout Co-Modeling:\nUnlike digital AI accelerators, analog systems integrate models tightly with devices/circuits, leading to inaccuracies in energy modeling due to unawareness of real workload data and precise device settings.\nBeyond simple analytical models, there is a strong need to incorporate rigorous simulations and even chip measurements into energy analysis.\nAdditionally, existing EPIC design tools overlook the layout when estimating the chip area.\nPrevious methods\u00a0[25, 4, 26] aggregate device footprints, leading to an underestimate of area.\nTherefore, a fast yet accurate layout estimator is essential for more reliable area analysis. In this work, we present SimPhony, an open-source cross-layer modeling and simulation framework for heterogeneous EPIC AI systems.\nBuilt with a customized EPIC device library, SimPhony enables the hierarchical construction of heterogeneous photonic architectures from arbitrary PTC circuit topologies.\nIntegrated with the ONN training library, SimPhony enables end-to-end simulation, including workload extraction, memory construction, and dataflow generation, while accurately analyzing system latency, data-aware energy, and layout-aware area. Our main contributions are as follows, We introduce SimPhony, a cross-layer simulation framework for EPIC AI systems, enabling end-to-end accurate performance and efficiency analysis. Unified PTC Representation:\nUtilizing our customized device library, we design a hierarchical netlist-based circuit representation that generates arbitrary PTC topologies with automatically derived scaling rules and critical paths. Photonics-Specific Dataflow Handling: SimPhony accommodates multi-dimensional parallelism and hierarchical accumulation, effectively integrating the unique characteristics and dataflow of photonic computing hardware. Accurate System Modeling: SimPhony provides accurate energy analysis based on real workload data and realistic device power models, along with accurate area analysis featuring auto-generated layout estimation. \nII Preliminary\n \nII-A Photonic Tensor Core Taxonomy and Modeling Challenges\n The diverse properties of PTC designs result in variations in circuit topology, devices, and operational principles, affecting speed and power.\nTable\u00a0I categorizes PTCs by expressivity, computing mechanism, operand range, and reconfiguration speed\u00a0[8].\nFor expressivity, universal PTCs can perform arbitrary matrix multiplication, e.g., micro-ring (MRR) arrays\u00a0[20], while subspace PTCs support only a subset of static linear transformation\u00a0[3].\nBased on the numerical range of input operands, full-range PTCs compute arbitrary values in one shot, whereas subspace coherent PTCs (e.g., Butterfly meshes) require two differential computations for full-range input/output.\nIncoherent PTCs, e.g., MRR arrays\u00a0[20], also require two computations to handle full-range inputs, while non-volatile phase change material (PCM) crossbar arrays\u00a0[2] need up to four cycles.\nReconfiguration speed is another key factor to determine its supported dataflow and operations.\nThermo-optic MZI arrays require matrix decomposition and thermal tuning to reconfigure the weights, leading to a delay of \u03bc\u2062s\ud835\udf07\ud835\udc60\\mu sitalic_\u03bc italic_s to m\u2062s\ud835\udc5a\ud835\udc60msitalic_m italic_s and limiting them to weight-stationary dataflows, unsuitable for dynamic self-attention.\nDynamic PTCs\u00a0[4, 17] use high-speed modulators for real-time matrix switching, enabling dynamic tensor products and output-static dataflows.\nDeveloping a modeling framework that accurately models complex system performance is challenging. PTCs also feature varied topologies beyond conventional crossbars, such as triangular\u00a0[21] and butterfly meshes\u00a0[3].\nArchitectural features like optical broadcast, multi-dimensional sharing\u00a0[4], analog-domain accumulation, and electrical-optical (E-O) interfaces further complicate system description and accurate modeling. \nII-B Photonic Accelerator Simulation Tools\n Prior photonic AI hardware is evaluated based on internal closed-source analytical device modeling and behavior-level modeling\u00a0[3, 28, 29, 15, 16], unaware of the numerical values in actual workloads.\nCimLoop is a recent analog NN simulator; its photonic version\u00a0[24] has showcased one architecture Albireo\u00a0[5].\nHowever, it focuses on architectural parameters (e.g., memory and dataflow), and lacks support for flexible PTC construction with photonics-specific dataflow and parallelism.\nIts oversimplified device modeling and the complicated interface of the Timeloop-based framework make it challenging for device and circuit researchers to explore hardware optimization efficiently.\nFurthermore, previous efforts have not interfaced with ONN training tools at the application level, lacking an integrated ONN training and device/circuit-centric system modeling infrastructure, which is needed to model EPIC AI system efficiency and performance. \nIII SimPhony: EPIC AI System Modeling and Simulation Framework\n Figure\u00a01 summarizes the simulation flow and key components of our proposed SimPhony framework.\nIt contains a customized electronic-photonic device library SimPhony-DevLib and supports flexible, hierarchical,\u00a0and parametric modeling of EPIC AI system architecture SimPhony-Arch.\nIntegrated with TorchONN model training toolkit, our simulation system SimPhony-Sim handles photonics-specific dataflow with bandwidth-adaptive memory hierarchy, data-aware power estimation, link budget analysis, and layout-aware chip area analysis. \nIII-A SimPhony-DevLib: Comprehensive and Customizable Electronic-Photonic Device Library\n One key novelty of our simulation framework is the support of a comprehensive and customizable device library SimPhony-DevLib, which lays the foundation for flexible architecture construction and accurate system modeling. Modeling Granularity: SimPhony-DevLib device modeling is based on experimental data reported, ensuring an accurate representation of device characteristics.\nSpecifically, the photonic device power models are obtained through Lumerical HEAT simulation or experimental measurements that are aware of actual device configuration, ensuring fast and accurate cost modeling. Modeling Approach: We organize key components into electrical and optical categories, including a variety of high-performance photonic and electronic devices.\nComprehensive device information is provided in detail to support accurate simulations of area, power, latency, and link budget.\nDevices from foundry PDKs can be plugged in.\nOur device library supports flexible scaling with different technology nodes, port numbers, working conditions, and more.\nFor example, digital-to-analog converters (DACs) in our library support power scaling with customized sampling rates and bit resolutions, enabling power optimization via gating or quantization.\nOn photonic device front, e.g., the electro-optic Mach-Zehnder modulator (MZM) is widely used for high-speed encoding.\nTo achieve precise performance modeling, we collect various properties such as spatial size, bandwidth, insertion loss, modulation efficiency, static power, extinction ratio, testing bitwidth, and more. \nIII-B SimPhony-Arch: Hierarchical, Parametric Heterogeneous EPIC AI System Architecture Builder\n To enable flexible construction of heterogeneous multi-core architectures with diverse PTC designs and dataflows, we introduce SimPhony-Arch, a hierarchical, parametric architecture builder. Existing simulators focus on dataflow-centric architectural modeling with fixed PTC designs, often neglecting device/circuit details.\nThis limits their suitability for fundamental device/circuit-level customization, underscoring the need for a universal representation to unify diverse PTC designs. To enable flexible PTC construction, we customize a netlist representation in SimPhony-Arch to describe devices as instances and port connectivity as directed 2-pin nets.\nUnlike electrical circuit netlists with undirected multi-pin nets, PTCs require directed 2-pin nets to capture the directional optical signal flow. Key observations of PTC design patterns inspire us to use modular circuit construction, avoiding manual scripting of large netlists.\nThis allows us to define a minimal building block denoted as node, e.g., a dot-product unit shown in Fig.\u00a02(a), and build the circuit according to specific scaling rules with a user-defined node connection topology.\nA weighted directed acyclic graph (DAG) is generated based on the node topology, as shown in Fig.\u00a02(b).\nThe topology and insertion loss-based edge weights are essential in link budget analysis and layout-aware area estimation.\nThis universal, hierarchical netlist interface also enables potential SPICE simulation and physical design as a future extension. In the following, we provide two case studies of how to construct representative PTC architectures in a parametric style. Case Study 1: Dynamic Array-style Tensor Cores TeMPO\u00a0[17, 4].\u00a0\nThe first case study demonstrates how to construct an array-style PTC architecture, TeMPO\u00a0[17, 4], designed for dynamic time-multiplexed tensor products, as shown in Fig.\u00a03(a).\nWe first define the architecture parameters: R\ud835\udc45Ritalic_R tiles, each containing C\ud835\udc36Citalic_C cores, with H\u00d7W\ud835\udc3b\ud835\udc4aH\\times Witalic_H \u00d7 italic_W dot-product nodes per core performing parallel computations.\nThen, we define the structure of the minimum building block, i.e., the dot-product unit, denoted as a node.\nA node netlist is used to describe the 6-device circuit topology using directed 2-pin nets to represent the waveguide connections and signal flow, shown in Fig.\u00a02(a).\nTo efficiently span the multi-core architecture without manually detailing every connection, we define scaling rules applied to each node and describe inter-node connections.\nThis approach supports parametric generation of the architecture, enabling automatic analysis of area, power, and link budget.\nFor example, the device area/power estimation engine will trace the netlist to count the number of devices considering hardware sharing.\nThere are R\u2062C\u2062H\u2062W\ud835\udc45\ud835\udc36\ud835\udc3b\ud835\udc4aRCHWitalic_R italic_C italic_H italic_W total nodes for parallel dot-product.\nAs the output of C\ud835\udc36Citalic_C cores in a tile are in-situ accumulated, integrators/ADCs can be shared and thus scaled by C\u2062H\u2062W\ud835\udc36\ud835\udc3b\ud835\udc4aCHWitalic_C italic_H italic_W.\nMZM group A encodes one matrix operand and can be broadcast to R\ud835\udc45Ritalic_R tiles.\nThus, the input encoders, i.e., DAC A and MZM A, are scaled by R\u2062H\ud835\udc45\ud835\udc3bRHitalic_R italic_H.\nThese scaling rules are expressed as customizable symbolic expressions in circuit description files, enabling user-defined reuse styles to suit specific designs.\nThe link budget analyzer will trace the netlist and auto-extract the longest path from the weighted DAG of the hierarchical netlist.\nEdge weights are automatically assigned to the insertion loss of incident vertices to match the parametric architecture settings, e.g., edge pointing to i\u20624\ud835\udc564i4italic_i 4 stores (CW\u22121)\u00d7(CW-1)\\times( italic_C italic_W - 1 ) \u00d7 the loss of device i\u20624\ud835\udc564i4italic_i 4. Case Study 2: Static Mesh-style MZI Array\u00a0[1, 22].\u00a0\nBesides array style, SimPhony-Arch can handle more challenging mesh-style PTCs, e.g., Clements-style MZI meshes in Fig.\u00a03(b).\nThe matrix is encoded by singular value decomposition (U\u03a3V)U\\Sigma V)italic_U roman_\u03a3 italic_V ), where unitary matrices U/V\ud835\udc48\ud835\udc49U/Vitalic_U / italic_V are decomposed to interleaved MZI meshes.\nTo parametrically generate MZI meshes, we follow the node description to define a single-MZI node-U\ud835\udc48Uitalic_U, node-\u03a3\u03a3\\Sigmaroman_\u03a3, and node-V\ud835\udc49Vitalic_V.\nOur flexible scaling rule allows users to independently build unitary matrices by scaling node-U/V\ud835\udc48\ud835\udc49U/Vitalic_U / italic_V by R\u2062C\u2062H\u2062(H\u22121)/2\ud835\udc45\ud835\udc36\ud835\udc3b\ud835\udc3b12RCH(H-1)/2italic_R italic_C italic_H ( italic_H - 1 ) / 2 times and build diagonal by scaling node-\u03a3\u03a3\\Sigmaroman_\u03a3 by R\u2062C\u2062min\u2061(H,W)\ud835\udc45\ud835\udc36\ud835\udc3b\ud835\udc4aRC\\min(H,W)italic_R italic_C roman_min ( italic_H , italic_W ) times, which is not representable by prior simulators that are based on arrays.\nDetails of the critical path and hardware sharing are omitted here, as they follow similar principles to Case Study 1. \nIII-C SimPhony-Sim: EPIC AI System Simulation Flow\n SimPhony-Sim is an end-to-end simulation flow, including NN model conversion and workload extraction to memory simulation and analysis of latency, area, and energy cost. For digital DNN accelerator simulation, model training and hardware mapping are sufficiently decoupled.\nIn contrast, analog mixed-signal EPIC AI systems require cross-layer co-design, resulting in closely coupled model training, conversion, mapping, and architecture simulation processes.\nOur SimPhony-Sim system can seamlessly interface with an open-source ONN training library, TorchONN\u00a0[30], that supports various customized ONN types.\nA digital DNN will be first converted to its analog optical version with layer-wise conversion, e.g., Conv2d to TeMPOConv2d.\nThe converted model will be trained with device non-ideality, quantization, pruning, and various co-design methodologies to ensure accuracy, robustness, and energy efficiency.\nThe converted ONN model will be parsed by SimPhony-Sim to extract the detailed workload configuration for each layer, including input/weight size, input/weight/output bitwidth, pruning mask, scaling factors, actual weight values, etc.\nWeight values can have different modes, e.g., matrix values, normalized device transmissions, phase shifts, or even control voltages, which are useful for precise value-aware power modeling.\nConvolution, linear, and attention layers will be converted to general matrix multiplication (GEMM) representations.\nOther less computation-intensive layers are offloaded to electrical processors and omitted here for simplicity.\nWith a layer-to-arch mapping configuration, we enable the flexibility to map different layers to different types of sub-architectures based on their compatibility and efficiency considerations, enabling heterogeneous computing paradigms. Besides the support for standard dataflow for GEMM, e.g., weight/input/output stationary, here, we emphasize unique photonic-specific mapping and parallelism in SimPhony-Sim. Multi-Dimension Parallelism and Hierarchical Accumulation.\u00a0\nBeyond the spatial and temporal dimensions of electrical hardware, optical systems have more physical dimensions for parallel computing and hardware sharing, e.g., spectral, polarization, modes, etc.\nFigure\u00a04 illustrates a partitioned GEMM workload mapped to a multi-core TeMPO architecture with multi-dimensional parallelism and hierarchical accumulation.\nAs shown in the nested loop representation, multiple wavelengths are used for spectral parallel summation, photocurrents from C\ud835\udc36Citalic_C cores are aggregated for analog-domain parallel summation, spatial parallelism is used for parallel outer product, temporal integration is used for analog sequential summation, and the partial sum is further sequentially accumulated digitally in the local buffer.\nBased on the mapping and parallelism, we will derive the system execution latency in units of cycles. Latency Penalty for Range-Restricted PTCs.\u00a0As highlighted in Section\u00a0II and Table\u00a0I, PTCs are constrained in their numerical range representation due to device modulation features, leading to varying processing times to complete full-range computations.\nWe denote the number of iterations to acquire full-range output as I\ud835\udc3cIitalic_I.\nEspecially for PTCs that can only encode positive inputs/weights, four times the execution cost is required to realize full-range output, i.e., I\ud835\udc3cIitalic_I=4.\nSimPhony will automatically analyze the tensor core property based on input/weight/output encoding properties and generate the corresponding dataflow with I\u00d7I\\timesitalic_I \u00d7 latency penalty. PTC Reconfiguration Latency Penalty.\u00a0\nIn weight-stationary PTCs, loading new weights is sometimes bottlenecked by slow device reprogramming rather than memory loading.\nFor instance, thermo-optic (TO) devices have a thermal time constant of \u00a010 \u00b5s, and writing to phase change material (PCM) cells incur a delay of over 100 ns.\nSimPhony-Sim automatically analyzes reprogramming latency and applies corresponding cycle penalty whenever weight loading causes circuit reconfiguration delays exceeding one clock cycle, e.g., 500 cycles per switch for 100 ns reconfiguration delay at 5 GHz.\nThe total latency of mapping one layer is \u03c4t\u2062o\u2062t=\u03c4load-input/weight+\u03c4write-out+I\u2062(\u03c4comp+\u03c4reconfig)subscript\ud835\udf0f\ud835\udc61\ud835\udc5c\ud835\udc61subscript\ud835\udf0fload-input/weightsubscript\ud835\udf0fwrite-out\ud835\udc3csubscript\ud835\udf0fcompsubscript\ud835\udf0freconfig\\tau_{tot}=\\tau_{\\text{load-input/weight}}+\\tau_{\\text{write-out}}+I(\\tau_{%\n\\text{comp}}+\\tau_{\\text{reconfig}})italic_\u03c4 start_POSTSUBSCRIPT italic_t italic_o italic_t end_POSTSUBSCRIPT = italic_\u03c4 start_POSTSUBSCRIPT load-input/weight end_POSTSUBSCRIPT + italic_\u03c4 start_POSTSUBSCRIPT write-out end_POSTSUBSCRIPT + italic_I ( italic_\u03c4 start_POSTSUBSCRIPT comp end_POSTSUBSCRIPT + italic_\u03c4 start_POSTSUBSCRIPT reconfig end_POSTSUBSCRIPT ). One of the key points to enabling photonics advantage in AI computing is sufficient data movement bandwidth and latency-hiding techniques.\nNote that we focus on memory bandwidth analysis and assume on-chip and cross-chiplet interconnects provide sufficient data transaction bandwidth, especially when optical interconnects are available for multi-Tbps signal fanout/broadcast bandwidth\u00a0[31].\nSimPhony-Arch adopts a four-level memory hierarchy consisting of off-chip High Bandwidth Memory (HBM), Global Buffer (GLB), Local Buffer (LB), and Register File (RF).\nEach memory level stores operands A, B, and the output in progressively smaller sizes:i.e., the entire model at the HBM level, a single layer at the GLB level, the processing matrix dimensions at the LB level, and data for a single cycle at the RF level.\nThe bandwidth of LB (B\u2062WL\u2062B\ud835\udc35subscript\ud835\udc4a\ud835\udc3f\ud835\udc35BW_{LB}italic_B italic_W start_POSTSUBSCRIPT italic_L italic_B end_POSTSUBSCRIPT) and RF (B\u2062WR\u2062F\ud835\udc35subscript\ud835\udc4a\ud835\udc45\ud835\udc39BW_{RF}italic_B italic_W start_POSTSUBSCRIPT italic_R italic_F end_POSTSUBSCRIPT) must accommodate the architecture\u2019s single-cycle processing throughput, calculated as B\u2062WL\u2062B,B\u2062WR\u2062F\u2265B\u2062y\u2062t\u2062e\u2062s\u2062P\u2062e\u2062r\u2062C\u2062y\u2062c\u2062l\u2062e\u00d7f\ud835\udc35subscript\ud835\udc4a\ud835\udc3f\ud835\udc35\ud835\udc35subscript\ud835\udc4a\ud835\udc45\ud835\udc39\ud835\udc35\ud835\udc66\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc43\ud835\udc52\ud835\udc5f\ud835\udc36\ud835\udc66\ud835\udc50\ud835\udc59\ud835\udc52\ud835\udc53BW_{LB},BW_{RF}\\geq BytesPerCycle\\times fitalic_B italic_W start_POSTSUBSCRIPT italic_L italic_B end_POSTSUBSCRIPT , italic_B italic_W start_POSTSUBSCRIPT italic_R italic_F end_POSTSUBSCRIPT \u2265 italic_B italic_y italic_t italic_e italic_s italic_P italic_e italic_r italic_C italic_y italic_c italic_l italic_e \u00d7 italic_f,\nwhere f\ud835\udc53fitalic_f is the PTC operating clock frequency.\nGLB\u2019s bandwidth (B\u2062WG\u2062L\u2062B\ud835\udc35subscript\ud835\udc4a\ud835\udc3a\ud835\udc3f\ud835\udc35BW_{GLB}italic_B italic_W start_POSTSUBSCRIPT italic_G italic_L italic_B end_POSTSUBSCRIPT) is calculated as B\u2062WG\u2062L\u2062B=M\u2062a\u2062x\u2062L\u2062a\u2062y\u2062e\u2062r\u2062S\u2062i\u2062z\u2062e\u22c5f/(Np\u22c5DP\u22c5Mp)\ud835\udc35subscript\ud835\udc4a\ud835\udc3a\ud835\udc3f\ud835\udc35\u22c5\ud835\udc40\ud835\udc4e\ud835\udc65\ud835\udc3f\ud835\udc4e\ud835\udc66\ud835\udc52\ud835\udc5f\ud835\udc46\ud835\udc56\ud835\udc67\ud835\udc52\ud835\udc53\u22c5subscript\ud835\udc41\ud835\udc5dsubscript\ud835\udc37\ud835\udc43subscript\ud835\udc40\ud835\udc5dBW_{GLB}=MaxLayerSize\\cdot f/(N_{p}\\cdot D_{P}\\cdot M_{p})italic_B italic_W start_POSTSUBSCRIPT italic_G italic_L italic_B end_POSTSUBSCRIPT = italic_M italic_a italic_x italic_L italic_a italic_y italic_e italic_r italic_S italic_i italic_z italic_e \u22c5 italic_f / ( italic_N start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT \u22c5 italic_D start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT \u22c5 italic_M start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ),\nwhere the partitioned matrix dimensions are Npsubscript\ud835\udc41\ud835\udc5dN_{p}italic_N start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT, Dpsubscript\ud835\udc37\ud835\udc5dD_{p}italic_D start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT, and Mpsubscript\ud835\udc40\ud835\udc5dM_{p}italic_M start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT.\nWe first profile the maximum bandwidth requirement (B\u2062W^^\ud835\udc35\ud835\udc4a\\widehat{BW}over^ start_ARG italic_B italic_W end_ARG) for all sub-architectures based on the GLB demand per cycle, considering data sharing and broadcast in a specific dataflow.\nTo enable full utilization of the computing cores without memory bottleneck, we adopt a SoTA multi-block SRAM design to meet the bandwidth demand \u2014\nA multi-block GLB can boost its bandwidth proportional to the number of blocks\u00a0[4, 17].\nSimPhony-Sim automatically searchs for the minimum number of required GLB blocks, #\u2062\u00a0of Blocks=\u03c4G\u2062L\u2062B\u22c5B\u2062W^/(bbus\u22c58)#\u00a0of Blocks\u22c5subscript\ud835\udf0f\ud835\udc3a\ud835\udc3f\ud835\udc35^\ud835\udc35\ud835\udc4a\u22c5subscript\ud835\udc4fbus8\\#\\text{ of Blocks}=\\tau_{GLB}\\cdot\\widehat{BW}/(b_{\\text{bus}}\\cdot 8)# of Blocks = italic_\u03c4 start_POSTSUBSCRIPT italic_G italic_L italic_B end_POSTSUBSCRIPT \u22c5 over^ start_ARG italic_B italic_W end_ARG / ( italic_b start_POSTSUBSCRIPT bus end_POSTSUBSCRIPT \u22c5 8 ),\nwhere \u03c4G\u2062L\u2062Bsubscript\ud835\udf0f\ud835\udc3a\ud835\udc3f\ud835\udc35\\tau_{GLB}italic_\u03c4 start_POSTSUBSCRIPT italic_G italic_L italic_B end_POSTSUBSCRIPT is the fastest cycle simulated by CACTI\u00a0[32], and bb\u2062u\u2062ssubscript\ud835\udc4f\ud835\udc4f\ud835\udc62\ud835\udc60b_{bus}italic_b start_POSTSUBSCRIPT italic_b italic_u italic_s end_POSTSUBSCRIPT is the buswidth in bits. Link budget analysis is important for the photonic systems to profile the critical-path insertion loss and derive the laser source power requirement and optical signal-to-noise ratio (SNR).\nTo obtain the IL on the critical path, we leverage our constructed hierarchical, weighted DAG representation of the architecture in Fig.\u00a03(a),\u00a03(b).\nFrom a given source node, i.e., laser, to a destination node, i.e., photodetector (PD), we will use the graph\u2019s longest path to get the critical path IL.\nGiven the PD sensitivity S\ud835\udc46Sitalic_S, to differentiate bi\u2062nsubscript\ud835\udc4f\ud835\udc56\ud835\udc5bb_{in}italic_b start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT-bit input levels with a target bit error rate, we can derive the lowest laser power required\u00a0[4, 3], where \u03b7W\u2062P\u2062Esubscript\ud835\udf02\ud835\udc4a\ud835\udc43\ud835\udc38\\eta_{WPE}italic_\u03b7 start_POSTSUBSCRIPT italic_W italic_P italic_E end_POSTSUBSCRIPT is the laser wall plug efficiency.\nNon-ideal modulation extinction ratio E\u2062R\ud835\udc38\ud835\udc45ERitalic_E italic_R is also considered a power penalty to recover the full modulation range\u00a0[20]. To accurately model the energy of EPIC AI systems, SimPhony-Sim captures data access energy via CACTI-simulated memory energy and computing energy based on actual operand values and realistic device power modeling. For data access cost, we derive the memory access size (Dm\u2062e\u2062msubscript\ud835\udc37\ud835\udc5a\ud835\udc52\ud835\udc5aD_{mem}italic_D start_POSTSUBSCRIPT italic_m italic_e italic_m end_POSTSUBSCRIPT) for off-chip HBM, GLB, LB, and RF based on the dataflow analysis and accumulate the energy cost with CACTI-simulated access energy per bit em\u2062e\u2062msubscript\ud835\udc52\ud835\udc5a\ud835\udc52\ud835\udc5ae_{mem}italic_e start_POSTSUBSCRIPT italic_m italic_e italic_m end_POSTSUBSCRIPT, i.e., Em\u2062e\u2062m=\u2211m\u2062e\u2062m\u223c{H\u2062B\u2062M,G\u2062L\u2062B,L\u2062B,R\u2062F}em\u2062e\u2062m\u2062Dm\u2062e\u2062msubscript\ud835\udc38\ud835\udc5a\ud835\udc52\ud835\udc5asubscriptsimilar-to\ud835\udc5a\ud835\udc52\ud835\udc5a\ud835\udc3b\ud835\udc35\ud835\udc40\ud835\udc3a\ud835\udc3f\ud835\udc35\ud835\udc3f\ud835\udc35\ud835\udc45\ud835\udc39subscript\ud835\udc52\ud835\udc5a\ud835\udc52\ud835\udc5asubscript\ud835\udc37\ud835\udc5a\ud835\udc52\ud835\udc5aE_{mem}=\\sum_{mem\\sim\\{HBM,GLB,LB,RF\\}}e_{mem}D_{mem}italic_E start_POSTSUBSCRIPT italic_m italic_e italic_m end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_m italic_e italic_m \u223c { italic_H italic_B italic_M , italic_G italic_L italic_B , italic_L italic_B , italic_R italic_F } end_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_m italic_e italic_m end_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT italic_m italic_e italic_m end_POSTSUBSCRIPT. For computing cost, SimPhony-Sim supports data-dependent energy analysis with accurate device power modeling, shown in Fig.\u00a05.\nFor analog hardware, the encoded data determines the device configuration, significantly impacting its power.\nHence, SimPhony accumulates the energy over cycles based on the values of the real operands.\nThis approach enables accurate energy profiling with fine-grained power gating from ONN pruning\u00a0[14].\nDevice power modeling is also critical.\nDefault library power references, like P\u03c0subscript\ud835\udc43\ud835\udf0bP_{\\pi}italic_P start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT for phase shifters (PSs), often overestimate actual power, while analytical models can be overly ideal.\nAs shown in Fig.\u00a05,\nSimPhony-Sim supports customized power models using analytical, simulation, or chip testing data for power estimation with different fidelity. The chip area is crucial for fabrication and packaging costs and guiding design optimization.\nSimPhony-Sim supports fast, realistic layout-aware area estimation. Unlike previous methods that simply sum all device footprints,\nSimPhony-Sim either takes in a user-defined bounding box or automatically generates a signal-flow-aware floorplan, as shown in Fig.\u00a06.\nSimPhony sets the placement site width to fit the longest device and attempts to hide other devices beneath it.\nThe floorplan follows the device\u2019s topological order from the netlist to adhere to the minimum bending rule in PIC placement, accounting for user-defined device/node spacing.\nThis approach closely matches the real layout area and can be potentially extended to interface with PIC placement tools. \nIV Evaluation Results\n \nIV-A SimPhony Validation\n we validate our simulation results by comparing them with architectural evaluation results reported in previous work.\nIt is important to clarify that, in the absence of system-level experimental demonstrations of multi-core photonic AI chips, we compare our results with prior architecture simulation results.\nWe re-emphasize that individual component data used are backed by experimental measurements, and area estimates are based on real chip layouts.\nThe functionality and accuracy of the ONN model are ensured by the TorchONN training framework, which is beyond the scope of our architecture performance modeling tool. GEMM Workloads: To validate the simulation results of SimPhony for GEMM, we compare the simulated area and energy metrics on a (280\u00d7\\times\u00d728)\u00d7\\times\u00d7(28\u00d7\\times\u00d7280) GEMM task against the reference values reported in the original TeMPO paper\u00a0[17] in Fig.\u00a07 with the following architecture settings.\nWe set the core width and height to 4 and the number of tiles and cores per tile to 2.\nThe area\u00a07(a) and energy\u00a07(b) breakdown from SimPhony match the reference results from TeMPO paper. Dynamic Transformer Workloads: For Transformers with self-attention operations, we validate SimPhony\u2019s area and energy results against Lightening-Transformer\u00a0[4] (LT), shown in Fig.\u00a08.\nwe simulate BERT-Base\u00a0[33] with a single (224\u00d7\\times\u00d7224) ImageNet-1K\u00a0[34] image.\nWe adopt LT\u2019s settings: 4 tiles, 2 cores per tile, each core sized 12\u00d7\\times\u00d712, with 12 wavelengths operating at 5 GHz.\nSince LT provides only power breakdowns, we report power instead of energy.\nSimPhony accurately reproduces the chip area when appropriate scaling factors are applied, matching LT\u2019s reported values.\nMinor deviations in photonic core and memory areas stem from differences in core area calculations, memory simulations, and device spacing. Our layout-aware estimator effectively generates realistic core areas. Device power aligns with LT\u2019s breakdown except for memory, where deviations result from different technology nodes used in memory simulation (SimPhony uses CACTI-45 nm\u00a0[32]; LT uses PCACTI-14 nm\u00a0[35]) and SRAM port/bus differences. \nIV-B SimPhony Use Cases\n To show the capability of SimPhony, we study multiple design examples by sweeping PTC architectural parameters in SimPhony to gain design insights and demonstrate their impacts on system performance and efficiency.\nAll simulations discussed below have a 4\u00d7\\times\u00d74 core size with 2 tiles and 2 cores per tile. As discussed in Section\u00a0III-C2, optical systems allow parallelism beyond spatial and temporal dimensions.\nHowever, data encoding in these dimensions requires extra power.\nFigure\u00a09(a) examines TeMPO\u00a0[17] under varying wavelength settings on a (280\u00d7\\times\u00d728)\u00d7\\times\u00d7(28\u00d7\\times\u00d7280) GEMM task while scaling MZM and laser sources with the number of wavelengths.\nIncreased wavelengths enhance parallelism, speeding up computation and reducing energy for components that do not scale with wavelength.\nHowever, the MZM\u2019s energy remains constant as the number of MZMs scale with # of wavelengths. To investigate how the ADC/DAC bit precision impacts the system power, in Fig.\u00a09(b), we sweep the energy for different tensor bitwidth.\nThe results show a clear trend of increasing energy with higher bits.\nUsers can leverage bitwidth simulation results to find the sweet spot for optimal efficiency. The exploration of layout-aware area estimation and data-dependent modeling is shown in Fig.\u00a010.\nThe layout-unaware method underestimates the node area by 72%, while our floorplan estimation enables accurate area estimation compared to the real layout.\nIn data-awareness evaluation, we focus on a weight-static PTC SCATTER\u00a0[14] where weight values impact the phase shifter\u2019s power.\nBy counting PS power based on real weight values, the PS energy decreases from 0.0537 \u03bc\ud835\udf07\\muitalic_\u03bcJ to 0.0215 \u03bc\ud835\udf07\\muitalic_\u03bcJ with an approximate power model.\nIf a rigorous device power model is used, the energy is further reduced to 0.0209 \u03bc\ud835\udf07\\muitalic_\u03bcJ with a substantial 60% reduction. Lastly, we show SimPhony\u2019s capability for heterogeneous architecture modeling with layer-to-sub-architecture mapping in Fig.\u00a011.\nThe convolutional layers of VGG-8 are mapped to SCATTER\u00a0[14], while the linear layers are mapped to MZI meshes\u00a0[1], while two sub-architectures share the same on-chip memory hierarchy.\nThis hybrid architecture modeling showcases SimPhony\u2019s flexibility in integrating hybrid systems and enabling fine-grained workload-to-hardware mapping.\nAs a future extension, SimPhony can be extended to enable automated design space exploration that combines the strengths of different photonic computing architectures in heterogeneous systems for diverse AI workloads. \nV Conclusion\n This paper presents SimPhony, a cross-layer modeling and simulation framework for EPIC AI hardware, enabling photonic-specific design evaluation and fair comparisons across implementations.\nBy emphasizing accurate device/circuit-level modeling with generic and extensible representations, SimPhony bridges hardware and software stacks to support flexible hardware construction, validation, and architecture exploration with multi-dimensional metric trade-offs.\nWe have validated SimPhony\u2019s accuracy against prior work and will continue expanding its capabilities to help researchers uncover insights and drive innovation in high-performance, energy-efficient photonic computing systems. References"}
{"text": "A low-rank balanced truncation approach for large-scale RLCk model order reduction based on extended Krylov subspace and a frequency-aware convergence criterion\n\n\n Model order reduction (MOR) is essential in integrated circuit design, particularly\u00a0when dealing with large-scale electromagnetic models extracted from complex designs. The numerous passive elements introduced in these models pose significant challenges in the simulation process. MOR methods based on balanced truncation (BT) help address these challenges by producing compact reduced-order models (ROMs) that preserve the original model\u2019s input-output port behavior. In this work, we present an extended Krylov subspace-based BT approach with a frequency-aware convergence criterion and efficient\u00a0implementation techniques for reducing large-scale models. Experimental results indicate that our method generates accurate and compact ROMs while achieving up to \u00d7\\times\u00d722 smaller ROMs with similar accuracy compared to ANSYS\u00a0RaptorX\u2122\u00a0ROMs for large-scale benchmarks. \n0.1 Introduction Electromagnetic model extraction is crucial for designing and verifying integrated circuits (ICs), enabling precise simulation of the passive elements of the design. However, simulating extracted RLCk models with millions of elements and multiple ports is extremely computationally expensive. Model order reduction (MOR) can reduce the complexity of such models while maintaining accurate input/output port behavior\u00a0[1, 2]. By constructing reduced-order models (ROMs) that capture the essential dynamics of the original system, MOR can significantly reduce simulation time, enabling faster design iterations in IC development. There are two main approaches to MOR. Moment matching (MM) methods are preferred for their efficiency, but they require manual selection of the number of moments\u00a0[1]. Most\u00a0importantly, they correlate the final ROM size with the number of moments and ports, limiting scalability. On the contrary, balanced truncation (BT) provides explicit theoretical bounds\u00a0for\u00a0the approximation error and is independent of the number of ports\u00a0[2]. However, BT is restricted to small-scale models due to the high computational complexity of solving Lyapunov equations\u00a0[2]. In this paper, we introduce an efficient low-rank BT technique to address the main scalability issue of the conventional BT approach. Specifically, we employ the extended Krylov subspace (EKS) method, which effectively solves the Lyapunov equations, drastically reducing the computational load of BT\u00a0[3]. Additionally, we incorporate a frequency-aware convergence criterion, ensuring accuracy in the frequency range of interest. Experimental evaluation indicates that the proposed method can be integrated into commercial extraction tools, such as the ANSYS RaptorX\u2122\u00a0[4], to generate more compact ROMs of large-scale multi-port\u00a0RLCk\u00a0models. \n0.2 Background Consider the modified nodal analysis (MNA) description\u00a0[5] of an n\ud835\udc5bnitalic_n-node, m\ud835\udc5amitalic_m-branch (inductive), p\ud835\udc5dpitalic_p-input, and q\ud835\udc5eqitalic_q-output RLCk circuit in the time domain: where \ud835\udc06\ud835\udc27\u2208\u211dn\u00d7nsubscript\ud835\udc06\ud835\udc27superscript\u211d\ud835\udc5b\ud835\udc5b\\mathbf{G_{n}}\\in\\mathbb{R}^{n\\times n}bold_G start_POSTSUBSCRIPT bold_n end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 italic_n end_POSTSUPERSCRIPT (node conductance matrix), \ud835\udc02\ud835\udc27\u2208\u211dn\u00d7nsubscript\ud835\udc02\ud835\udc27superscript\u211d\ud835\udc5b\ud835\udc5b\\mathbf{C_{n}}\\in\\mathbb{R}^{n\\times n}bold_C start_POSTSUBSCRIPT bold_n end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 italic_n end_POSTSUPERSCRIPT (node capacitance matrix), \ud835\udc0c\u2208\u211dm\u00d7m\ud835\udc0csuperscript\u211d\ud835\udc5a\ud835\udc5a\\mathbf{M}\\in\\mathbb{R}^{m\\times m}bold_M \u2208 blackboard_R start_POSTSUPERSCRIPT italic_m \u00d7 italic_m end_POSTSUPERSCRIPT (branch inductance matrix), \ud835\udc04\u2208\u211dn\u00d7m\ud835\udc04superscript\u211d\ud835\udc5b\ud835\udc5a\\mathbf{E}\\in\\mathbb{R}^{n\\times m}bold_E \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 italic_m end_POSTSUPERSCRIPT (node-to-branch incidence matrix), \ud835\udc2f\u2208\u211dn\ud835\udc2fsuperscript\u211d\ud835\udc5b\\mathbf{v}\\in\\mathbb{R}^{n}bold_v \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT (vector of node voltages), \ud835\udc22\u2208\u211dm\ud835\udc22superscript\u211d\ud835\udc5a\\mathbf{i}\\in\\mathbb{R}^{m}bold_i \u2208 blackboard_R start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT (vector of inductive branch currents), \ud835\udc2e\u2208\u211dp\ud835\udc2esuperscript\u211d\ud835\udc5d\\mathbf{u}\\in\\mathbb{R}^{p}bold_u \u2208 blackboard_R start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT (vector of input excitations), \ud835\udc011\u2208\u211dn\u00d7psubscript\ud835\udc011superscript\u211d\ud835\udc5b\ud835\udc5d\\mathbf{B}_{1}\\in\\mathbb{R}^{n\\times p}bold_B start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 italic_p end_POSTSUPERSCRIPT (input-to-node connectivity matrix), \ud835\udc32\u2208\u211dq\ud835\udc32superscript\u211d\ud835\udc5e\\mathbf{y}\\in\\mathbb{R}^{q}bold_y \u2208 blackboard_R start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT (vector of output measurements), and \ud835\udc0b1\u2208\u211dq\u00d7nsubscript\ud835\udc0b1superscript\u211d\ud835\udc5e\ud835\udc5b\\mathbf{L}_{1}\\in\\mathbb{R}^{q\\times n}bold_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_q \u00d7 italic_n end_POSTSUPERSCRIPT (node-to-output connectivity matrix).\nMoreover, we denote\n\ud835\udc2f\u02d9\u2062(t)\u2261d\u2062\ud835\udc2f\u2062(t)d\u2062t\u02d9\ud835\udc2f\ud835\udc61\ud835\udc51\ud835\udc2f\ud835\udc61\ud835\udc51\ud835\udc61\\dot{\\mathbf{v}}(t)\\equiv\\frac{d\\mathbf{v}(t)}{dt}over\u02d9 start_ARG bold_v end_ARG ( italic_t ) \u2261 divide start_ARG italic_d bold_v ( italic_t ) end_ARG start_ARG italic_d italic_t end_ARG and \ud835\udc22\u02d9\u2062(t)\u2261d\u2062\ud835\udc22\u2062(t)d\u2062t\u02d9\ud835\udc22\ud835\udc61\ud835\udc51\ud835\udc22\ud835\udc61\ud835\udc51\ud835\udc61\\dot{\\mathbf{i}}(t)\\equiv\\frac{d\\mathbf{i}(t)}{dt}over\u02d9 start_ARG bold_i end_ARG ( italic_t ) \u2261 divide start_ARG italic_d bold_i ( italic_t ) end_ARG start_ARG italic_d italic_t end_ARG.\nIf we now define the model order as N\u2261n+m\ud835\udc41\ud835\udc5b\ud835\udc5aN\\equiv n+mitalic_N \u2261 italic_n + italic_m, the state vector as \ud835\udc31\u2062(t)\u2261(\ud835\udc2f\u2062(t)\ud835\udc22\u2062(t))\ud835\udc31\ud835\udc61matrix\ud835\udc2f\ud835\udc61\ud835\udc22\ud835\udc61\\mathbf{x}(t)\\equiv\\begin{pmatrix}\\mathbf{v}(t)\\\\\n\\mathbf{i}(t)\\end{pmatrix}bold_x ( italic_t ) \u2261 ( start_ARG start_ROW start_CELL bold_v ( italic_t ) end_CELL end_ROW start_ROW start_CELL bold_i ( italic_t ) end_CELL end_ROW end_ARG ), and\u00a0also: then Eq.\u00a0(1) can be written in the generalized state-space form, or so-called descriptor form: The objective of MOR is to produce an equivalent ROM: where \ud835\udc06~,\ud835\udc02~\u2208\u211dr\u00d7r~\ud835\udc06~\ud835\udc02superscript\u211d\ud835\udc5f\ud835\udc5f\\mathbf{\\tilde{G}},\\mathbf{\\tilde{C}}\\in\\mathbb{R}^{r\\times r}over~ start_ARG bold_G end_ARG , over~ start_ARG bold_C end_ARG \u2208 blackboard_R start_POSTSUPERSCRIPT italic_r \u00d7 italic_r end_POSTSUPERSCRIPT, \ud835\udc01~\u2208\u211dr\u00d7p~\ud835\udc01superscript\u211d\ud835\udc5f\ud835\udc5d\\mathbf{\\tilde{B}}\\in\\mathbb{R}^{r\\times p}over~ start_ARG bold_B end_ARG \u2208 blackboard_R start_POSTSUPERSCRIPT italic_r \u00d7 italic_p end_POSTSUPERSCRIPT, \ud835\udc0b~\u2208\u211dq\u00d7r~\ud835\udc0bsuperscript\u211d\ud835\udc5e\ud835\udc5f\\mathbf{\\tilde{L}}\\in\\mathbb{R}^{q\\times r}over~ start_ARG bold_L end_ARG \u2208 blackboard_R start_POSTSUPERSCRIPT italic_q \u00d7 italic_r end_POSTSUPERSCRIPT, the reduced order r<<Nmuch-less-than\ud835\udc5f\ud835\udc41r<<Nitalic_r < < italic_N, and the output error is bounded as \u2016\ud835\udc32~\u2062(t)\u2212\ud835\udc32\u2062(t)\u20162\u2062<\u03b5|\u2062|\ud835\udc2e\u2062(t)||2evaluated-atsubscriptnorm~\ud835\udc32\ud835\udc61\ud835\udc32\ud835\udc612bra\ud835\udf00\ud835\udc2e\ud835\udc612||\\mathbf{\\tilde{y}}(t)-\\mathbf{y}(t)||_{2}<\\varepsilon||\\mathbf{u}(t)||_{2}| | over~ start_ARG bold_y end_ARG ( italic_t ) - bold_y ( italic_t ) | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT < italic_\u03b5 | | bold_u ( italic_t ) | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT for given \ud835\udc2e\u2062(t)\ud835\udc2e\ud835\udc61\\mathbf{u}(t)bold_u ( italic_t ) and small \u03b5\ud835\udf00\\varepsilonitalic_\u03b5. The output error bound can be expressed in the frequency domain as \u2016\ud835\udc32~\u2062(s)\u2212\ud835\udc32\u2062(s)\u20162\u2062<\u03b5|\u2062|\ud835\udc2e\u2062(s)||2evaluated-atsubscriptnorm~\ud835\udc32\ud835\udc60\ud835\udc32\ud835\udc602bra\ud835\udf00\ud835\udc2e\ud835\udc602||\\mathbf{\\tilde{y}}(s)-\\mathbf{y}(s)||_{2}<\\varepsilon||\\mathbf{u}(s)||_{2}| | over~ start_ARG bold_y end_ARG ( italic_s ) - bold_y ( italic_s ) | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT < italic_\u03b5 | | bold_u ( italic_s ) | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT via Plancherel\u2019s theorem\u00a0[6].\u00a0If are the transfer functions of the original model and the ROM, the corresponding\noutput\nerror\u00a0is: where ||.||\u221e||.||_{\\infty}| | . | | start_POSTSUBSCRIPT \u221e end_POSTSUBSCRIPT is the\n\u21122subscript\u21122\\mathcal{L}_{2}caligraphic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT matrix norm or \u210b\u221esubscript\u210b\\mathcal{H}_{\\infty}caligraphic_H start_POSTSUBSCRIPT \u221e end_POSTSUBSCRIPT norm of a rational transfer function.\nThus, to\u00a0bound this error, we need to bound the distance between the transfer functions:\u00a0\u2016\ud835\udc07~\u2062(s)\u2212\ud835\udc07\u2062(s)\u2016\u221e<\u03b5subscriptnorm~\ud835\udc07\ud835\udc60\ud835\udc07\ud835\udc60\ud835\udf00||\\mathbf{\\tilde{H}}(s)-\\mathbf{H}(s)||_{\\infty}~{}<~{}\\varepsilon| | over~ start_ARG bold_H end_ARG ( italic_s ) - bold_H ( italic_s ) | | start_POSTSUBSCRIPT \u221e end_POSTSUBSCRIPT < italic_\u03b5. \n0.3 MOR by Balanced Truncation BT relies on the computation of the controllability Gramian \ud835\udc0f\ud835\udc0f\\mathbf{P}bold_P and observability Gramian\u00a0\ud835\udc10\ud835\udc10\\mathbf{Q}bold_Q, which are calculated as the solutions of the following Lyapunov matrix equations [2]: The controllability Gramian \ud835\udc0f\ud835\udc0f\\mathbf{P}bold_P describes the degree to which the states are controllable by the inputs, while the observability Gramian \ud835\udc10\ud835\udc10\\mathbf{Q}bold_Q reflects the degree to which the states are observable at the outputs. A ROM can theoretically be generated by eliminating the states that are difficult to control or observe. However, in the original state-space coordinates, certain states may be easy to control but difficult to observe, and vice versa. The process of \u201cbalancing\u201d transforms the state vector to a new coordinate system, where the controllability and observability of each state are balanced, meaning each state is equally difficult to control and observe. An appropriate transformation \ud835\udc13\ud835\udc31\u2062(t)\ud835\udc13\ud835\udc31\ud835\udc61\\mathbf{Tx}(t)bold_Tx ( italic_t ) exists, leading to the balanced state-space model: This balanced representation preserves the system\u2019s transfer function \ud835\udc07\u2062(s)\ud835\udc07\ud835\udc60\\mathbf{H}(s)bold_H ( italic_s ) and simplifies to \ud835\udc0f\ud835\udc0f\\mathbf{P}bold_P\u00a0=\u00a0\ud835\udc10\ud835\udc10\\mathbf{Q}bold_Q\u00a0=\u00a0d\u2062i\u2062a\u2062g\u2062(\u03c31,\u03c32,\u2026,\u03c3N)\ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54subscript\ud835\udf0e1subscript\ud835\udf0e2\u2026subscript\ud835\udf0e\ud835\udc41diag(\\sigma_{1},\\sigma_{2},\\dots,\\sigma_{N})italic_d italic_i italic_a italic_g ( italic_\u03c3 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_\u03c3 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , italic_\u03c3 start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT )\u00a0[2],\n\u00a0where \u03c3isubscript\ud835\udf0e\ud835\udc56\\sigma_{i}italic_\u03c3 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are the Hankel singular values (HSVs). These HSVs are the square roots of the eigenvalues of the product \ud835\udc0f\ud835\udc10\ud835\udc0f\ud835\udc10\\mathbf{PQ}bold_PQ,\ni.e., \u03c3i=\u03bbi\u2062(\ud835\udc0f\ud835\udc10)subscript\ud835\udf0e\ud835\udc56subscript\ud835\udf06\ud835\udc56\ud835\udc0f\ud835\udc10\\sigma_{i}=\\sqrt{\\lambda_{i}(\\mathbf{PQ})}italic_\u03c3 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = square-root start_ARG italic_\u03bb start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_PQ ) end_ARG.\nIn the above balanced model,\nthe states with the largest HSVs are the easiest to both control and observe. If r\ud835\udc5fritalic_r of them are retained (truncating the N\u2212r\ud835\udc41\ud835\udc5fN-ritalic_N - italic_r states associated with the smallest HSVs), the error between the original and the reduced-order transfer functions is bounded as: The above serves as an \u201ca-priori\u201d criterion that offers flexibility by allowing either the specification of a ROM size r\ud835\udc5fritalic_r to compute the error or a target error (t\u2062a\u2062r\u2062g\u2062e\u2062t\u2062_\u2062e\u2062r\u2062r\u2062o\u2062r\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52\ud835\udc61_\ud835\udc52\ud835\udc5f\ud835\udc5f\ud835\udc5c\ud835\udc5ftarget\\_erroritalic_t italic_a italic_r italic_g italic_e italic_t _ italic_e italic_r italic_r italic_o italic_r) to determine\u00a0the\u00a0number r\ud835\udc5fritalic_r of HSVs to be preserved. This adaptability is a key advantage of BT over MM methods. Inputs: \ud835\udc06,\ud835\udc02,\ud835\udc01,\ud835\udc0b\ud835\udc06\ud835\udc02\ud835\udc01\ud835\udc0b\\mathbf{G},\\mathbf{C},\\mathbf{B},\\mathbf{L}bold_G , bold_C , bold_B , bold_L\nOutputs: \ud835\udc06~,\ud835\udc02~,\ud835\udc01~,\ud835\udc0b~~\ud835\udc06~\ud835\udc02~\ud835\udc01~\ud835\udc0b\\mathbf{\\tilde{G}},\\mathbf{\\tilde{C}},\\mathbf{\\tilde{B}},\\mathbf{\\tilde{L}}over~ start_ARG bold_G end_ARG , over~ start_ARG bold_C end_ARG , over~ start_ARG bold_B end_ARG , over~ start_ARG bold_L end_ARG The main steps of the BT procedure are summarized in Algorithm 1. The main limitation of BT is its high computational and memory cost, which makes it impractical for large-scale models (with N\ud835\udc41Nitalic_N over a few thousand states).\nThis is due to the computationally expensive operations required, such as solving Lyapunov equations and performing singular value decomposition (SVD), both of which have a complexity of O\u2062(N3)\ud835\udc42superscript\ud835\udc413O(N^{3})italic_O ( italic_N start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ). Additionally, they are applied on dense matrices, since the Gramians \ud835\udc0f,\ud835\udc10\ud835\udc0f\ud835\udc10\\mathbf{P},\\mathbf{Q}bold_P , bold_Q are dense even if the system matrices \ud835\udc02,\ud835\udc06,\ud835\udc01,\ud835\udc0b\ud835\udc02\ud835\udc06\ud835\udc01\ud835\udc0b\\mathbf{C},\\mathbf{G},\\mathbf{B},\\mathbf{L}bold_C , bold_G , bold_B , bold_L\u00a0are\u00a0sparse. However, the products (\ud835\udc02\u22121\u2062\ud835\udc01)\u2062(\ud835\udc02\u2212\ud835\udfcf\u2062\ud835\udc01)Tsuperscript\ud835\udc021\ud835\udc01superscriptsuperscript\ud835\udc021\ud835\udc01\ud835\udc47(\\mathbf{C}^{-1}\\mathbf{B})(\\mathbf{C^{-1}}\\mathbf{B})^{T}( bold_C start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_B ) ( bold_C start_POSTSUPERSCRIPT - bold_1 end_POSTSUPERSCRIPT bold_B ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT and \ud835\udc0bT\u2062\ud835\udc0bsuperscript\ud835\udc0b\ud835\udc47\ud835\udc0b\\mathbf{L}^{T}\\mathbf{L}bold_L start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_L have a much lower numerical rank compared\u00a0to\u00a0N\ud835\udc41Nitalic_N, as p,q<<Nmuch-less-than\ud835\udc5d\ud835\udc5e\ud835\udc41p,q<<Nitalic_p , italic_q < < italic_N. This results in low-rank Gramians that can be approximated using low-rank techniques, significantly reducing the complexity and memory requirements for solving the Lyapunov equations and performing SVD, which are now performed with a complexity of order k\ud835\udc58kitalic_k rather than N\ud835\udc41Nitalic_N. \n0.3.1 Low-rank BT MOR The essence of low-rank BT MOR is to iteratively project the Lyapunov equations onto a lower-dimensional Krylov subspace and solve the resulting small-scale equations to obtain low-rank approximate solutions of Eq.\u00a0(4). The k\ud835\udc58kitalic_k-dimensional standard Krylov subspace is defined as: where \ud835\udc06C\u2261\ud835\udc02\u22121\u2062\ud835\udc06,\ud835\udc01C\u2261\ud835\udc02\u22121\u2062\ud835\udc01formulae-sequencesubscript\ud835\udc06\ud835\udc36superscript\ud835\udc021\ud835\udc06subscript\ud835\udc01\ud835\udc36superscript\ud835\udc021\ud835\udc01\\mathbf{G}_{C}\\equiv\\mathbf{C}^{-1}\\mathbf{G},\\hskip 2.5pt\\mathbf{B}_{C}\\equiv%\n\\mathbf{C}^{-1}\\mathbf{B}bold_G start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT \u2261 bold_C start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_G , bold_B start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT \u2261 bold_C start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_B.\nIf \ud835\udc0a\u2208\u211dN\u00d7k\ud835\udc0asuperscript\u211d\ud835\udc41\ud835\udc58\\mathbf{K}\\in\\mathbb{R}^{N\\times k}bold_K \u2208 blackboard_R start_POSTSUPERSCRIPT italic_N \u00d7 italic_k end_POSTSUPERSCRIPT (k<<Nmuch-less-than\ud835\udc58\ud835\udc41k<<Nitalic_k < < italic_N) is a projection matrix whose columns span the k\ud835\udc58kitalic_k-dimensional standard Krylov subspace, then the projected Lyapunov equation (for the controllability Gramian \ud835\udc0f\ud835\udc0f\\mathbf{P}bold_P) onto \ud835\udca6k\u2062(\ud835\udc06C,\ud835\udc01C)subscript\ud835\udca6\ud835\udc58subscript\ud835\udc06\ud835\udc36subscript\ud835\udc01\ud835\udc36\\mathcal{K}_{k}(\\mathbf{G}_{C},\\mathbf{B}_{C})caligraphic_K start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_G start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT , bold_B start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ) is: (the same holds true for the observability Gramian \ud835\udc10\ud835\udc10\\mathbf{Q}bold_Q with \ud835\udc06CTsuperscriptsubscript\ud835\udc06\ud835\udc36\ud835\udc47\\mathbf{G}_{C}^{T}bold_G start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT, \ud835\udc0bTsuperscript\ud835\udc0b\ud835\udc47\\mathbf{L}^{T}bold_L start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT in place of \ud835\udc06Csubscript\ud835\udc06\ud835\udc36\\mathbf{G}_{C}bold_G start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT, \ud835\udc01Csubscript\ud835\udc01\ud835\udc36\\mathbf{B}_{C}bold_B start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT). The solution \ud835\udc17\u2208\u211dk\u00d7k\ud835\udc17superscript\u211d\ud835\udc58\ud835\udc58\\mathbf{X}\\in\\mathbb{R}^{k\\times k}bold_X \u2208 blackboard_R start_POSTSUPERSCRIPT italic_k \u00d7 italic_k end_POSTSUPERSCRIPT of Eq.\u00a0(5) can be back-projected to the N\ud835\udc41Nitalic_N-dimensional space to give an approximate solution \ud835\udc0f\u2248\ud835\udc0a\ud835\udc17\ud835\udc0aT\ud835\udc0fsuperscript\ud835\udc0a\ud835\udc17\ud835\udc0a\ud835\udc47\\mathbf{P}\\approx\\mathbf{K}\\mathbf{XK}^{T}bold_P \u2248 bold_KXK start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT for the original large-scale Eq.\u00a0(4), and a low-rank factor \ud835\udc19\u2208\u211dN\u00d7k\ud835\udc19superscript\u211d\ud835\udc41\ud835\udc58\\mathbf{Z}\\in\\mathbb{R}^{N\\times k}bold_Z \u2208 blackboard_R start_POSTSUPERSCRIPT italic_N \u00d7 italic_k end_POSTSUPERSCRIPT of \ud835\udc0f\ud835\udc0f\\mathbf{P}bold_P can be obtained as \ud835\udc19=\ud835\udc0a\ud835\udc14\u2062\ud835\udeba1/2\ud835\udc19\ud835\udc0a\ud835\udc14superscript\ud835\udeba12\\mathbf{Z}=\\mathbf{K}\\mathbf{U}\\mathbf{\\Sigma}^{1/2}bold_Z = bold_KU bold_\u03a3 start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT, where [\ud835\udc14,\ud835\udeba,\ud835\udc15]=S\u2062V\u2062D\u2062(\ud835\udc17)\ud835\udc14\ud835\udeba\ud835\udc15\ud835\udc46\ud835\udc49\ud835\udc37\ud835\udc17[\\mathbf{U},\\mathbf{\\Sigma},\\mathbf{V}]=SVD(\\mathbf{X})[ bold_U , bold_\u03a3 , bold_V ] = italic_S italic_V italic_D ( bold_X ) and \ud835\udc0f\u2248\ud835\udc19\ud835\udc19T\ud835\udc0fsuperscript\ud835\udc19\ud835\udc19\ud835\udc47\\mathbf{P}\\approx\\mathbf{Z}\\mathbf{Z}^{T}bold_P \u2248 bold_ZZ start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT. While the projection process is independent of the chosen subspace, its effectiveness heavily relies on it.\nThe convergence to an accurate solution can be accelerated by enhancing the standard Krylov subspace \ud835\udca6k\u2062(\ud835\udc06C,\ud835\udc01C)subscript\ud835\udca6\ud835\udc58subscript\ud835\udc06\ud835\udc36subscript\ud835\udc01\ud835\udc36\\mathcal{K}_{k}(\\mathbf{G}_{C},\\mathbf{B}_{C})caligraphic_K start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_G start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT , bold_B start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ) with information from the subspace \ud835\udca6k\u2062(\ud835\udc06C\u22121,\ud835\udc01C)subscript\ud835\udca6\ud835\udc58superscriptsubscript\ud835\udc06\ud835\udc361subscript\ud835\udc01\ud835\udc36\\mathcal{K}_{k}(\\mathbf{G}_{C}^{-1},\\mathbf{B}_{C})caligraphic_K start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_G start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT , bold_B start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ), which corresponds to the inverse matrix \ud835\udc06C\u22121superscriptsubscript\ud835\udc06\ud835\udc361\\mathbf{G}_{C}^{-1}bold_G start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, leading to the EKS\u00a0[3, 8]: The EKS method (EKSM) begins with the vectors {\ud835\udc01C,\ud835\udc06C\u22121\u2062\ud835\udc01C}subscript\ud835\udc01\ud835\udc36superscriptsubscript\ud835\udc06\ud835\udc361subscript\ud835\udc01\ud835\udc36\\{\\mathbf{B}_{C},\\mathbf{G}_{C}^{-1}\\mathbf{B}_{C}\\}{ bold_B start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT , bold_G start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_B start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT } and iteratively builds an EKS \ud835\udca6kC\u2062(\ud835\udc06C,\ud835\udc01C)superscriptsubscript\ud835\udca6\ud835\udc58\ud835\udc36subscript\ud835\udc06\ud835\udc36subscript\ud835\udc01\ud835\udc36\\mathcal{K}_{k}^{C}(\\mathbf{G}_{C},\\mathbf{B}_{C})caligraphic_K start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT ( bold_G start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT , bold_B start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ) of increasing dimension, solving the projected Lyapunov Eq.\u00a0(5) in each iteration, until a sufficiently accurate approximation of the solution of Eq.\u00a0(4) is achieved. The complete EKSM is presented in Algorithm\u00a02.\nBelow are some efficient implementation details: Matrix inversion by linear solves:\nAlgorithm 2 uses the system matrices \ud835\udc06\ud835\udc06\\mathbf{G}bold_G, \ud835\udc02\ud835\udc02\\mathbf{C}bold_C or \ud835\udc06Tsuperscript\ud835\udc06\ud835\udc47\\mathbf{G}^{T}bold_G start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT, \ud835\udc02Tsuperscript\ud835\udc02\ud835\udc47\\mathbf{C}^{T}bold_C start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT instead of \ud835\udc06C\u2261\ud835\udc02\u22121\u2062\ud835\udc06subscript\ud835\udc06\ud835\udc36superscript\ud835\udc021\ud835\udc06\\mathbf{G}_{C}\\equiv\\mathbf{C}^{-1}\\mathbf{G}bold_G start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT \u2261 bold_C start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_G or \ud835\udc06CT\u2261(\ud835\udc02\u22121\u2062\ud835\udc06)Tsubscriptsuperscript\ud835\udc06\ud835\udc47\ud835\udc36superscriptsuperscript\ud835\udc021\ud835\udc06\ud835\udc47\\mathbf{G}^{T}_{C}\\equiv(\\mathbf{C}^{-1}\\mathbf{G})^{T}bold_G start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT \u2261 ( bold_C start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_G ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT since the (generally dense) inverse matrices are only required for products with p\ud835\udc5dpitalic_p vectors (in step \u00a02) and 2\u2062p\u2062j2\ud835\udc5d\ud835\udc572pj2 italic_p italic_j vectors (in steps \u00a04 and \u00a011 of each iteration), which can be handled as linear solves like \ud835\udc02\ud835\udc18=\ud835\udc11\ud835\udc02\ud835\udc18\ud835\udc11\\mathbf{C}\\mathbf{Y}=\\mathbf{R}bold_CY = bold_R and \ud835\udc06\ud835\udc18=\ud835\udc11\ud835\udc06\ud835\udc18\ud835\udc11\\mathbf{G}\\mathbf{Y}=\\mathbf{R}bold_GY = bold_R (or \ud835\udc02T\u2062\ud835\udc18=\ud835\udc11superscript\ud835\udc02\ud835\udc47\ud835\udc18\ud835\udc11\\mathbf{C}^{T}\\mathbf{Y}=\\mathbf{R}bold_C start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_Y = bold_R, \ud835\udc06T\u2062\ud835\udc18=\ud835\udc11superscript\ud835\udc06\ud835\udc47\ud835\udc18\ud835\udc11\\mathbf{G}^{T}\\mathbf{Y}=\\mathbf{R}bold_G start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_Y = bold_R), using either direct or iterative methods\u00a0[9]. Handling of sparse/dense matrices: Matrix \ud835\udc0c\ud835\udc0c\\mathbf{M}bold_M of Eq.\u00a0(1) is typically very dense due to the huge number of mutual inductances. To efficiently handle both sparse (\ud835\udc02nsubscript\ud835\udc02\ud835\udc5b\\mathbf{C}_{n}bold_C start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT) and dense (\ud835\udc0c\ud835\udc0c\\mathbf{M}bold_M) matrix blocks of \ud835\udc02\ud835\udc02\\mathbf{C}bold_C, we use\nspecialized data structures and numerical\u00a0techniques. This includes parallel CPU-optimized methods for sparse matrices and GPU-accelerated techniques\u00a0[10] for dense matrices. Solution of the small-scale Lyapunov equations:\nTo solve the small-scale\u00a0(2\u2062p\u2062j\u00d72\u2062p\u2062j2\ud835\udc5d\ud835\udc572\ud835\udc5d\ud835\udc572pj\\times 2pj2 italic_p italic_j \u00d7 2 italic_p italic_j) Lyapunov\u00a0equations in step\u00a05 of each iteration,\nwe employ the Bartels-Stewart\u00a0algorithm\u00a0[7]. Convergence criterion: The error estimation\u00a0[11] relies on the ROM transfer function \ud835\udc07~\u2062(s)~\ud835\udc07\ud835\udc60\\mathbf{\\tilde{H}}(s)over~ start_ARG bold_H end_ARG ( italic_s ) and is described by: where \ud835\udc07~j\u2062(si)subscript~\ud835\udc07\ud835\udc57subscript\ud835\udc60\ud835\udc56\\mathbf{\\tilde{H}}_{j}(s_{i})over~ start_ARG bold_H end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) is the ROM transfer function at the j\ud835\udc57jitalic_j-th iteration (calculated at frequency si=2\u2062\u03c0\u2062fisubscript\ud835\udc60\ud835\udc562\ud835\udf0bsubscript\ud835\udc53\ud835\udc56s_{i}=2\\pi f_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 2 italic_\u03c0 italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) and l\ud835\udc59litalic_l is the number of evaluated frequency points evenly distributed across a frequency range [fm\u2062i\u2062nsubscript\ud835\udc53\ud835\udc5a\ud835\udc56\ud835\udc5bf_{min}italic_f start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT, fm\u2062a\u2062xsubscript\ud835\udc53\ud835\udc5a\ud835\udc4e\ud835\udc65f_{max}italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT]. The proposed criterion offers insight into the extent to which the transfer function changes between iterations at the frequencies of interest. Moreover, it proves to be practical and effective for circuit simulation problems, where designers are only interested in the circuit\u2019s behavior in certain frequency windows. The iterative procedure stops when the error remains below a certain threshold (t\u2062o\u2062l\ud835\udc61\ud835\udc5c\ud835\udc59tolitalic_t italic_o italic_l) for three consecutive iterations. Input: \ud835\udc06C\u2261\ud835\udc02\u22121\u2062\ud835\udc06,\ud835\udc01C\u2261\ud835\udc02\u22121\u2062\ud835\udc01formulae-sequencesubscript\ud835\udc06\ud835\udc36superscript\ud835\udc021\ud835\udc06subscript\ud835\udc01\ud835\udc36superscript\ud835\udc021\ud835\udc01\\mathbf{G}_{C}\\equiv\\mathbf{C}^{-1}\\mathbf{G},\\mathbf{B}_{C}\\equiv\\mathbf{C}^{%\n-1}\\mathbf{B}bold_G start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT \u2261 bold_C start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_G , bold_B start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT \u2261 bold_C start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_B (or \ud835\udc06CTsuperscriptsubscript\ud835\udc06\ud835\udc36\ud835\udc47\\mathbf{G}_{C}^{T}bold_G start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT, \ud835\udc0bTsuperscript\ud835\udc0b\ud835\udc47\\mathbf{L}^{T}bold_L start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT)\nOutput: \ud835\udc19\ud835\udc19\\mathbf{Z}bold_Z such that \ud835\udc0f\u2248\ud835\udc19\ud835\udc19T\ud835\udc0fsuperscript\ud835\udc19\ud835\udc19\ud835\udc47\\mathbf{P}\\approx\\mathbf{Z}\\mathbf{Z}^{T}bold_P \u2248 bold_ZZ start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT \n0.4 Experimental Evaluation We evaluated EKSM using RLCk models extracted from various circuits via ANSYS RaptorX\u2122\u00a0[4]. The evaluated designs consist of a phase-locked loop (PLL), an analog mixer, a time-interleaved digital-to-analog converter (TI_DAC), an injection-locked frequency multiplier (ILFM), a VGA circuit, hybrid couplers (HCs), Wilkinson power dividers (WPDs), and typical transceiver blocks, such as low-noise-amplifiers (LNAs) and oscillators (VCO). Their detailed characteristics are listed in Tables\u00a01 and\u00a02. Two experiments were conducted: in the first one, we used small-scale benchmarks (<<< 30K nodes), where the original and ROM transfer functions could be directly compared; in the second one, we used large-scale benchmarks and compared EKSM to golden RaptorX\u2122\u2009ROMs through S-parameter plotting. For the reduction process, t\u2062a\u2062r\u2062g\u2062e\u2062t\u2062_\u2062e\u2062r\u2062r\u2062o\u2062r\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52\ud835\udc61_\ud835\udc52\ud835\udc5f\ud835\udc5f\ud835\udc5c\ud835\udc5ftarget\\_erroritalic_t italic_a italic_r italic_g italic_e italic_t _ italic_e italic_r italic_r italic_o italic_r and t\u2062o\u2062l\ud835\udc61\ud835\udc5c\ud835\udc59tolitalic_t italic_o italic_l were set to 1e-2 and the number of frequencies l\ud835\udc59litalic_l was set to 20. Experiments were performed on a Linux server with a 2.80 GHz 16-thread CPU and 64 GB of memory. For the first experiment, the results are presented in Table\u00a01, where the error refers to the max relative error between the transfer functions of the original models and ROMs, which is calculated as \u2016\ud835\udc07\u2062(s)\u2212\ud835\udc07~\u2062(s)\u2016\u221e/\u2016\ud835\udc07\u2062(s)\u2016\u221esubscriptnorm\ud835\udc07\ud835\udc60~\ud835\udc07\ud835\udc60subscriptnorm\ud835\udc07\ud835\udc60||\\mathbf{H}(s)-\\mathbf{\\tilde{H}}(s)||_{\\infty}\\ /\\ ||\\mathbf{H}(s)||_{\\infty}| | bold_H ( italic_s ) - over~ start_ARG bold_H end_ARG ( italic_s ) | | start_POSTSUBSCRIPT \u221e end_POSTSUBSCRIPT / | | bold_H ( italic_s ) | | start_POSTSUBSCRIPT \u221e end_POSTSUBSCRIPT at the designated frequencies. For every benchmark, a base frequency of 100 MHz is chosen (fm\u2062i\u2062nsubscript\ud835\udc53\ud835\udc5a\ud835\udc56\ud835\udc5bf_{min}italic_f start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT = 1e+8) and the maximum frequency is set to twice the resonance frequency of each circuit (e.g., fm\u2062a\u2062xsubscript\ud835\udc53\ud835\udc5a\ud835\udc4e\ud835\udc65f_{max}italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT = 56e+9 for PLL\u2009@\u200928\u2009GHz). As can be seen, EKSM generates accurate and compact ROMs across every type of benchmark with a maximum error below 0.14%. Additionally, the convergence criterion effectively strikes a balance between error and final ROM size while being computationally efficient. This is also visible through the performance results, where the reduction time remains below 3 min for benchmarks with less than 30K nodes and the memory requirements are not significantly high. For the second experiment, the results are demonstrated in Table\u00a02. The S-parameters plots of Figure\u00a01 indicate that EKSM achieves accuracy close to that of RaptorX\u2122\u00a0while producing on average \u00d7\\times\u00d713.2 more compact ROMs. Although EKSM has higher reduction\u00a0time and memory requirements, they are still reasonable and can be significantly improved in futurk work. \n0.5 Conclusions An alternative MOR technique for accurately reducing large-scale RLCk models is introduced. The proposed low-rank BT approach incorporates an iterative EKS projection method with a frequency-aware convergence criterion to produce accurate and compact ROMs. Experimental results demonstrate that our method provides up to \u00d722 smaller ROMs than ROMs obtained by\u00a0ANSYS\u00a0RaptorX\u2122\u2009\u2009for large-scale benchmarks with negligible deviations in S-parameters. References"}
{"text": "Generalized Ping-Pong: Off-Chip Memory Bandwidth Centric Pipelining Strategy for Processing-In-Memory Accelerators Processing-in-memory (PIM) is a promising choice for accelerating deep neural networks (DNNs) featuring high efficiency and low power. However, the rapid upscaling of neural network model sizes poses a crucial challenge for the limited on-chip PIM capacity. When the PIM presumption of \u201cpre-loading DNN weights/parameters only once before repetitive computing\u201d is no longer practical, concurrent writing and computing techniques become necessary for PIM. Conventional methods of naive ping-pong or in\u00a0situ concurrent write/compute scheduling for PIM cause low utilization of off-chip memory bandwidth, subsequently offsetting the efficiency gain brought by PIM technology. To address this challenge, we propose an off-chip memory bandwidth centric pipelining strategy, named \u201cgeneralized ping-pong\u201d, to maximize the utilization and performance of PIM accelerators toward large DNN models. The core idea of the proposed generalized ping-pong strategy is to evenly distribute the active time and fully utilize the off-chip memory bandwidth. Based on a programmable and scalable SRAM PIM architecture, we quantitatively analyze and compare the generalized ping-pong with the conventional scheduling strategies of naive ping-pong and in\u00a0situ write/compute for PIM. Experiments show that the generalized ping-pong strategy achieves acceleration of over 1.67\u00d7\\times\u00d7 when fully utilizing the off-chip memory bandwidth. When further limiting the off-chip memory bandwidth ranging in 8\u223csimilar-to\\sim\u223c256 bytes per clock cycle,\nthe proposed generalized ping-pong strategy accelerates 1.22\u223csimilar-to\\sim\u223c7.71\u00d7\\times\u00d7 versus naive ping-pong. The developed PIM accelerator design with the generalized ping-poing strategy is open-sourced at https://github.com/rw999creator/gpp-pim. \nI Introduction\n Processing-In-Memory (PIM) is an innovative approach that holds the potential to significantly accelerate deep learning operations by enabling computations within memory arrays rather than transferring data back and forth between processing units and storage mediums\u00a0[1].\nThe fundamental concept of PIM revolves around integrating computing circuits within or near memory arrays to process data directly on stored information, especially vector-matrix multiplication\u00a0[2, 3]. However, deep neural network (DNN) models, following the deep learning scaling law, are scaling up at an exponential speed\u00a0[4, 5, 6, 7, 8], inflicting unprecedented challenges for the limited on-chip PIM capacity in that most of the conventional PIM architectures hold the presumption that loading weights (parameters) of deep learning models only once before repetitive computation based on a weight-stationary parallelism scheme\u00a0[9]. In contrast, the trending largest ever deep learning models (e.g. Transformer-based large language models\u00a0[10, 11] and large multimodal models\u00a0[12]) have required reloading DNN weights in PIM architectures into a necessary feature\u00a0[13].\nThe weights are sliced and programmed to the PIM subarray (i.e. macros) in batches amid PIM computation for general matrix multiplication (GeMM), i.e. concurrent write/compute (Fig.\u00a01). There are two existing practical strategies to schedule the weight writing and PIM computation\u00a0[14, 15].\n(a) In\u00a0situ write/compute strategy stalls the computation of PIM macro for rewriting new weights. This strategy aims to synchronize all PIM macros for rewriting and computing, which degrades the utilization rate and subsequently offsets the performance gain brought by PIM.\n(b) Naive ping-pong strategy facilitates concurrent write/compute pairing of two PIM macros, where one is computing and the other is updating weights. Although the naive ping-pong strategy hides the weight rewriting delay, it hardly balances the two operations, causing potential pipeline bubbles and low utilization. In other words,the compute time and weight updating time are different in most cases, resulting some PIM macros keep waiting for the other ones.\nWe observe that the existing two common strategies have certain limitations and are primarily focused on the PIM chip itself, hardly taking into account the impact of off-chip memory bandwidth on the overall performance. To address this challenge, we propose an off-chip memory bandwidth centric pipelining strategy, named \u201cgeneralized ping-pong\u201d, to maximize the utilization and performance of PIM accelerators toward large DNN models. The core idea of the proposed generalized ping-pong strategy is to evenly distribute the active time and fully utilize the off-chip memory bandwidth to achive high utilization of PIM arrays and off-chip memory bandwidth. Moreover, we tailor a scalable PIM architecture equipped with an assembler and customized instruction set, thereby analyzing metrics such as execution time, peak bandwidth requirements, and macro utilization across different strategies. The developed PIM accelerator design with the generalized ping-poing strategy is open-sourced at https://github.com/rw999creator/gpp-pim. Experiments show that the generalized ping-pong strategy achieves an acceleration of over 1.67\u00d7\\times\u00d7 when fully utilizing the off-chip memory bandwidth. When further limiting the off-chip memory bandwidth ranging from 8\u223csimilar-to\\sim\u223c256 bytes per clock cycle, the proposed generalized ping-pong strategy accelerates 1.22\u223csimilar-to\\sim\u223c7.71\u00d7\\times\u00d7 versus the existing naive ping-pong strategy. \nII Preliminaries\n \nII-A SRAM-Based PIM Designs\n PIM GeMM accelerator consists of multiple PIM vector-matrix multiplication (VMM) macros (subarrays) to perform complete GeMM operations (Fig.\u00a02)\u00a0[16, 17, 18, 19]. Each PIM macro works in two primary operational modes: memory model and compute mode\u00a0[20]. The memory mode serves a crucial role in loading weights/parameters into the PIM macro for maximum reuse in the compute mode. The compute mode is dedicated to performing in-memory vector-matrix multiplication (VMM) computations that leverage the physical locality of data within SRAM bitcells. Static Random Access Memory (SRAM)-based PIM offers both fast computing speed in the compute mode and low read/write latency in the memory mode. Also, SRAM is more appropriate for repetitively reloading with over 1015 bitcell endurance. However, the density of SRAM-PIM leads to limited on-chip capacity (e.g. 16kb\u223csimilar-to\\sim\u223c4.5Mb/macro). Toward the upscaled deep learning models, concurrent write/compute strategies is in urgent need for SRAM-based PIM. \nII-B Existing Concurrent Write/Compute Strategies\n Fig.\u00a03 illustrates the comparison between different concurrent write/compute strategies using an exemplary PIM accelerator comprising 4 PIM macros.\nFig.\u00a03(a) illustrates the in\u00a0situ write/compute strategy. It synchronizes all PIM macros for writing or computing.\nOnly writing occupies the off-chip memory bandwidth, reflecting an intermittent characteristic.\nFig.\u00a03(b) depicts the naive ping-pong strategy\u00a0[21].\nWith >>>2 PIM macros, the naive ping-pong strategy divides all macros into two groups, say, bank1 and bank2. While bank1 performs computations for the nth GeMM operation, bank2 loads the weights for the (n+1)th opeartion; once the computations for the nth operation are completed, bank1 loads the weights for the (n+1)th, and bank2 executes computations for the (n+1)th operation.\nThis partitioning of computation and rewriting areas is achieved through two methods: inter-macro ping-pong\u00a0[14], which partitions between macros, and intra-macro ping-pong, which partitions within a macro\u00a0[22, 23, 24, 25, 26].\nIt alleviates the utilization for off-chip memory bandwidth, but idle time still exists depending on the comparison between intrinsic PIM macro computing throughput and data capacity\u00a0[27, 28]. \nIII Generalization for Ping-Pong Pipelining\n In order to achieve full usage for the off-chip memory bandwidth, we propose to generalize the ping-pong pipelining strategy for arbitan rary number of cores.\nFirstly, we would like to quantitatively analyze the utilization for the in\u00a0situ write/compute strategy and the naive ping-pong strategy. Firstly, we would like to formulate the latency for the memory mode and compute mode.\nGiven that both weight rewriting and computation are essential operations, we posit that a macro is considered \u201cidle\u201d when it is neither performing rewriting nor computation.\nWhen the weight reloading time is less than the PIM time, the rewritten bank has to wait for the PIM bank to finish the computation task of the current layer before starting the computation of the next GeMM operation.\nAssume s\u2062i\u2062z\u2062em\u2062a\u2062c\u2062r\u2062o\ud835\udc60\ud835\udc56\ud835\udc67subscript\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc50\ud835\udc5f\ud835\udc5csize_{macro}italic_s italic_i italic_z italic_e start_POSTSUBSCRIPT italic_m italic_a italic_c italic_r italic_o end_POSTSUBSCRIPT, s\u2062i\u2062z\u2062eO\u2062U\ud835\udc60\ud835\udc56\ud835\udc67subscript\ud835\udc52\ud835\udc42\ud835\udc48size_{OU}italic_s italic_i italic_z italic_e start_POSTSUBSCRIPT italic_O italic_U end_POSTSUBSCRIPT, ni\u2062nsubscript\ud835\udc5b\ud835\udc56\ud835\udc5bn_{in}italic_n start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT, and s\ud835\udc60sitalic_s represent macro size, operation unit size, number of input vector words for VMM calculaton, and rewrite speed, respectively. During a complete cycle of write and compute, the compute time is: t\u2062i\u2062m\u2062eP\u2062I\u2062M=s\u2062i\u2062z\u2062em\u2062a\u2062c\u2062r\u2062o\u2217ni\u2062ns\u2062i\u2062z\u2062eO\u2062U\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40\ud835\udc60\ud835\udc56\ud835\udc67subscript\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc50\ud835\udc5f\ud835\udc5csubscript\ud835\udc5b\ud835\udc56\ud835\udc5b\ud835\udc60\ud835\udc56\ud835\udc67subscript\ud835\udc52\ud835\udc42\ud835\udc48time_{PIM}=\\frac{size_{macro}*n_{in}}{size_{OU}}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT = divide start_ARG italic_s italic_i italic_z italic_e start_POSTSUBSCRIPT italic_m italic_a italic_c italic_r italic_o end_POSTSUBSCRIPT \u2217 italic_n start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT end_ARG start_ARG italic_s italic_i italic_z italic_e start_POSTSUBSCRIPT italic_O italic_U end_POSTSUBSCRIPT end_ARG\nThe writing time is t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e=s\u2062i\u2062z\u2062em\u2062a\u2062c\u2062r\u2062os\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc67subscript\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60time_{rewrite}=\\frac{size_{macro}}{s}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT = divide start_ARG italic_s italic_i italic_z italic_e start_POSTSUBSCRIPT italic_m italic_a italic_c italic_r italic_o end_POSTSUBSCRIPT end_ARG start_ARG italic_s end_ARG.\nWhen the PIM time is greater than the writing time, the macro utilization is: When the PIM time is less than the rewriting time, the macro utilization is: With this formulation, Fig.\u00a04 shows t\u2062i\u2062m\u2062eP\u2062I\u2062M/t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{PIM}/time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT / italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT ratio and macro utilization for the naive ping-pong strategy under various ni\u2062nsubscript\ud835\udc5b\ud835\udc56\ud835\udc5bn_{in}italic_n start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT within a specific PIM architecture configuration. In this example, the macro size s\u2062i\u2062z\u2062em\u2062a\u2062c\u2062r\u2062o\ud835\udc60\ud835\udc56\ud835\udc67subscript\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc50\ud835\udc5f\ud835\udc5csize_{macro}italic_s italic_i italic_z italic_e start_POSTSUBSCRIPT italic_m italic_a italic_c italic_r italic_o end_POSTSUBSCRIPT is set to 32\u00d7\\times\u00d732 bytes, the output unit size s\u2062i\u2062z\u2062eO\u2062U\ud835\udc60\ud835\udc56\ud835\udc67subscript\ud835\udc52\ud835\udc42\ud835\udc48size_{OU}italic_s italic_i italic_z italic_e start_POSTSUBSCRIPT italic_O italic_U end_POSTSUBSCRIPT is set to 4\u00d7\\times\u00d78 bytes, and the bandwidth s\ud835\udc60sitalic_s is set to 4byte/cycle. It can be observed that only when the number of inputs ni\u2062nsubscript\ud835\udc5b\ud835\udc56\ud835\udc5bn_{in}italic_n start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT equals 8, where t\u2062i\u2062m\u2062eP\u2062I\u2062M=t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{PIM}=time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT = italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT (i.e. matching the computing time and weight reloading time), at which point the naive ping-pong strategy achieves the highest macro utilization rate. Apart from this scenario, the naive ping-pong strategy significantly reduces macro utilization. With the aforementioned analysis, in order to maintain the highest macro utilization and off-chip bandwidth utilization during execution for varying values of ni\u2062nsubscript\ud835\udc5b\ud835\udc56\ud835\udc5bn_{in}italic_n start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT, we propose the generalized ping-pong strategy, which directly focuses on the ratio of t\u2062i\u2062m\u2062eP\u2062I\u2062M/t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{PIM}/time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT / italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT, and adjusts the start time of each macro execution. This approach averages the demand for off-chip bandwidth across each cycle, thereby reducing the peak demand for off-chip bandwidth. Simultaneously, each macro will immediately transition to the next write/compute operation upon completing the current one, thereby sustaining the highest macro utilization rate. Fig.\u00a03(c) illustrates the timing diagram and off-chip memory bandwidth utilization of proposed generalized ping-pong pipeline. The core idea of the generalized ping-pong is maintain a peak usage for the off-chip memory bandwidth with multi-core PIM accelerators. It groups multiple macros for writing and for computing. A deep pipelined pattern is exploited with balanced writing (memory bandwidth occupation) and PIM computing. This scheme has both advantages of in situ write/compute (consistently maintain a high macro utilization rate) and naive ping-pong (keeping high utilization rate for off-chip memory bandwidth). Assuming the presence of 4 macros in a PIM accelerator, when the ratio of weight updating to computation time is 1:3, macro2 initiates its weight updating process subsequent to the completion of macro1\u2019s rewrite. This sequence continues with macro3 and macro4, effectively distributing the bandwidth demand across each cycle. In this example, compared to the in situ write/compute strategy and the naive ping-pong, the proportion of bandwidth idle time in generalized ping-pong decreased from 75% and 66% to 0%, while the peak bandwidth demand is reduced to 25% of that required by the in situ write/compute approach. The macro utilization rate in generalized ping-pong remains at 100%, as the strategy does not induce idle states in the macros. Less bandwidth idle time and higher macro utilization ensure that generalized ping-pong delivers optimal performance under the same bandwidth constraints. \nIV Implement/Deploy Generalized Ping-Pong\n Generalized ping-pong strategy can improve the performance in two cases: (a) design phase: design space exploration for full usage of off-chip memory bandwidth in designing a PIM accelerator before tape-out; (b) runtime phase: scheduling PIM macros write/compute operations toward the maximum off-chip memory bandwidth utilization after PIM accelerator ASIC fabrication. \nIV-A Design Phase Optimization\n To implement the proposed generalized ping-pong, we choose PUMA\u00a0[29] design as a synthesizable base PIM accelerator architecture. It executes GeMM computing with compilation optimization. In addition to the original PUMA accelerator design, here we revise the PIM-oriented instruction set architecture (ISA)\u00a0[30]. This base architecture supports the aforementioned in\u00a0situ write/compute strategy, naive ping-pong strategy and the proposed generalized ping-pong stratety. The ISA comes with an assembler to convert assembly code into binary machine code. The focused scheduling strategies leads to different assembly code for different pipelined execution. Fig.\u00a05 shows the overall revised base architecture. It consists of a global weight memory, a global input memory, a global intermediate result memory, and an instruction memory, which transmit instructions and data between the core and them. The intermediate results are accumulated using vector processing unit (VPU). The architecture also includes a top controller and an instruction generation module. Each PIM core consists of 4 PIM macros, a buffer for storing weights/inputs/intermediate results, a control unit, and core instruction memory. The generalized execution unit is for managing the progress of instruction execution by the core control unit. By allowing or prohibiting the core control unit to operate on specific macros based on the current execution strategy, it enables a specific number of macros to synchronize and perform write/compute operations concurrently. \nIV-B Generalized Ping-Pong in Exploring Design Space\n During the design phase of PIM accelerators, we start from a given off-chip bandwidth and perform the design space exploration with the target generalized ping-pong scheduling strategy. Generalized ping-pong can offer enhanced computational throughput or reduced area overhead. Because of the interruption of PIM computation by weight rewrite, the pursuit is to minimize the number of rewrite operation. Ideally, weights should be written to the macro only once for further reuse. All input vectors should complete VMM with the weights already loaded into the PIM array before the next weight rewrite. However, both input vectors and intermediate result vectors require buffering in on-chip memory. Due to the limited capacity of on-chip memory, the number of input vectors that can be processed at one time is restricted, necessitating the computation of a large number of inputs in batches. This results in a fixed ratio of weight rewrite time to PIM computation time, enabling the integration of the generalized ping-pong strategy into the hardware design phase. To find the sweet point of 100% utilization of off-chip memory bandwidth, the design exploration should take generalized ping-pong scheduling into account to match the PIM memory capacity and computing throughput. Table\u00a0I presents the parameters used in the model. The time of PIM computation is contingent upon the velocity at which the macro completes vector-matrix operations and the number of vectors that need to be computed within a batch. In generalized ping-pong, the time for a single weight rewrite is: t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e=s\u2062i\u2062z\u2062em\u2062a\u2062c\u2062r\u2062o/s\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc67subscript\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60time_{rewrite}=size_{macro}/sitalic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT = italic_s italic_i italic_z italic_e start_POSTSUBSCRIPT italic_m italic_a italic_c italic_r italic_o end_POSTSUBSCRIPT / italic_s, and the time for a PIM compute is t\u2062i\u2062m\u2062eP\u2062I\u2062M=s\u2062i\u2062z\u2062em\u2062a\u2062c\u2062r\u2062o\u22c5ni\u2062n/s\u2062i\u2062z\u2062eO\u2062U\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40\u22c5\ud835\udc60\ud835\udc56\ud835\udc67subscript\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc50\ud835\udc5f\ud835\udc5csubscript\ud835\udc5b\ud835\udc56\ud835\udc5b\ud835\udc60\ud835\udc56\ud835\udc67subscript\ud835\udc52\ud835\udc42\ud835\udc48time_{PIM}=size_{macro}\\cdot n_{in}/size_{OU}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT = italic_s italic_i italic_z italic_e start_POSTSUBSCRIPT italic_m italic_a italic_c italic_r italic_o end_POSTSUBSCRIPT \u22c5 italic_n start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT / italic_s italic_i italic_z italic_e start_POSTSUBSCRIPT italic_O italic_U end_POSTSUBSCRIPT.\nThe number of macros that can be supported under a fixed off-chip bandwidth (with full usage) is given by Note in the ping-pong strategy, where macros are divided into two groups that rewrite alternately, the average bandwidth demand per macro is reduced to (s/2)\ud835\udc602(s/2)( italic_s / 2 ). Generalized ping-pong sets the number of macros that rewrite simultaneously according to the ratio of t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT to t\u2062i\u2062m\u2062eP\u2062I\u2062M\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40time_{PIM}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT, with each macro\u2019s average bandwidth demand being t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\u2217st\u2062i\u2062m\u2062eP\u2062I\u2062M+t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52\\frac{time_{rewrite}*s}{time_{PIM}+time_{rewrite}}divide start_ARG italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT \u2217 italic_s end_ARG start_ARG italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT + italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT end_ARG, and the number of macros that can be supported is given by When the ratio of t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT to t\u2062i\u2062m\u2062eP\u2062I\u2062M\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40time_{PIM}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPTM is not equal to 1, the naive ping-pong strategy may result in idle states of macros, whereas the in\u00a0situ write/compute and generalized ping-pong strategies remain unaffected. As a result, the performance of every macro under the ping-pong strategy reduce to\nt\u2062i\u2062m\u2062eP\u2062I\u2062M+t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062et\u2062i\u2062m\u2062eP\u2062I\u2062M+t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e+|t\u2062i\u2062m\u2062eP\u2062I\u2062M\u2212t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e|\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52\\frac{time_{PIM}+time_{rewrite}}{time_{PIM}+time_{rewrite}+|time_{PIM}-time_{%\nrewrite}|}divide start_ARG italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT + italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT end_ARG start_ARG italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT + italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT + | italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT - italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT | end_ARG\nof its original capability. Based on the number of macros supported and the performance of each macro, it can be derived that under the current band., the ratio of the number of macros for the three strategies generalized ping-pong:in\u00a0situ write/compute: naive ping-pong is and the execution time ratio for generalized ping-pong:in\u00a0situ write/compute: naive ping-pong is When t\u2062i\u2062m\u2062eP\u2062I\u2062M>t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{PIM}>time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT > italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT, the generalized ping-pong strategy demonstrates better performance compared to the other two strategies. When t\u2062i\u2062m\u2062eP\u2062I\u2062M<t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{PIM}<time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT < italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT, generalized ping-pong outperforms the in\u00a0situ write/compute strategy and offers equivalent performance to the ping-pong strategy while utilizing fewer macros, which translates to a lower area overhead. When t\u2062i\u2062m\u2062eP\u2062I\u2062M=t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{PIM}=time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT = italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT, generalized ping-pong provides better performance than the in\u00a0situ write/compute strategy, and its performance and number of macros are identical to those of the naive ping-pong strategy. This is because, at this point, the macros in the naive ping-pong strategy do not enter an idle state, and the actual execution methods of the two strategies are completely aligned. \nIV-C Runtime Phase Pipeline Adaption\n In a large system-on-a-chip (SoC) design, the off-chip memory bandwidth for PIM accelerator is often assigned dynamically in runtime. Chances are the accelerator cannot get its full off-chip memory bandwidth. The proposed generalized ping-pong scheduling strategy is also helpful for this case. For a PIM accelerator after fabrication, when encountering a reduction in off-chip bandwidth during the execution of computational tasks, the generalized ping-pong strategy can preserve a greater portion of performance compared to other strategies. We discuss about the performance degradation caused by the reduction of off-chip bandwidth under the in\u00a0situ write/compute, ping-pong, and generalized ping-pong strategies through a modeling approach. For the in\u00a0situ write/compute strategy, when the off-chip bandwidth is reduced to band.n\\frac{band.}{n}divide start_ARG italic_b italic_a italic_n italic_d . end_ARG start_ARG italic_n end_ARG, the optimal response is not to decrease the number of active macros but to reduce the speed of weight updating operations for each macro, thereby lowering the demand for off-chip bandwidth. This means that the number of functioning macros remains constant, but the performance of each macro is diminished. In this case, the performance degradation is: In comparison to the strategy of maintaining the speed of weight updating while reducing the number of active macros, which results in performance degradation to 1n1\ud835\udc5b\\frac{1}{n}divide start_ARG 1 end_ARG start_ARG italic_n end_ARG of the original case, it can preserve a better proportion of performance. For the naive ping-pong strategy, when t\u2062i\u2062m\u2062eP\u2062I\u2062M>t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{PIM}>time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT > italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT, the response strategy is to maintain the number of active macros and reduce the speed of weight updating operations for each macro to decrease the demand on the off-chip bandwidth. At this point, although t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT increases, it still satisfies the condition t\u2062i\u2062m\u2062eP\u2062I\u2062M>t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{PIM}>time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT > italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT, which means that the increase in t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT merely leads to a reduction in the idle time of the macros, with performance remaining constant until t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT increases to the point where t\u2062i\u2062m\u2062eP\u2062I\u2062M=t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{PIM}=time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT = italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT. At t\u2062i\u2062m\u2062eP\u2062I\u2062M=t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{PIM}=time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT = italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT, each macro\u2019s utilization reaches its peak since the macros do not enter an idle state. At this juncture, if the off-chip bandwidth decreases again and t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT increases to the point where t\u2062i\u2062m\u2062eP\u2062I\u2062M<t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{PIM}<time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT < italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT, the strategy is to maintain the weight updating speed at t\u2062i\u2062m\u2062eP\u2062I\u2062M=t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{PIM}=time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT = italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT and reduce the number of active macros. In this scenario, performance degradation is Compared to the strategy of reducing the updating speed of weights without decreasing the number of active macros, which results in performance degradation to 1n1\ud835\udc5b\\frac{1}{n}divide start_ARG 1 end_ARG start_ARG italic_n end_ARG of the original, the strategy that maintains the speed of weight updating while reducing the number of active macros offers the same performance but with fewer macros in use, thereby reducing energy consumption. For the generalized ping-pong strategy, when off-chip bandwidth is reduced, the speed of weight updating remains constant while the number of active macros is decreased. Unlike the previous two strategies, generalized ping-pong adjusts the ratio of t\u2062i\u2062m\u2062eP\u2062I\u2062M\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40time_{PIM}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT to t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT to reduce the number of working macros. As previously mentioned, t\u2062i\u2062m\u2062eP\u2062I\u2062M\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40time_{PIM}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT depends on the speed at which a macro completes vector-matrix operations and the number of vectors that need to be computed within a batch, which is determined by the amount of on-chip memory each macro can access. When the number of working macros is reduced and the on-chip memory capacity remains unchanged, the amount of on-chip memory available to each macro increases, in increases. This implies that t\u2062i\u2062m\u2062eP\u2062I\u2062M\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40time_{PIM}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT increases. According to Eq.\u00a04, when t\u2062i\u2062m\u2062eP\u2062I\u2062M\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40time_{PIM}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT increases and t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT remains constant, it supports a greater number of active macros. When the off-chip bandwidth is reduced to band./nband./nitalic_b italic_a italic_n italic_d . / italic_n, the number of active macros becomes n\u2062u\u2062mm\u2062a\u2062c\u2062r\u2062o/m\ud835\udc5b\ud835\udc62subscript\ud835\udc5a\ud835\udc5a\ud835\udc4e\ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc5anum_{macro}/mitalic_n italic_u italic_m start_POSTSUBSCRIPT italic_m italic_a italic_c italic_r italic_o end_POSTSUBSCRIPT / italic_m accordingly.\nThe ratio of t\u2062i\u2062m\u2062eP\u2062I\u2062M\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40time_{PIM}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT to t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT becomes: s\u2062i\u2062z\u2062em\u2062a\u2062c\u2062r\u2062os\u2062i\u2062z\u2062eO\u2062U\u22c5ni\u2062n\u22c5m:s\u2062i\u2062z\u2062em\u2062a\u2062c\u2062r\u2062os:\u22c5\ud835\udc60\ud835\udc56\ud835\udc67subscript\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc67subscript\ud835\udc52\ud835\udc42\ud835\udc48subscript\ud835\udc5b\ud835\udc56\ud835\udc5b\ud835\udc5a\ud835\udc60\ud835\udc56\ud835\udc67subscript\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\\frac{size_{macro}}{size_{OU}}\\cdot n_{in}\\cdot m:\\frac{size_{macro}}{s}divide start_ARG italic_s italic_i italic_z italic_e start_POSTSUBSCRIPT italic_m italic_a italic_c italic_r italic_o end_POSTSUBSCRIPT end_ARG start_ARG italic_s italic_i italic_z italic_e start_POSTSUBSCRIPT italic_O italic_U end_POSTSUBSCRIPT end_ARG \u22c5 italic_n start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT \u22c5 italic_m : divide start_ARG italic_s italic_i italic_z italic_e start_POSTSUBSCRIPT italic_m italic_a italic_c italic_r italic_o end_POSTSUBSCRIPT end_ARG start_ARG italic_s end_ARG.\nAt this point, the average demand for off-chip bandwidth per macro is t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\u22c5st\u2062i\u2062m\u2062eP\u2062I\u2062M+t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\u22c5\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52\\frac{time_{rewrite}\\cdot s}{time_{PIM}+time_{rewrite}}divide start_ARG italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT \u22c5 italic_s end_ARG start_ARG italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT + italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT end_ARG\n, and multiply it with \u22c5n\u2062u\u2062mm\u2062a\u2062c\u2062r\u2062o/m\u22c5absent\ud835\udc5b\ud835\udc62subscript\ud835\udc5a\ud835\udc5a\ud835\udc4e\ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc5a\\cdot num_{macro}/m\u22c5 italic_n italic_u italic_m start_POSTSUBSCRIPT italic_m italic_a italic_c italic_r italic_o end_POSTSUBSCRIPT / italic_m, which should be equal to band./nband./nitalic_b italic_a italic_n italic_d . / italic_n.\nThen we can solve\nthe performance degradation:\n In Eq\u00a0.9, all parameters except for n\ud835\udc5bnitalic_n and m\ud835\udc5amitalic_m are numerical values obtained during the hardware design phase using the generalized ping-pong strategy.\nThe slopes of Eq\u00a0.9, Eq\u00a0.7, Eq\u00a0.8 demonstrate that the generalized ping-pong strategy can retain a greater portion of performance compared to the other two strategies. \nV Evaluation\n \nV-A Experimental Setup\n The proposed generalized ping-pong strategy focuses on the throughput improvement for multi-macro PIM GeMM accelerators. To evaluate it, we implement different accelerator-level concurrent write/computing pipeline strategies on a revised PUMA\u00a0[29] design.\nTo simplify the analysis and control the variables, we focus on large-scale consecutive GeMM operations with basic linear algebra subprograms (BLAS) level benchmarks\u00a0[31]. Because the target pipeline strategy emphasizes the alignment on clock cycles, the timing simulation is based on synthesizable Verilog HDL design (check our open-source repository https://github.com/rw999creator/gpp-pim to reproduce the simulation results). The example design parameters are set to: PIM accelerator has 16 cores, where each is equipped with 16 macros. The macro size is 32\u00d7\\times\u00d732 bytes, with a write speed ranging from 1 to 8byte/cycle, and the size of the operating unit is 4\u00d7\\times\u00d78byte. \nV-B Evaluation for Design Phase Optimization\n Fig.\u00a06 presents a comparison of performance and macro count between the generalized ping-pong and other strategies during the hardware design exploration phase. At this stage, the off-chip bandwidth memory b\u2062a\u2062n\u2062d.\ud835\udc4f\ud835\udc4e\ud835\udc5b\ud835\udc51band.italic_b italic_a italic_n italic_d . is set to 128byte/cycle. The x-axis is the ratio of weight write time111The \u201cweight write time\u201d refers to entirely rewriting the data stored in PIM. over the PIM compute time. The y-axis is the execution latency in cycle numbers. When t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e<t\u2062i\u2062m\u2062eP\u2062I\u2062M\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40time_{rewrite}<time_{PIM}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT < italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT, under the same off-chip bandwidth conditions, generalized ping-pong can support a greater computational power compared to the other two strategies, and it requires the use of more macros. In the scenario where the ratio of t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT to t\u2062i\u2062m\u2062eP\u2062I\u2062M\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40time_{PIM}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT is 1:7, generalized ping-pong achieves a 2.51\u00d7\\times\u00d7 performance improvement over naive ping-pong and a 5.03\u00d7\\times\u00d7 improvement over in situ write/compute. When t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e=t\u2062i\u2062m\u2062eP\u2062I\u2062M\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40time_{rewrite}=time_{PIM}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT = italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT, the generalized ping-pong and naive ping-pong strategies completely overlap, and they exhibit a 2\u00d7\\times\u00d7 performance improvement over in situ write/compute in terms of performance. When t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e>t\u2062i\u2062m\u2062eP\u2062I\u2062M\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40time_{rewrite}>time_{PIM}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT > italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT, generalized ping-pong outperforms in situ write/compute and matches the performance of naive ping-pong, but with the advantage of using fewer macros, which conserves area and power consumption. In the case where the ratio of t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT to t\u2062i\u2062m\u2062eP\u2062I\u2062M\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40time_{PIM}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT is 8:1, generalized ping-pong reduces the number of macros by 43.75% compared to naive ping-pong and achieves 1.78\u00d7\\times\u00d7 performance improvement over in situ write/compute strategy. The improvement brought by the generalized ping-pong on performance and area depends on the ratio of t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT to t\u2062i\u2062m\u2062eP\u2062I\u2062M\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40time_{PIM}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT. \nV-C Evaluation for Runtime Phase Adaptation\n Fig.\u00a07 shows the results for runtime phase optimization. It shows the comparative performance of the three strategies (in\u00a0situ write/compute, naive ping-pong, generalized ping-pong) in response to bandwidth fluctuations. The x-axis is how many times of off-chip memory bandwidth reduction compared to that given during design phase. The y-axes are (a) normalized execution, (b) average on-chip memory utilization rate, (c) off-chip memory bandwidth utilization rate, and (d) average macro utilization rate. For Fig.\u00a07(a) and (b) This comparison is performed on the design phase optimization goal of t\u2062i\u2062m\u2062er\u2062e\u2062w\u2062r\u2062i\u2062t\u2062e\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52time_{rewrite}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_r italic_e italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT = t\u2062i\u2062m\u2062eP\u2062I\u2062M\ud835\udc61\ud835\udc56\ud835\udc5asubscript\ud835\udc52\ud835\udc43\ud835\udc3c\ud835\udc40time_{PIM}italic_t italic_i italic_m italic_e start_POSTSUBSCRIPT italic_P italic_I italic_M end_POSTSUBSCRIPT and exerts a progressive reduction in bandwidth to monitor the trend in performance variation. The experimental results indicate that generalized ping-pong can retain a greater degree of performance as off-chip bandwidth decreases, in comparison to current execution schemes. When the bandwidth is reduced to band.64\\frac{band.}{64}divide start_ARG italic_b italic_a italic_n italic_d . end_ARG start_ARG 64 end_ARG, our strategy achieves 5.38\u00d7\\times\u00d7 improvement in performance over in situ write/compute and 7.71\u00d7\\times\u00d7 improvement in performance over naive ping-pong. Fig.\u00a07(c) and (d) show the comparison of off-chip bandwidth utilization and macro utilization rate, respectively. The in situ write/compute strategy yields a lower off-chip bandwidth utilization, whereas the naive ping-pong strategy has a lower macro utilization. The advantage of generalized ping-pong is with both high off-chip bandwidth utilization and macro utilization. Table\u00a0II shows the design space optimization with generalized ping-pong at different off-chip bandwidth (unit: byte/cycle). The discrepancy between the execution strategies calculated by the model (with a fractional number of PIM macros) and those actually implemented in Verilog HDL (with integer number of PIM macros) diminishes as the number of macros increases. For the in situ write/compute strategy, the optimal scheduling is to reduce the speed at which each macro rewrites weights, thereby decreasing the demand for off-chip bandwidth, while keeping the number of active macros constant. However, the speed of weight updating cannot be infinitely reduced as a latency overhead. When the speed of weight updating reaches the minimum value determined by hardware design, it becomes necessary to reduce the number of active macros to cope with further decreases in bandwidth. This leads to a more rapid decline in performance. For generalized ping-pong, due to the finite number of macros, the actual execution results are an approximation of the model. \nVI Conclusion\n This paper attempts to answer the question of how to realize concurrent weight trasnfer and PIM computation towards upscaled GeMM operations.\nTo achieve this, we propose a novel generalized ping-pong pipelining strategy for arbitary scale of PIM accelerators. With an exemplary PIM accelerator implemented, we demonstrate the efficacy of the generalized ping-pong strategy. It is applicable for design space exploration and improve runtime off-chip memory bandwidth utilization. Compared to existing strategies, our approach achieves superior performance boost under the same off-chip memory bandwidth. This work reveils the fundamental theory of pipeling optimization for PIM architectures. References"}
{"text": "Topkima-Former: Low-energy, Low-Latency Inference for Transformers using top-k In-memory ADC Transformer model has gained prominence as a popular deep neural network architecture for neural language processing (NLP) and computer vision (CV) applications. However, the extensive use of nonlinear operations, like softmax, poses a performance bottleneck during transformer inference and comprises up to 40% of the total latency. Hence, we propose innovations at the circuit, architecture, and algorithm levels to accelerate the transformer. At the circuit level, we propose topkima\u2014combining top-k\ud835\udc58kitalic_k activation selection with in-memory ADC (IMA) to implement a low-energy and low-latency softmax without any sorting latency. Only the k\ud835\udc58kitalic_k largest activations are sent to the softmax calculation block, reducing the huge computational cost of softmax. Using a modified training scheme with top-k\ud835\udc58kitalic_k only in the forward pass, experimental results demonstrate only a 0.4% to 1.2% reduction in accuracy across ViT, distilBERT, and BERT-base models when evaluated on CIFAR-10, CIFAR-100, and SQuAD datasets with k\ud835\udc58kitalic_k=5. At the architecture level, an improved scale-free technique is introduced to reduce the computational cost of attention. The combined system, dubbed Topkima-Former, enhances 1.8\u00d7\u221284\u00d71.8\\times-84\\times1.8 \u00d7 - 84 \u00d7 speedup and 1.3\u00d7\u221235\u00d71.3\\times-35\\times1.3 \u00d7 - 35 \u00d7 energy efficiency (EE) over prior In-memory computing (IMC) accelerators. Compared to a conventional softmax macro and a digital top-k\ud835\udc58kitalic_k (Dtopk) softmax macro, our proposed tokima softmax macro achieves about 15\u00d715\\times15 \u00d7 and 8\u00d78\\times8 \u00d7 faster speed respectively. \nI Introduction\n Transformer can achieve superior performance in various applications involving neural language processing (NLP) and computer vision (CV) due to the attention mechanism. However, as the models evolve, the required computational and memory resources increase rapidly. To efficiently implement transformer on hardware, many software (SW) and hardware (HW) techniques have been proposed to accelerate the linear dot-product operation. On the SW side, there are two categories: (1) Quantization, where the weights and activations of matrix multiplications are quantized to low precision data [1]. (2) Sparsity, where the insensitive data of weights and activations are removed to reduce the required computational number, like tokens pruning[2] and dynamic attention score sparsity [3]. On the HW front, In-memory computing (IMC) is a popular method that reduces data movement to and from memory. Both volatile (SRAM, DRAM) and nonvolatile (RRAM, Flash) solutions have been explored. Resistive memory approaches have gained more traction recently due to their possibility of low-energy combined with low-area implementation above the active silicon area. These methods can implement vector-matrix multiplies very efficiently [4], but they suffer from ADC overheads [5]. To address this challenge, In-memory ADC (IMA) is proposed to embed ADC into each row of the IMC macro [6]. This approach allows for the reuse of bit cells for both multiply-accumulate (MAC) and ADC operations, resulting in enhanced energy efficiency (EE). However, the latency is slightly high due to the utilization of ramp ADC, which requires 2nsuperscript2\ud835\udc5b2^{n}2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT cycles for an n\ud835\udc5bnitalic_n-bit ADC. On the contrary, few works focus on optimizing the nonlinear (NL) operations in transformer, like softmax, which can result in a large overhead in latency and energy. For example, the softmax operation consumes up to 40% inference time, severely impeding the efficient implementation of transformer [7]. To solve this problem, low-precision implementation changes the base of exponential from e\ud835\udc52eitalic_e to 2, achieving 1.25\u00d71.25\\times1.25 \u00d7 speed improvement [8]. And using Taylor series to approximate exponentials reduces inference time by only 19% [9]. The lack of significant improvement can be attributed to the necessity of handling all inputs of the softmax function. This becomes particularly challenging when dealing with lengthy inputs. Motivated by competitive networks in neuroscience [10], this work integrates the concept of top-k\ud835\udc58kitalic_k winner take all with IMA (topkima) such that only k\ud835\udc58kitalic_k (\u226admuch-less-thanabsent\ud835\udc51\\ll d\u226a italic_d) values need to be passed to the NL operator. This works well since the output of the softmax exponentially increases larger values over others. Our proposed method eliminates sorting overheads and even reduces the latency overhead of ramp ADC. This design fully harnesses the potential of top-k\ud835\udc58kitalic_k, significantly reducing the overhead of the softmax operation in transformers. To further accelerate transformer, the integrated design, dubbed Topkima-Former, spans circuit, architecture, and algorithm innovations, which has the following main features: Circuit: Integrating sorting operation for top-k\ud835\udc58kitalic_k selection in the IMA array without sorting overhead. This is achieved by replacing the increasing ramp in prior works with a decreasing ramp, allowing it to cross larger MAC voltages earlier. The integrated topkima can save energy and latency in both the data conversion stage (by early stopping) and the subsequent softmax operation by processing a reduced number of values. Algorithm: Modified training scheme where k\ud835\udc58kitalic_k=5 results in only a 0.4% to 1.2% reduction in accuracy across ViT, distilBERT, and BERT-base models when evaluated on CIFAR-10, CIFAR-100, and SQuAD datasets. Architecture: Removing scaling operations by just adjusting the weights of WQsubscript\ud835\udc4a\ud835\udc44W_{Q}italic_W start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT without any overhead. Topkima-Former achieves 1.8\u00d7\u221284\u00d71.8\\times-84\\times1.8 \u00d7 - 84 \u00d7 faster and 1.3\u00d7\u221235\u00d71.3\\times-35\\times1.3 \u00d7 - 35 \u00d7 more EE than prior IMC-based accelerators. Compared to a conventional softmax macro and a digital top-k\ud835\udc58kitalic_k (Dtopk) softmax macro, our proposed topkima softmax macro (topkima-SM) achieves about 15\u00d715\\times15 \u00d7 and 8\u00d78\\times8 \u00d7 faster speed. \nII Preliminaries and related works\n \nII-A Transformer models\n Transformer models are built up with multiple encoders and/or decoders. Multi-head attention is the main component of each coder. As shown in Fig.\u00a01, there are three weight matrixes in one attention module: WQsubscript\ud835\udc4a\ud835\udc44W_{Q}italic_W start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT, WKsubscript\ud835\udc4a\ud835\udc3eW_{K}italic_W start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT and WVsubscript\ud835\udc4a\ud835\udc49W_{V}italic_W start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT. Then Queries (Q\ud835\udc44Qitalic_Q), Keys (K\ud835\udc3eKitalic_K), and Values (V\ud835\udc49Vitalic_V) could be obtained by multiplying the input X\ud835\udc4bXitalic_X: where WQ,WK\u2208\u211ddm\u2062o\u2062d\u2062e\u2062l\u00d7dksubscript\ud835\udc4a\ud835\udc44subscript\ud835\udc4a\ud835\udc3esuperscript\u211dsubscript\ud835\udc51\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59subscript\ud835\udc51\ud835\udc58W_{Q},W_{K}\\in\\mathbb{R}^{d_{model}\\times d_{k}}italic_W start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT , italic_W start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT \u00d7 italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, WV\u2208\u211ddm\u2062o\u2062d\u2062e\u2062l\u00d7dvsubscript\ud835\udc4a\ud835\udc49superscript\u211dsubscript\ud835\udc51\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59subscript\ud835\udc51\ud835\udc63W_{V}\\in\\mathbb{R}^{d_{model}\\times d_{v}}italic_W start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT \u00d7 italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, X\u2208\u211dS\u2062L\u00d7dm\u2062o\u2062d\u2062e\u2062l\ud835\udc4bsuperscript\u211d\ud835\udc46\ud835\udc3fsubscript\ud835\udc51\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59X\\in\\mathbb{R}^{{SL}\\times d_{model}}italic_X \u2208 blackboard_R start_POSTSUPERSCRIPT italic_S italic_L \u00d7 italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, Q,K\u2208\u211dS\u2062L\u00d7dk\ud835\udc44\ud835\udc3esuperscript\u211d\ud835\udc46\ud835\udc3fsubscript\ud835\udc51\ud835\udc58Q,K\\in\\mathbb{R}^{SL\\times d_{k}}italic_Q , italic_K \u2208 blackboard_R start_POSTSUPERSCRIPT italic_S italic_L \u00d7 italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT. dm\u2062o\u2062d\u2062e\u2062lsubscript\ud835\udc51\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59d_{model}italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT, dksubscript\ud835\udc51\ud835\udc58d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, dvsubscript\ud835\udc51\ud835\udc63d_{v}italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT and S\u2062L\ud835\udc46\ud835\udc3fSLitalic_S italic_L are the dimensions of the model, K\ud835\udc3eKitalic_K, V\ud835\udc49Vitalic_V and sequence length of X\ud835\udc4bXitalic_X. The result of this attention module is calculated by: where the dimension of the result \u2208\u211ddk\u00d7dvabsentsuperscript\u211dsubscript\ud835\udc51\ud835\udc58subscript\ud835\udc51\ud835\udc63\\in\\mathbb{R}^{d_{k}\\times d_{v}}\u2208 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u00d7 italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUPERSCRIPT. The softmax function is: s\u2062o\u2062f\u2062t\u2062m\u2062a\u2062x\u2062(xi)=exi\u2211j=0dkexj\ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65subscript\ud835\udc65\ud835\udc56superscript\ud835\udc52subscript\ud835\udc65\ud835\udc56superscriptsubscript\ud835\udc570subscript\ud835\udc51\ud835\udc58superscript\ud835\udc52subscript\ud835\udc65\ud835\udc57softmax(x_{i})=\\frac{e^{x_{i}}}{\\sum_{j=0}^{d_{k}}e^{x_{j}}}italic_s italic_o italic_f italic_t italic_m italic_a italic_x ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = divide start_ARG italic_e start_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_j = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG. It requires dksubscript\ud835\udc51\ud835\udc58d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT exponentials and dksubscript\ud835\udc51\ud835\udc58d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT divisions. These NL operations render softmax difficult for HW implementation [11]. \nII-B Related works\n Softmax HW implementation schemes. Softmax involves expensive operations of exponential and division. The popular solution is to use function approximation to reduce the overhead of these two operations. Softermax switches the base of exponential from e\ud835\udc52eitalic_e to 2 to simplify HW implementations [7]. I-BERT introduces a more general approximation, replacing the exponential with 2nd-order polynomials [12]. Similarly, SpAtten employs a 5th-order Taylor approximation [2]. In addition to the approximation of the exponential, Du et al. utilize the log sum-exp trick to avoid the division operation [13]. However, the aforementioned schemes require processing all inputs for softmax, leading to considerable latency and energy consumption when the input size (S\u2062L\ud835\udc46\ud835\udc3fSLitalic_S italic_L) is very large. For example, latency increases drastically by 137\u00d7137\\times137 \u00d7 when S\u2062L\ud835\udc46\ud835\udc3fSLitalic_S italic_L increases from 256256256256 to 4096409640964096 [13]. Top-k\ud835\udc58kitalic_k method has also been proposed to process only the k\ud835\udc58kitalic_k largest inputs of softmax [3]. It can achieve 2.6\u00d72.6\\times2.6 \u00d7 speedup when k\ud835\udc58kitalic_k=30 with a small loss in accuracy. However, it suffers from the sorting complexity of O\u2062(S\u2062L)\ud835\udc42\ud835\udc46\ud835\udc3fO(SL)italic_O ( italic_S italic_L ). In our estimation, the sorting operation consumes at least 75%percent7575\\%75 % latency (See Section IV-B). IMC-based accelerators for transformer. Retransformer leverages the promising RRAM device to expedite the inference of transformer [1]. However, the low endurance of RRAM renders it unsuitable for deployment within crossbar arrays that require frequent programming, such as the crossbar KTsuperscript\ud835\udc3e\ud835\udc47K^{T}italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT with input Q\ud835\udc44Qitalic_Q. Conversely, TranCIM adopts the mature fabrication technology of SRAM to design a full-SRAM IMC accelerator for transformer [14]. But this design forfeits benefits associated with RRAM, including high storage density, fast read speed and low energy consumption. Subsequently, X-former proposes a hybrid IMC architecture built up with RRAM and SRAM together to efficiently execute different workloads of transformer [4]. However, it lacks a comprehensive co-design from circuit level, to architecture level and up to algorithm level. \nIII Design features\n We optimized Topkima-Former by using SW-HW co-design as described in this section. \nIII-A HW implementation\n Overall architecture design. The overall system simulation, conducted using the NeuroSim framework [5], comprises chip, tile, processing element (PE) and array hierarchies. The weights (WQ,K,Vsubscript\ud835\udc4a\ud835\udc44\ud835\udc3e\ud835\udc49W_{Q,K,V}italic_W start_POSTSUBSCRIPT italic_Q , italic_K , italic_V end_POSTSUBSCRIPT) of projection operations in Fig.\u00a01 are mapped onto RRAM IMC because RRAM provides high storage density, fast read speed, and low energy consumption when performing vector-matrix multiplication with fixed weights. However, SRAM is chosen for attention operations (Q\u22c5KT\u22c5\ud835\udc44superscript\ud835\udc3e\ud835\udc47Q\\cdot K^{T}italic_Q \u22c5 italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT and A\u22c5V\u22c5\ud835\udc34\ud835\udc49A\\cdot Vitalic_A \u22c5 italic_V) since it needs to be written for every input sample. Therefore, KTsuperscript\ud835\udc3e\ud835\udc47K^{T}italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT and V\ud835\udc49Vitalic_V are written in the SRAM IMC for every input. Note that the design of the RRAM IMC and SRAM IMC for A\u22c5V\u22c5\ud835\udc34\ud835\udc49A\\cdot Vitalic_A \u22c5 italic_V are consistent with NeuroSim design. Our focus in this work is the second SRAM IMC\u2013 topkima-SM that combines topkima macro (topkima-M) that uses SRAM array for Q\u22c5KT\u22c5\ud835\udc44superscript\ud835\udc3e\ud835\udc47Q\\cdot K^{T}italic_Q \u22c5 italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT together with a digital softmax, as explained next. Topkima-M design. As seen in the block diagram of topkima-M (Fig.\u00a02(a)), KTsuperscript\ud835\udc3e\ud835\udc47K^{T}italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT is stored in the dual 10T SRAM array while Q\u22c5KT\u22c5\ud835\udc44superscript\ud835\udc3e\ud835\udc47Q\\cdot K^{T}italic_Q \u22c5 italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT is performed by SRAM IMC where Q\ud835\udc44Qitalic_Q values are sent as inputs by pulse-width modulating the WL, a commonly used technique [15]. To facilitate read-disturb free computing and signed inputs, four extra transistors (illustrated as the blue component in Fig.\u00a02(d)) are added to the basic 6T cell. For ternary weight (-1(QLsubscript\ud835\udc44\ud835\udc3fQ_{L}italic_Q start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT=L, QRsubscript\ud835\udc44\ud835\udc45Q_{R}italic_Q start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT=H)/ 0((QLsubscript\ud835\udc44\ud835\udc3fQ_{L}italic_Q start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT=L, QRsubscript\ud835\udc44\ud835\udc45Q_{R}italic_Q start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT=L))/ +1((QLsubscript\ud835\udc44\ud835\udc3fQ_{L}italic_Q start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT=H, QRsubscript\ud835\udc44\ud835\udc45Q_{R}italic_Q start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT=L))) storage, dual(Left-Right) cells are used (Fig.\u00a02(d)). The pre-charged bitlines (BL) are discharged in accordance with the MAC operation of Q\u22c5KT\u22c5\ud835\udc44superscript\ud835\udc3e\ud835\udc47Q\\cdot K^{T}italic_Q \u22c5 italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT as follows: when RWL+/RWL- is at high voltage, four blue transistors in either the L or R side will conduct based on high/low logic levels of signals QLsubscript\ud835\udc44\ud835\udc3fQ_{L}italic_Q start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT, QL\u00af\u00afsubscript\ud835\udc44\ud835\udc3f\\bar{Q_{L}}over\u00af start_ARG italic_Q start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT end_ARG, QRsubscript\ud835\udc44\ud835\udc45Q_{R}italic_Q start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT and QR\u00af\u00afsubscript\ud835\udc44\ud835\udc45\\bar{Q_{R}}over\u00af start_ARG italic_Q start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT end_ARG. This will cause a discharge current to flow through the read BL (R\u2062B\u2062LL\ud835\udc45\ud835\udc35subscript\ud835\udc3f\ud835\udc3fRBL_{L}italic_R italic_B italic_L start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT or R\u2062B\u2062LR\ud835\udc45\ud835\udc35subscript\ud835\udc3f\ud835\udc45RBL_{R}italic_R italic_B italic_L start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT), resulting in a small voltage drop on these read BL, as the basic multiplication in the table of Fig.\u00a02(d). The accumulation happens by addition of the voltage drops for all activated bitcells. In contrast, when the RWL+/RWL- is at low voltage, there are no voltage changes. Fig.\u00a02(c) shows a case where KTsuperscript\ud835\udc3e\ud835\udc47K^{T}italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT values are quantized into 4 bits where 1 bit is the sign value. To achieve this by using ternary weight cell as shown in Fig.\u00a02(d), three cells are used to represent one weight, with the three corresponding input PWM signals scaled by the factors of 1, 2 and 4 to achieve binary scaling. This allows for 15 levels of weight (-7 to 7), which is approximately equivalent to 4 bits precision. These MAC voltages (V1subscript\ud835\udc491V_{1}italic_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT to Vnsubscript\ud835\udc49\ud835\udc5bV_{n}italic_V start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT in Fig.\u00a02(a)) are then quantized using a ramp IMA (which enables replica bitcells one by one to create the ramp voltage) [6] and a sense amplifier (SA) as the comparator. After the MAC operation is completed, 32323232 pulses are simultaneously sent in one clock cycle (Fig.\u00a02(c)) to dual 10T SRAM, leading to a discharge of R\u2062B\u2062LR\ud835\udc45\ud835\udc35subscript\ud835\udc3f\ud835\udc45RBL_{R}italic_R italic_B italic_L start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT for setting initial ramp voltage and calibration as necessary (see [6]). Subsequently, another 32323232 ramp pulses (5 bits ADC) are sequentially sent over 32323232 cycles to the dual 10T cells for ADC, causing a progressive discharge on R\u2062B\u2062LL\ud835\udc45\ud835\udc35subscript\ud835\udc3f\ud835\udc3fRBL_{L}italic_R italic_B italic_L start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT with each clock cycle, thereby creating the ramp. The main novelty of the proposed design is that it can integrate the sorting operation for top-k\ud835\udc58kitalic_k selection in this IMA array. This is done by first changing the increasing ramp in prior work [6] to a decreasing one. The decreasing ramp crosses larger voltages earlier (t1<tksubscript\ud835\udc611subscript\ud835\udc61\ud835\udc58t_{1}<t_{k}italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT if V1>Vksubscript\ud835\udc491subscript\ud835\udc49\ud835\udc58V_{1}>V_{k}italic_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > italic_V start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT in Fig.\u00a02(b)). This can be verified by the example of 3 columns array with top-1 selected in Fig.\u00a02(e), where the largest MAC result from column 3 leads to early stopping. To determine which columns\u2019 SA got triggered in a clock cycle, an arbiter-encoder combination is used similar to address event representation (AER) [16] where the latched SA outputs are treated as requests (REQ) and the acknowledgment (ACK) signals are used to disable the SA. Along with the encoded \u2018address\u2019 of the column, the conversion cycle at which the ramp crossing occurred is stored as the ADC output in registers. Further, a counter tracks the number of requests and stops the data conversion early (before 2nsuperscript2\ud835\udc5b2^{n}2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT clock periods) when the count equals or exceeds \u2018k\ud835\udc58kitalic_k\u2019. In the rare case of count exceeding k\ud835\udc58kitalic_k due to ties (many similar large values), the number of outputs is reduced to k\ud835\udc58kitalic_k by giving preference to smaller column addresses. After obtaining the top-k\ud835\udc58kitalic_k quantized values, they are sent to a digital softmax core [17] to get the scores A\ud835\udc34Aitalic_A. Note that we include the operations of Q\u22c5KT\u22c5\ud835\udc44superscript\ud835\udc3e\ud835\udc47Q\\cdot K^{T}italic_Q \u22c5 italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT and the following softmax in the complexity comparisons of topkima-SM done later. Topkima-SM benefits. The benefit of our proposed method can be understood by considering the latency of a conventional softmax macro, Tc\u2062o\u2062n\u2062v\u2212S\u2062Msubscript\ud835\udc47\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc63\ud835\udc46\ud835\udc40T_{conv-SM}italic_T start_POSTSUBSCRIPT italic_c italic_o italic_n italic_v - italic_S italic_M end_POSTSUBSCRIPT, given by: Tc\u2062o\u2062n\u2062v\u2212S\u2062M=Tw\u2062r+d\u22c5(Tp\u2062w\u2062m,i\u2062n\u2062p+Ti\u2062m\u2062a+d\u22c5TN\u2062L,d\u2062i\u2062g)subscript\ud835\udc47\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc63\ud835\udc46\ud835\udc40subscript\ud835\udc47\ud835\udc64\ud835\udc5f\u22c5\ud835\udc51subscript\ud835\udc47\ud835\udc5d\ud835\udc64\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc5dsubscript\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc4e\u22c5\ud835\udc51subscript\ud835\udc47\ud835\udc41\ud835\udc3f\ud835\udc51\ud835\udc56\ud835\udc54T_{conv-SM}=T_{wr}+d\\cdot(T_{pwm,inp}+T_{ima}+d\\cdot T_{NL,dig})italic_T start_POSTSUBSCRIPT italic_c italic_o italic_n italic_v - italic_S italic_M end_POSTSUBSCRIPT = italic_T start_POSTSUBSCRIPT italic_w italic_r end_POSTSUBSCRIPT + italic_d \u22c5 ( italic_T start_POSTSUBSCRIPT italic_p italic_w italic_m , italic_i italic_n italic_p end_POSTSUBSCRIPT + italic_T start_POSTSUBSCRIPT italic_i italic_m italic_a end_POSTSUBSCRIPT + italic_d \u22c5 italic_T start_POSTSUBSCRIPT italic_N italic_L , italic_d italic_i italic_g end_POSTSUBSCRIPT ) where Tw\u2062rsubscript\ud835\udc47\ud835\udc64\ud835\udc5fT_{wr}italic_T start_POSTSUBSCRIPT italic_w italic_r end_POSTSUBSCRIPT is the time to write the KTsuperscript\ud835\udc3e\ud835\udc47K^{T}italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT values in SRAM, Tp\u2062w\u2062m,i\u2062n\u2062psubscript\ud835\udc47\ud835\udc5d\ud835\udc64\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc5dT_{pwm,inp}italic_T start_POSTSUBSCRIPT italic_p italic_w italic_m , italic_i italic_n italic_p end_POSTSUBSCRIPT is the time to create WL pulses of Q\ud835\udc44Qitalic_Q (generally \u221d2nbproportional-toabsentsuperscript2subscript\ud835\udc5b\ud835\udc4f\\propto 2^{n_{b}}\u221d 2 start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT end_POSTSUPERSCRIPT where nbsubscript\ud835\udc5b\ud835\udc4fn_{b}italic_n start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT is the bit-width of input), Ti\u2062m\u2062asubscript\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc4eT_{ima}italic_T start_POSTSUBSCRIPT italic_i italic_m italic_a end_POSTSUBSCRIPT is the time for data conversion by IMA and TN\u2062L,d\u2062i\u2062gsubscript\ud835\udc47\ud835\udc41\ud835\udc3f\ud835\udc51\ud835\udc56\ud835\udc54T_{NL,dig}italic_T start_POSTSUBSCRIPT italic_N italic_L , italic_d italic_i italic_g end_POSTSUBSCRIPT is the time taken for digital implementation of exponentiation and division. Here, Tw\u2062rsubscript\ud835\udc47\ud835\udc64\ud835\udc5fT_{wr}italic_T start_POSTSUBSCRIPT italic_w italic_r end_POSTSUBSCRIPT occurs once while d\ud835\udc51ditalic_d colmuns of Q\ud835\udc44Qitalic_Q are applied one by one. For a top-k\ud835\udc58kitalic_k approach, the latencies for digital sorting TD\u2062t\u2062o\u2062p\u2062k\u2212S\u2062Msubscript\ud835\udc47\ud835\udc37\ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc58\ud835\udc46\ud835\udc40T_{Dtopk-SM}italic_T start_POSTSUBSCRIPT italic_D italic_t italic_o italic_p italic_k - italic_S italic_M end_POSTSUBSCRIPT and our proposed approach Tt\u2062o\u2062p\u2062k\u2062i\u2062m\u2062a\u2212S\u2062Msubscript\ud835\udc47\ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc58\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc46\ud835\udc40T_{topkima-SM}italic_T start_POSTSUBSCRIPT italic_t italic_o italic_p italic_k italic_i italic_m italic_a - italic_S italic_M end_POSTSUBSCRIPT are given by: TD\u2062t\u2062o\u2062p\u2062k\u2212S\u2062M=Tw\u2062r+d\u22c5(Tp\u2062w\u2062m,i\u2062n\u2062p+Ti\u2062m\u2062a+Ts\u2062o\u2062r\u2062t+k\u22c5TN\u2062L,d\u2062i\u2062g)subscript\ud835\udc47\ud835\udc37\ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc58\ud835\udc46\ud835\udc40subscript\ud835\udc47\ud835\udc64\ud835\udc5f\u22c5\ud835\udc51subscript\ud835\udc47\ud835\udc5d\ud835\udc64\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc5dsubscript\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc4esubscript\ud835\udc47\ud835\udc60\ud835\udc5c\ud835\udc5f\ud835\udc61\u22c5\ud835\udc58subscript\ud835\udc47\ud835\udc41\ud835\udc3f\ud835\udc51\ud835\udc56\ud835\udc54T_{Dtopk-SM}=T_{wr}+d\\cdot(T_{pwm,inp}+T_{ima}+T_{sort}+k\\cdot T_{NL,dig})italic_T start_POSTSUBSCRIPT italic_D italic_t italic_o italic_p italic_k - italic_S italic_M end_POSTSUBSCRIPT = italic_T start_POSTSUBSCRIPT italic_w italic_r end_POSTSUBSCRIPT + italic_d \u22c5 ( italic_T start_POSTSUBSCRIPT italic_p italic_w italic_m , italic_i italic_n italic_p end_POSTSUBSCRIPT + italic_T start_POSTSUBSCRIPT italic_i italic_m italic_a end_POSTSUBSCRIPT + italic_T start_POSTSUBSCRIPT italic_s italic_o italic_r italic_t end_POSTSUBSCRIPT + italic_k \u22c5 italic_T start_POSTSUBSCRIPT italic_N italic_L , italic_d italic_i italic_g end_POSTSUBSCRIPT ) Tt\u2062o\u2062p\u2062k\u2062i\u2062m\u2062a\u2212S\u2062M=Tw\u2062r+d\u22c5(Tp\u2062w\u2062m,i\u2062n\u2062p+Ti\u2062m\u2062a,a\u2062r\u2062b+k\u22c5TN\u2062L,d\u2062i\u2062g)subscript\ud835\udc47\ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc58\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc46\ud835\udc40subscript\ud835\udc47\ud835\udc64\ud835\udc5f\u22c5\ud835\udc51subscript\ud835\udc47\ud835\udc5d\ud835\udc64\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc5dsubscript\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc4e\ud835\udc5f\ud835\udc4f\u22c5\ud835\udc58subscript\ud835\udc47\ud835\udc41\ud835\udc3f\ud835\udc51\ud835\udc56\ud835\udc54T_{topkima-SM}=T_{wr}+d\\cdot(T_{pwm,inp}+T_{ima,arb}+k\\cdot T_{NL,dig})italic_T start_POSTSUBSCRIPT italic_t italic_o italic_p italic_k italic_i italic_m italic_a - italic_S italic_M end_POSTSUBSCRIPT = italic_T start_POSTSUBSCRIPT italic_w italic_r end_POSTSUBSCRIPT + italic_d \u22c5 ( italic_T start_POSTSUBSCRIPT italic_p italic_w italic_m , italic_i italic_n italic_p end_POSTSUBSCRIPT + italic_T start_POSTSUBSCRIPT italic_i italic_m italic_a , italic_a italic_r italic_b end_POSTSUBSCRIPT + italic_k \u22c5 italic_T start_POSTSUBSCRIPT italic_N italic_L , italic_d italic_i italic_g end_POSTSUBSCRIPT ) where Ti\u2062m\u2062a,a\u2062r\u2062b=m\u2062a\u2062x\u2062(\u03b1\u2062Ti\u2062m\u2062a+Ta\u2062r\u2062b,Tc\u2062l\u2062k+k\u22c5Ta\u2062r\u2062b)subscript\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc4e\ud835\udc5f\ud835\udc4f\ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udefcsubscript\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc4esubscript\ud835\udc47\ud835\udc4e\ud835\udc5f\ud835\udc4fsubscript\ud835\udc47\ud835\udc50\ud835\udc59\ud835\udc58\u22c5\ud835\udc58subscript\ud835\udc47\ud835\udc4e\ud835\udc5f\ud835\udc4fT_{ima,arb}=max(\\alpha T_{ima}+T_{arb},T_{clk}+k\\cdot T_{arb})italic_T start_POSTSUBSCRIPT italic_i italic_m italic_a , italic_a italic_r italic_b end_POSTSUBSCRIPT = italic_m italic_a italic_x ( italic_\u03b1 italic_T start_POSTSUBSCRIPT italic_i italic_m italic_a end_POSTSUBSCRIPT + italic_T start_POSTSUBSCRIPT italic_a italic_r italic_b end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT italic_c italic_l italic_k end_POSTSUBSCRIPT + italic_k \u22c5 italic_T start_POSTSUBSCRIPT italic_a italic_r italic_b end_POSTSUBSCRIPT ) denotes latency of the IMA and arbiter-encoder in our proposed method. For both approaches, the NL compute time reduces by d/k\ud835\udc51\ud835\udc58d/kitalic_d / italic_k but an added time for sorting (Ts\u2062o\u2062r\u2062t=m\u2062i\u2062n\u2062(d\u22c5l\u2062o\u2062g\u2062(d),d\u22c5k)\u00d7Tc\u2062l\u2062ksubscript\ud835\udc47\ud835\udc60\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc5a\ud835\udc56\ud835\udc5b\u22c5\ud835\udc51\ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc51\u22c5\ud835\udc51\ud835\udc58subscript\ud835\udc47\ud835\udc50\ud835\udc59\ud835\udc58T_{sort}=min(d\\cdot log(d),d\\cdot k)\\times T_{clk}italic_T start_POSTSUBSCRIPT italic_s italic_o italic_r italic_t end_POSTSUBSCRIPT = italic_m italic_i italic_n ( italic_d \u22c5 italic_l italic_o italic_g ( italic_d ) , italic_d \u22c5 italic_k ) \u00d7 italic_T start_POSTSUBSCRIPT italic_c italic_l italic_k end_POSTSUBSCRIPT) is required in the digital sorting case.\nAdditionally, the IMA latency in topkima reduces by a factor \u03b1\ud835\udefc\\alphaitalic_\u03b1 due to early stopping of the ramp. Note that the term Ti\u2062m\u2062a,a\u2062r\u2062bsubscript\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc4e\ud835\udc5f\ud835\udc4fT_{ima,arb}italic_T start_POSTSUBSCRIPT italic_i italic_m italic_a , italic_a italic_r italic_b end_POSTSUBSCRIPT in (4) includes one cycle of arbiter-encoder latency for the k\ud835\udc58kitalic_k-th or last ramp crossing in addition to \u03b1\u2062Ti\u2062m\u2062a\ud835\udefcsubscript\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc4e\\alpha T_{ima}italic_\u03b1 italic_T start_POSTSUBSCRIPT italic_i italic_m italic_a end_POSTSUBSCRIPT. Considerations of crossbar size. Practical crossbar dimensions can be smaller than the dimensions of KTsuperscript\ud835\udc3e\ud835\udc47K^{T}italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT. In that case, the matrix KTsuperscript\ud835\udc3e\ud835\udc47K^{T}italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT has to be split and mapped into multiple crossbars. Two issues have to be handled in this case\u2013(1) Reduced bit precision of KTsuperscript\ud835\udc3e\ud835\udc47K^{T}italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT since we have less number of SRAM rows in one column and (2) Inaccurate evaluation of top-k\ud835\udc58kitalic_k since the top-k\ud835\udc58kitalic_k selection for each array is now separate and there is no global information available. In this case, each array i\ud835\udc56iitalic_i chooses top-kisubscript\ud835\udc58\ud835\udc56k_{i}italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT such that \u2211ki=ksubscript\ud835\udc58\ud835\udc56\ud835\udc58\\sum k_{i}=k\u2211 italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_k. We refer to this technique as sub top-k\ud835\udc58kitalic_k and show the resulting drop in accuracy in Section IV. \nIII-B SW algorithmic optimization\n To mitigate the accuracy penalties of retaining only k\ud835\udc58kitalic_k out of d\ud835\udc51ditalic_d values in the top-k\ud835\udc58kitalic_k method [3], we especially propose a modified training method: top-k\ud835\udc58kitalic_k forward-complete backward propagation (TFCBP), inspired by the quantization aware training [1]. Only top-k\ud835\udc58kitalic_k activations are used to calculate softmax probabilities in the forward propagation stage, while all (i.e., d\ud835\udc51ditalic_d) activations participate in the gradient computation in the backward propagation stage. Moreover, since the latency and energy consumption of ramp IMA increases exponentially with the resolution, quantization-aware training (QAT) is introduced to reduce the activation precision [1]. To reduce accuracy drops, quantized activations are passed for forward propagation while the backward propagation exploits FP-32 based activations to update the weight. Note that these two methods are exclusively employed for training transformer in SW, following which the trained quantized network is mapped onto HW for inference. \nIII-C Architecture level optimizations\n Scaling operation is needed to realize the scaling factor dksubscript\ud835\udc51\ud835\udc58\\sqrt{d_{k}}square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG in (2). Each dot product result of Q\u22c5KT\u22c5\ud835\udc44superscript\ud835\udc3e\ud835\udc47Q\\cdot K^{T}italic_Q \u22c5 italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT requires a division operation, which introduces high design complexity in HW, such as combining bit shifting with constant multiplication[1].\nHence, we propose an improved scale-free design, inspired by [21]. We rewrite the operation as: Q\u22c5KTdk=X\u22c5WQ\u22c5KTdk=Qs\u22c5KT\u22c5\ud835\udc44superscript\ud835\udc3e\ud835\udc47subscript\ud835\udc51\ud835\udc58\u22c5\ud835\udc4bsubscript\ud835\udc4a\ud835\udc44superscript\ud835\udc3e\ud835\udc47subscript\ud835\udc51\ud835\udc58\u22c5superscript\ud835\udc44\ud835\udc60superscript\ud835\udc3e\ud835\udc47\\frac{Q\\cdot K^{T}}{\\sqrt{d_{k}}}=\\frac{X\\cdot W_{Q}\\cdot K^{T}}{\\sqrt{d_{k}}}%\n=Q^{s}\\cdot K^{T}divide start_ARG italic_Q \u22c5 italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG = divide start_ARG italic_X \u22c5 italic_W start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT \u22c5 italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG = italic_Q start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT \u22c5 italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT\nwhere Qs=X\u22c5WQdksuperscript\ud835\udc44\ud835\udc60\u22c5\ud835\udc4bsubscript\ud835\udc4a\ud835\udc44subscript\ud835\udc51\ud835\udc58Q^{s}=X\\cdot\\frac{W_{Q}}{\\sqrt{d_{k}}}italic_Q start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT = italic_X \u22c5 divide start_ARG italic_W start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG. So we can simply adjust the weights from WQsubscript\ud835\udc4a\ud835\udc44W_{Q}italic_W start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT to WQdksubscript\ud835\udc4a\ud835\udc44subscript\ud835\udc51\ud835\udc58\\frac{W_{Q}}{\\sqrt{d_{k}}}divide start_ARG italic_W start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARGto achieve the scale function without any HW overhead for it[18]. \nIV Results\n We first present SW design space exploration to show acceptable ranges of k\ud835\udc58kitalic_k over several datasets. Next, HW performances from macro level, architecture level to system level are shown to present the benefits of using topkima in transformer. Lastly, the system performance is compared with state-of-the-art. \nIV-A SW Design Space Exploration\n At the SW level, we evaluate top-k\ud835\udc58kitalic_k design with two types of transformers. For ViT, we run two representative datasets: CIFAR-10 and CIFAR-100, while for DistilBERT and BERT-base, SQuAD v1.1 (SQuAD) is run for evaluation. Fig.\u00a03 shows the result for varying k\ud835\udc58kitalic_k from 1 to 20. As expected, smaller k\ud835\udc58kitalic_k indicates a more aggressive approximation, leading to a higher accuracy drop. For ViT on CIFAR-100 and DistilBERT / BERT-base on SQuAD, top-1 results in nonnegligible performance degradation. However, top-5 can achieve a good trade-off between radical approximation and accuracy, with less than a 1.2% accuracy drop. For ViT on CIFAR-10, top-1 can achieve comparable accuracy with the baseline (w/o top-k\ud835\udc58kitalic_k approximation), with only 0.4% accuracy loss. This significant advantage of our design over [3] in terms of radical approximation comes from the TFCBP training method. \nIV-B HW evaluation\n The HW evaluation is done for BERT-base model on the SQUAD dataset, with 0.2% accuracy loss after quantization. In this case, Q\u22c5KT\u22c5\ud835\udc44superscript\ud835\udc3e\ud835\udc47Q\\cdot K^{T}italic_Q \u22c5 italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT is evaluated using SPICE simulations in 65656565 nm CMOS technology, where Q\ud835\udc44Qitalic_Q and KTsuperscript\ud835\udc3e\ud835\udc47K^{T}italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT are quantized to 5555 bits and 4444 bits (15 levels of weight as detailed in Sec.III-A) respectively, using QAT. Additionally, X\u22c5WQ,K,V\u22c5\ud835\udc4bsubscript\ud835\udc4a\ud835\udc44\ud835\udc3e\ud835\udc49X\\cdot W_{Q,K,V}italic_X \u22c5 italic_W start_POSTSUBSCRIPT italic_Q , italic_K , italic_V end_POSTSUBSCRIPT and A\u22c5V\u22c5\ud835\udc34\ud835\udc49A\\cdot Vitalic_A \u22c5 italic_V are evaluated using NeuroSim[5]. In this evaluation, WQ,K,Vsubscript\ud835\udc4a\ud835\udc44\ud835\udc3e\ud835\udc49W_{Q,K,V}italic_W start_POSTSUBSCRIPT italic_Q , italic_K , italic_V end_POSTSUBSCRIPT are quantized into 8888 bits by post-training quantization, and both inputs X\ud835\udc4bXitalic_X and A\ud835\udc34Aitalic_A, along with weight V\ud835\udc49Vitalic_V, are quantized into 5 bits using QAT. Macro level analysis: Topkima-M SPICE simulation.\nFirst, we evaluate Topkima-M (computing Q\u22c5KT\u22c5\ud835\udc44superscript\ud835\udc3e\ud835\udc47Q\\cdot K^{T}italic_Q \u22c5 italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT), followed by Topkima-SM which includes the subsequent softmax. The topkima-M with nb=5subscript\ud835\udc5b\ud835\udc4f5n_{b}=5italic_n start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT = 5 resolution for ADC selects the top-5 values from 384384384384 MAC results (Q\ud835\udc44Qitalic_Q size of one head: 384\u00d76438464384\\times 64384 \u00d7 64, KTsuperscript\ud835\udc3e\ud835\udc47K^{T}italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT size of one head: 64\u00d73846438464\\times 38464 \u00d7 384). Due to the limitation of crossbar size, the weights KTsuperscript\ud835\udc3e\ud835\udc47K^{T}italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT and top-5 are split into 2222 parts: a (64\u00d73)\u00d725664\\times 3)\\times 25664 \u00d7 3 ) \u00d7 256 sub-crossbar with sub-top-3 (k1=3subscript\ud835\udc5813k_{1}=3italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 3) and a (64\u00d73)\u00d712864\\times 3)\\times 12864 \u00d7 3 ) \u00d7 128 sub-crossbar with sub-top-2 (k2=2subscript\ud835\udc5822k_{2}=2italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 2), where 3 pairs of bitcells represent 4 bits weight precision of KTsuperscript\ud835\udc3e\ud835\udc47K^{T}italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT. Combined with 64646464 replica bit cells per column (split evenly for calibration and ramp generation), the simulated sub-array size is set to 256\u00d7256256256256\\times 256256 \u00d7 256.\nSimulation of the arbiter, encoder, and counter across corners and power supply results in worst-case delays at SS corner and Vd\u2062d=0.8subscript\ud835\udc49\ud835\udc51\ud835\udc510.8V_{dd}=0.8italic_V start_POSTSUBSCRIPT italic_d italic_d end_POSTSUBSCRIPT = 0.8 V of 1.511.511.511.51, 0.570.570.570.57, and 0.510.510.510.51 ns respectively resulting in Ta\u2062r\u2062b<2.08subscript\ud835\udc47\ud835\udc4e\ud835\udc5f\ud835\udc4f2.08T_{arb}<2.08italic_T start_POSTSUBSCRIPT italic_a italic_r italic_b end_POSTSUBSCRIPT < 2.08 ns. Tc\u2062l\u2062k,i\u2062m\u2062asubscript\ud835\udc47\ud835\udc50\ud835\udc59\ud835\udc58\ud835\udc56\ud835\udc5a\ud835\udc4eT_{clk,ima}italic_T start_POSTSUBSCRIPT italic_c italic_l italic_k , italic_i italic_m italic_a end_POSTSUBSCRIPT is set at 4444 ns resulting in Ti\u2062m\u2062a=128subscript\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc4e128T_{ima}=128italic_T start_POSTSUBSCRIPT italic_i italic_m italic_a end_POSTSUBSCRIPT = 128 ns while \u03b1\u22480.31\ud835\udefc0.31\\alpha\\approx 0.31italic_\u03b1 \u2248 0.31 in (4) averaged across the dataset. Vd\u2062d,S\u2062R\u2062A\u2062Msubscript\ud835\udc49\ud835\udc51\ud835\udc51\ud835\udc46\ud835\udc45\ud835\udc34\ud835\udc40V_{dd,SRAM}italic_V start_POSTSUBSCRIPT italic_d italic_d , italic_S italic_R italic_A italic_M end_POSTSUBSCRIPT is set at 0.50.50.50.5 V to reduce the unit cell discharge current; this requires slow (5555 ns) writing for robust operation across corners. Estimated from [1] and [13], Tw\u2062r\u2062i\u2062t\u2062e=320subscript\ud835\udc47\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52320T_{write}=320italic_T start_POSTSUBSCRIPT italic_w italic_r italic_i italic_t italic_e end_POSTSUBSCRIPT = 320 ns (row-by-row parallel writing) and TN\u2062L,d\u2062i\u2062g=6.5subscript\ud835\udc47\ud835\udc41\ud835\udc3f\ud835\udc51\ud835\udc56\ud835\udc546.5T_{NL,dig}=6.5italic_T start_POSTSUBSCRIPT italic_N italic_L , italic_d italic_i italic_g end_POSTSUBSCRIPT = 6.5 ns.\nUsing a 2222 GHz clock for input 5 bits PWM results in maximum value of Tp\u2062w\u2062m,i\u2062n\u2062p=15.5subscript\ud835\udc47\ud835\udc5d\ud835\udc64\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc5d15.5T_{pwm,inp}=15.5italic_T start_POSTSUBSCRIPT italic_p italic_w italic_m , italic_i italic_n italic_p end_POSTSUBSCRIPT = 15.5 ns for the LSB and Tp\u2062w\u2062m,i\u2062n\u2062p=62subscript\ud835\udc47\ud835\udc5d\ud835\udc64\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc5d62T_{pwm,inp}=62italic_T start_POSTSUBSCRIPT italic_p italic_w italic_m , italic_i italic_n italic_p end_POSTSUBSCRIPT = 62 ns for the MSB. In Fig.\u00a04(a), the latency of topkima-SM, Tt\u2062o\u2062p\u2062k\u2062i\u2062m\u2062a\u2212S\u2062Msubscript\ud835\udc47\ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc58\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc46\ud835\udc40T_{topkima-SM}italic_T start_POSTSUBSCRIPT italic_t italic_o italic_p italic_k italic_i italic_m italic_a - italic_S italic_M end_POSTSUBSCRIPT, is found to be \u224815\u00d7\\approx 15\\times\u2248 15 \u00d7 and 8\u00d78\\times8 \u00d7 lesser than Tc\u2062o\u2062n\u2062v\u2212S\u2062Msubscript\ud835\udc47\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc63\ud835\udc46\ud835\udc40T_{conv-SM}italic_T start_POSTSUBSCRIPT italic_c italic_o italic_n italic_v - italic_S italic_M end_POSTSUBSCRIPT and TD\u2062t\u2062o\u2062p\u2062k\u2212S\u2062Msubscript\ud835\udc47\ud835\udc37\ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc58\ud835\udc46\ud835\udc40T_{Dtopk-SM}italic_T start_POSTSUBSCRIPT italic_D italic_t italic_o italic_p italic_k - italic_S italic_M end_POSTSUBSCRIPT respectively. The latency benefit of our design over conventional/Dtopk softmax mainly comes from a reduced number of inputs for the softmax operation and the elimination of the need to sort the top-k\ud835\udc58kitalic_k values.\nNote that Dtopk does not improve much over conventional softmax due to the dominant sorting time overhead. Similarly, the energy consumption of our design Et\u2062o\u2062p\u2062k\u2062i\u2062m\u2062a\u2212S\u2062Msubscript\ud835\udc38\ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc58\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc46\ud835\udc40E_{topkima-SM}italic_E start_POSTSUBSCRIPT italic_t italic_o italic_p italic_k italic_i italic_m italic_a - italic_S italic_M end_POSTSUBSCRIPT is 30\u00d730\\times30 \u00d7 and 3\u00d73\\times3 \u00d7 lesser than Ec\u2062o\u2062n\u2062v\u2212S\u2062Msubscript\ud835\udc38\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc63\ud835\udc46\ud835\udc40E_{conv-SM}italic_E start_POSTSUBSCRIPT italic_c italic_o italic_n italic_v - italic_S italic_M end_POSTSUBSCRIPT and ED\u2062t\u2062o\u2062p\u2062k\u2212S\u2062Msubscript\ud835\udc38\ud835\udc37\ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc58\ud835\udc46\ud835\udc40E_{Dtopk-SM}italic_E start_POSTSUBSCRIPT italic_D italic_t italic_o italic_p italic_k - italic_S italic_M end_POSTSUBSCRIPT respectively. The primary reason the EE improvement of our design over Dtopk is not as significant as the latency reduction is that the sorting energy is not a major contributor. Note that these speed/EE improvements increase with increasing S\u2062L\ud835\udc46\ud835\udc3fSLitalic_S italic_L which bodes well for the scalability of this method (e.g. GPT 3.5 has S\u2062L=4096\ud835\udc46\ud835\udc3f4096SL=4096italic_S italic_L = 4096). Fig.\u00a04(b) shows the distribution of the IMA circuit output compared to SW calculations averaged across 256256256256 conversions. The corresponding error was used to inject errors in SW simulations (Q\u22c5KT\u22c5\ud835\udc44superscript\ud835\udc3e\ud835\udc47Q\\cdot K^{T}italic_Q \u22c5 italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT and A\u22c5V\u22c5\ud835\udc34\ud835\udc49A\\cdot Vitalic_A \u22c5 italic_V which are mapped to SRAM) resulting in a small drop in accuracy from 86.7%percent86.786.7\\%86.7 % to 85.1%percent85.185.1\\%85.1 %. We also assess the impact of crossbar size limitation on accuracy, by simulating with 128\u00d7128128128128\\times 128128 \u00d7 128 and 256\u00d7256256256256\\times 256256 \u00d7 256 crossbars. Specifically in Fig.\u00a04(c), when using 128\u00d7128128128128\\times 128128 \u00d7 128 crossbar to implement one head of KTsuperscript\ud835\udc3e\ud835\udc47K^{T}italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT (size: 64\u00d73846438464\\times 38464 \u00d7 384), 3 crossbars are required. Each crossbar allocates 64 rows for ADC and then leaves only 64 rows for MAC operations, resulting in only ternary precision of KTsuperscript\ud835\udc3e\ud835\udc47K^{T}italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT. Also, global top-5 (k=5\ud835\udc585k=5italic_k = 5) is used for four cases (Fig.\u00a04(e)) which gets divided into three sub-top-k\ud835\udc58kitalic_k: k1=2subscript\ud835\udc5812k_{1}=2italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 2, k2=2subscript\ud835\udc5822k_{2}=2italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 2 and k3=1subscript\ud835\udc5831k_{3}=1italic_k start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT = 1. For example, if the output Q\u22c5KT\u22c5\ud835\udc44superscript\ud835\udc3e\ud835\udc47Q\\cdot K^{T}italic_Q \u22c5 italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT is [1, 2, 3, \u2026, 384], the selected values by three sub-top-k\ud835\udc58kitalic_k are [127, 128], [255, 256] and [384] and then combined to be sent to digital softmax core for probability calculation. This is quite different from the selected values by global top-k\ud835\udc58kitalic_k: [380, 381, 382, 383, 384], leading to an accuracy drop.\nThe case for 256\u00d7256256256256\\times 256256 \u00d7 256 crossbars are same as described earlier with two sub-top-k\ud835\udc58kitalic_k arrays (k1=3subscript\ud835\udc5813k_{1}=3italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 3 and k2=2subscript\ud835\udc5822k_{2}=2italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 2) constituting the global top-5. Each crossbar has 64 rows allocated for ADC and calibration. This leaves 192 additional rows for MAC operations, increasing the precision of KTsuperscript\ud835\udc3e\ud835\udc47K^{T}italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT to 4 bits (See details in Section III-A). It is observed that smaller crossbar size leads to larger accuracy degradation in Fig.\u00a04(c). One reason is that a smaller crossbar size leads to reduced weight precision. Additionally, it fragments the global top-k\ud835\udc58kitalic_k into more sub-top-k\ud835\udc58kitalic_k, thereby weakening the winner-takes-all principle. But for the datasets and networks considered here, commonly used crossbar size of 256\u00d7256256256256\\times 256256 \u00d7 256 is sufficient to achieve an accuracy comparable to that of global top-k\ud835\udc58kitalic_k. Architecture level analysis.\nTo account for architecture/system-level overheads, a full attention module is simulated in NeuroSim following the architecture described in Section III-A. RRAM and SRAM technologies are adopted from [19] and [5] respectively. The SRAM array is programmed row-by-row, where the latency is one clock cycle per row and the dynamic power per cell is 1.8\u00d710\u221271.8superscript107{1.8\\times 10^{-7}}1.8 \u00d7 10 start_POSTSUPERSCRIPT - 7 end_POSTSUPERSCRIPT mW/MHz [20]. The peripheral configuration setting is the same as [5]. The read pulse is 0.5 V, adopted from [4]. Note that the peripheral overhead of topkima-M, like interconnect and buffer cost, are not provided in SPICE simulation. So these overheads are also estimated from Neurosim. Also, we only estimate one attention module from BERT-base on SQuAD to report the HW performance because transformer is built by stacking attention modules. As seen in Fig.\u00a04(d), we compare our scale-free design with left shift scale [1] and Tron\u2019s free scale [21], achieving 2.4\u00d72.4\\times2.4 \u00d7 and 1.5\u00d71.5\\times1.5 \u00d7 speedup respectively. Left shift scale is not efficient due to the necessity of scaling for all elements from Q\u22c5KT\u22c5\ud835\udc44superscript\ud835\udc3e\ud835\udc47Q\\cdot K^{T}italic_Q \u22c5 italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT while Tron\u2019s free scale lacks parallelism and requires additional transpose operations. System level analysis.\nTo clearly demonstrate the benefits of topkima-SM on the entire system, we breakdown the latency and energy by components, as shown in Figs.\u00a04(e) and (f). The latency/energy of softmax is significantly reduced after introducing topkima-SM. The synaptic array dominates the latency for two main reasons. First, the 4\u00d74\\times4 \u00d7 pulse width required for higher weight precision results in longer MAC times. Second, the MUX design in NeuroSim increases latency by determining which column\u2019s MAC output needs to be sent to the ADC [5]. The buffer dominates the energy primarily because the 12 heads in the attention module require more buffers to store intermediate data. Unlike latency, the parallel operation of 12 heads does not conceal the energy overhead. Similarly, the latency and energy are broken down by operations in Figs.\u00a04 (g) and (h). We find that the speed of X\u22c5WQ,K,V\u22c5\ud835\udc4bsubscript\ud835\udc4a\ud835\udc44\ud835\udc3e\ud835\udc49X\\cdot W_{Q,K,V}italic_X \u22c5 italic_W start_POSTSUBSCRIPT italic_Q , italic_K , italic_V end_POSTSUBSCRIPT is about 2\u00d72\\times2 \u00d7 lower than Q\u22c5KT\u22c5\ud835\udc44superscript\ud835\udc3e\ud835\udc47Q\\cdot K^{T}italic_Q \u22c5 italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT and A\u22c5V\u22c5\ud835\udc34\ud835\udc49A\\cdot Vitalic_A \u22c5 italic_V due to the larger size of WQ,K,Vsubscript\ud835\udc4a\ud835\udc44\ud835\udc3e\ud835\udc49W_{Q,K,V}italic_W start_POSTSUBSCRIPT italic_Q , italic_K , italic_V end_POSTSUBSCRIPT compared to one attention head of KTsuperscript\ud835\udc3e\ud835\udc47K^{T}italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT and V\ud835\udc49Vitalic_V. Although there are 12 heads, they can operate in parallel. However, the energy consumption of Q\u22c5KT\u22c5\ud835\udc44superscript\ud835\udc3e\ud835\udc47Q\\cdot K^{T}italic_Q \u22c5 italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT and A\u22c5V\u22c5\ud835\udc34\ud835\udc49A\\cdot Vitalic_A \u22c5 italic_V is dominant due to the 12 heads. The sparse input A\ud835\udc34Aitalic_A after topkima-softmax makes A\u22c5V\u22c5\ud835\udc34\ud835\udc49A\\cdot Vitalic_A \u22c5 italic_V more energy-efficient. Table 1 presents a comparison with state of the art accelerators, including traditional systems and IMC-based accelerators. Our proposed Topkima-Former can achieve 6.706.706.706.70 TOPS throughput and 16.8416.8416.8416.84 TOPS/W EE at 200 MHz. Compared to ELSA [22], ReTransformer [1], X-Former [4] and HARDSEA [23], Topkima-Former can achieve 1.8\u00d7\u221284\u00d71.8\\times-84\\times1.8 \u00d7 - 84 \u00d7 higher speed, and 1.3\u00d7\u221235\u00d71.3\\times-35\\times1.3 \u00d7 - 35 \u00d7 energy reduction, respectively. Note that no dedicated pipelining is introduced into Topkima-Former. Otherwise, the speed can be even faster. \nV Conclusion\n In this work, we propose Topkima-Former, co-designed from circuit level, architecture level to algorithm level. At circuit level, we propose topkima which can use IMA to select the top k\ud835\udc58kitalic_k activation without any sorting latency and then achieve a low-energy and low-latency softmax.\nAt algorithm level, k\ud835\udc58kitalic_k=5 can achieve satisfied accuracy with only a 0.4%-1.2% accuracy drop across ViT, distilBERT, and BERT-base models when evaluated on CIFAR-10, CIFAR-100, and SQuAD datasets due to the proposed TFCBP training method. At architecture level, we introduce a scale-free technique for removing expensive scale operations. The combined system can achieve 1.8\u00d7\u221284\u00d71.8\\times-84\\times1.8 \u00d7 - 84 \u00d7 faster and 1.3\u00d7\u221235\u00d71.3\\times-35\\times1.3 \u00d7 - 35 \u00d7 EE compared to prior IMC-based accelerators. In addition, the designed tokima-SM, demonstrates a 15\u00d715\\times15 \u00d7 and 8\u00d78\\times8 \u00d7 speedup compared to a conventional softmax macro and a Dtopk softmax macro, respectively. References"}
{"text": "Towards Automated Verification of Logarithmic Arithmetic Correctness proofs for floating point programs are difficult to verify. To simplify the task, a similar, but less complex system, known as logarithmic arithmetic can be used, The Boyer-Moore Theorem Prover, NQTHM, mechanically verified the correctness of a simple implementation of logarithmic arithmetic. It also verified some useful theorems about accumulated relative error bounds for addition, multiplication and division in this logarithmic number system. These theorems were used to verify a program that approximates exsuperscript\ud835\udc52\ud835\udc65e^{x}italic_e start_POSTSUPERSCRIPT italic_x end_POSTSUPERSCRIPT using a truncated Taylor series.\nAxioms that characterize the finite precision of the logarithmic system using a rational base, b\ud835\udc4fbitalic_b, were shown by the prover to be satisfiable for any choice of 1<b<21\ud835\udc4f21<b<21 < italic_b < 2. The prover verified the correctness of a function for converting an arbitrary rational value to a logarithmic representation. It also verified that multiplication and division implementations produce exact results for exact inputs, and that addition implementation produces a result as accurate as possible for exact inputs.\nWhen these operations are used in combination by a program, such as evaluating a polynomial, the relative error increases in a way that can be bounded by simple expressions, referred to here as tolerances. Several mechanically verified theorems about tolerances allow us to construct mechanically verified proofs about logarithmic arithmetic programs. Although similar to interval arithmetic, tolerances are especially suited to logarithmic arithmetic. \n1 Introduction It is ironic that floating point computation, which is seemingly one of the most rigorous and well defined areas of computer systems, should be one of the last areas to see application of automated formal techniques. Although languages [23], operating systems [6], and even hardware [12] have been mechanically verified, little has been done with floating point programs. Recently, Wilding [36] has mechanically checked the proof of a simple floating point search program that calculates the mid-point in an interval, and proceeds recursively on either the left or right side until a root of a continuous function is found. Even though the only floating point computation in this program is\n(a+b)/2\ud835\udc4e\ud835\udc4f2(a+b)/2( italic_a + italic_b ) / 2, the proof is over fifty pages long. Wilding claims this is the only floating program whose correctness proof has ever been mechanically verified, although some proofs of floating point correctness have been manually constructed [11]. Higher order arithmetic algorithms [28] can be appended to conventional floating point programs to verily that a result for a particular input is accurate, but to our knowledge, no such floating point result verifying algorithm has had its proof of correctness mechanically verified. Although we have no reason to doubt the correctness of such algorithms, it would be desirable to have a verified implementation. Similarly, interval machine arithmetic [21] can be used to obtain upper and lower bounds for an entire computation, but this doubles the storage requirements, and often produces impractical bounds when applied naively.\nSince mechanical verification of correct floating point programs appears to be a challenging problem, it is sensible to choose the simplest possible model af floating point. Several studies of floating point behavior [20, 3, 8] have used logarithmic arithmetic to model an idealized floating point system,\nLogarithmic arithmetic maintains constant relative precision over the entire range of representable values [3], Because of this, the error analysis for logarithmic arithmetic is similar to, but somewhat easier than, the analysis for conventional floating point arithmetic, The ranges and relative precisions when the two arithmetics use the same word size are nearly identical, except the relative precision of conventional base two floating point representations wobble by a factor of two,\nLogarithmic arithmetic is more than just a theoretical tool. It has been used in a variety of practical applications, including: battery-opernted hearing aids [24], circuit simulation [19], control algorithms [35], digital filters [29], Fast Fourier Transforms [32], graphics [16], image processing [15], matrix inversion [26], neural networks [1], speech recognition [25] and tomography [31]. Several successful implementations have been reported in the literature [14, 34, 37], Developers of logarithmic arithmetic software for personal computers [27, 10, 2] claim that on machines with no hardware support for floating point, programs with intensive real computations execute much faster with logarithmic arithmetic than with conventional software floating point arithmetic. The cost and power consumption of many math coprocessor chips makes them impractical for applications which demand low power consumption and/or low cost microprocessors that lack on chip floating point hardware (e.g., 68020, 86C010 or 80486SX), and so logarithmic arithmetic is attractive for such applications.\nThere has been moderate interest lately in the so called SLI number system, which offers freedom from overflow in most computations [33]. Should this unusual number system ever gain wider acceptance, there will be a need ta provide mechanically checked proofs of SLI correctness, just as such a need currently exists for floating point and logarithmic arithmetics, In fact, SLI, floating point, and logarithmic number systems are members of a class of number systems that are closely related. SLI is a generalization of logarithmic arithmetic, and SLI representations near one are identical, bit for bit, to their logarithmic counterparts. SLI differs only in how numbers that approach zero and infinity are represented, Therefore, our work here provides some insight not only in how mechanically checked proofs can be constructed for logarithmic arithmetic programs, but also how they might be done for floating point and SLI programs. \n2 Finite Precision and the Base The logbsubscript\ud835\udc4f\\log_{b}roman_log start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT function, for any real positive b\u22601\ud835\udc4f1b\\neq 1italic_b \u2260 1, maps positive reals onto the reals. If we could store logarithms with infinite precision, real arithmetic could be carried out exactly. Let x\ud835\udc65xitalic_x and y\ud835\udc66yitalic_y be positive real values represented by lx=logb\u2061(x)subscript\ud835\udc59\ud835\udc65subscript\ud835\udc4f\ud835\udc65l_{x}=\\log_{b}(x)italic_l start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT = roman_log start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_x ) and ly=logb\u2061(y)subscript\ud835\udc59\ud835\udc66subscript\ud835\udc4f\ud835\udc66l_{y}=\\log_{b}(y)italic_l start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT = roman_log start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_y ) respectively.\nThen x\u2212y\ud835\udc65\ud835\udc66x-yitalic_x - italic_y, x/y\ud835\udc65\ud835\udc66x/yitalic_x / italic_y, and x+y\ud835\udc65\ud835\udc66x+yitalic_x + italic_y are represented by the real (often irrational) representations: lx+lysubscript\ud835\udc59\ud835\udc65subscript\ud835\udc59\ud835\udc66l_{x}+l_{y}italic_l start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT + italic_l start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT, lx\u2212lysubscript\ud835\udc59\ud835\udc65subscript\ud835\udc59\ud835\udc66l_{x}-l_{y}italic_l start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT - italic_l start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT, and sb\u2062(l\u2062y\u2212lx)+lxsubscript\ud835\udc60\ud835\udc4f\ud835\udc59\ud835\udc66subscript\ud835\udc59\ud835\udc65subscript\ud835\udc59\ud835\udc65s_{b}(ly-l_{x})+l_{x}italic_s start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_l italic_y - italic_l start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ) + italic_l start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT, where the transcendental function sb\u2062(z)=logb\u2061(bz+1)subscript\ud835\udc60\ud835\udc4f\ud835\udc67subscript\ud835\udc4fsuperscript\ud835\udc4f\ud835\udc671s_{b}(z)=\\log_{b}(b^{z}+1)italic_s start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_z ) = roman_log start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_b start_POSTSUPERSCRIPT italic_z end_POSTSUPERSCRIPT + 1 ) is known as the addition logarithm. Of course, to be practical in high speed, cost sensitive applications, the representations lxsubscript\ud835\udc59\ud835\udc65l_{x}italic_l start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT and lysubscript\ud835\udc59\ud835\udc66l_{y}italic_l start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT can only be stored with a finite number of bits. Although how lxsubscript\ud835\udc59\ud835\udc65l_{x}italic_l start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT and lysubscript\ud835\udc59\ud835\udc66l_{y}italic_l start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT are truncated is arbitrary, the simplest approach is to use the integers\nX=\u230alx\u230b\ud835\udc4bsubscript\ud835\udc59\ud835\udc65X=\\lfloor l_{x}\\rflooritalic_X = \u230a italic_l start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT \u230b\nas the representation of the real value x\ud835\udc65xitalic_x, and\nY=\u230aly\u230b\ud835\udc4csubscript\ud835\udc59\ud835\udc66Y=\\lfloor l_{y}\\rflooritalic_Y = \u230a italic_l start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT \u230b\nas the representation of y\ud835\udc66yitalic_y.\nIn this case, the choice of b\ud835\udc4fbitalic_b is no longer arbitrary, since the relative precision of the logarithmic system, F\ud835\udc39Fitalic_F, determines the base, Although such a representation is often thought of as a logarithm of arbitrary base (often two) stored as a fixed point number whose scaling determines the precision [14], the integer view of the representation is completely equivalent [20]. We choose this way of viewing the truncation, since it facilitates the formalization and machine verification described below.\nUsing the irrational given above for b\ud835\udc4fbitalic_b is as impractical for the purpose of automated verification as trying to store the irrational lxsubscript\ud835\udc59\ud835\udc65l_{x}italic_l start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT. For a given precision F\ud835\udc39Fitalic_F and a finite dynamic range, there is a rational choice for b\ud835\udc4fbitalic_b that would produce the same results, bit for bit, as the irrational choice. This is because the irrational base may be approximated by a rational number as closely as needed to make all results identical within the finite range. Thus in the formalization of logarithmic arithmetic, the base b\ud835\udc4fbitalic_b is assumed to be a rational larger than 1.\nWhen b=22F\ud835\udc4fsuperscript2\ud835\udc392b=\\sqrt[2^{F}]{2}italic_b = nth-root start_ARG 2 start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT end_ARG start_ARG 2 end_ARG, we note that F=log2\u2061(logb\u20612)\ud835\udc39subscript2subscript\ud835\udc4f2F=\\log_{2}(\\log_{b}2)italic_F = roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( roman_log start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT 2 ). So for any choice of base, b\ud835\udc4fbitalic_b, the precision is defined to be the integer, F\ud835\udc39Fitalic_F, given by The condition, F\u22650\ud835\udc390F\\geq 0italic_F \u2265 0, forces b\u22642\ud835\udc4f2b\\leq 2italic_b \u2264 2. Except for showing the axioms presented below are satisfiable, there is no interest in the cases when F=0\ud835\udc390F=0italic_F = 0, so it is safe to exclude the possibility that b=2\ud835\udc4f2b=2italic_b = 2. \n3 Axioms The base is characterized by b=P/Q\ud835\udc4f\ud835\udc43\ud835\udc44b=P/Qitalic_b = italic_P / italic_Q, where P\ud835\udc43Pitalic_P and Q\ud835\udc44Qitalic_Q are integers that satisfy 0<Q<P<2\u22c5Q0\ud835\udc44\ud835\udc43\u22c52\ud835\udc440<Q<P<2\\cdot Q0 < italic_Q < italic_P < 2 \u22c5 italic_Q. Since P\ud835\udc43Pitalic_P and Q\ud835\udc44Qitalic_Q are integers, it is not possible for Q=1\ud835\udc441Q=1italic_Q = 1, and so slightly stronger conditions can be imposed on P\ud835\udc43Pitalic_P and Q\ud835\udc44Qitalic_Q as the first axiom in our formalization: Given a base, b=P/Q\ud835\udc4f\ud835\udc43\ud835\udc44b=P/Qitalic_b = italic_P / italic_Q, that satisfies (1), an integer valued function, S\u2062(z)\ud835\udc46\ud835\udc67S(z)italic_S ( italic_z ), known as the quantized addition logarithm, is required to approximate the transcendental function sb\u2062(z)=logb\u2061(bz+1)subscript\ud835\udc60\ud835\udc4f\ud835\udc67subscript\ud835\udc4fsuperscript\ud835\udc4f\ud835\udc671s_{b}(z)=\\log_{b}(b^{z}+1)italic_s start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_z ) = roman_log start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_b start_POSTSUPERSCRIPT italic_z end_POSTSUPERSCRIPT + 1 ). The asymptotic properties of sbsubscript\ud835\udc60\ud835\udc4fs_{b}italic_s start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT, make it feasible to implement S\u2062(z)\ud835\udc46\ud835\udc67S(z)italic_S ( italic_z ) with finite tables.\nFor large positive inputs, sb\u2062(z)subscript\ud835\udc60\ud835\udc4f\ud835\udc67s_{b}(z)italic_s start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_z ) approaches z.\nFor negative inputs with large absolute value, sb\u2062(z)subscript\ud835\udc60\ud835\udc4f\ud835\udc67s_{b}(z)italic_s start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_z ) approaches 0. Since the curve y=sb\u2062(z)\ud835\udc66subscript\ud835\udc60\ud835\udc4f\ud835\udc67y=s_{b}(z)italic_y = italic_s start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_z ) approaches its asymptotes from above, when S\u2062(z)\ud835\udc46\ud835\udc67S(z)italic_S ( italic_z ) is defined by then there is a nonnegative integer, SEZ, and a nonpositive integer, Sez, known as the essential zero, such that for all integers Z\ud835\udc4dZitalic_Z larger than SEZ, S\u2062(Z)=Z\ud835\udc46\ud835\udc4d\ud835\udc4dS(Z)=Zitalic_S ( italic_Z ) = italic_Z and for all integers Z\ud835\udc4dZitalic_Z less than Sez, S\u2062(Z)=0\ud835\udc46\ud835\udc4d0S(Z)=0italic_S ( italic_Z ) = 0. Since\nS\u2062(\u2212Z)<sb\u2062(\u2212Z)<S\u2062(\u2212Z)+1\ud835\udc46\ud835\udc4dsubscript\ud835\udc60\ud835\udc4f\ud835\udc4d\ud835\udc46\ud835\udc4d1S(-Z)<s_{b}(-Z)<S(-Z)+1italic_S ( - italic_Z ) < italic_s start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( - italic_Z ) < italic_S ( - italic_Z ) + 1,\nwe see that\nS\u2062(\u2212Z)+Z<sb\u2062(\u2212Z)+Z<S\u2062(\u2212Z)+Z+1\ud835\udc46\ud835\udc4d\ud835\udc4dsubscript\ud835\udc60\ud835\udc4f\ud835\udc4d\ud835\udc4d\ud835\udc46\ud835\udc4d\ud835\udc4d1S(-Z)+Z<s_{b}(-Z)+Z<S(-Z)+Z+1italic_S ( - italic_Z ) + italic_Z < italic_s start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( - italic_Z ) + italic_Z < italic_S ( - italic_Z ) + italic_Z + 1.\nThen\nS\u2062(\u2212Z)+Z<sb\u2062(Z)<S\u2062(\u2212Z)+Z+1\ud835\udc46\ud835\udc4d\ud835\udc4dsubscript\ud835\udc60\ud835\udc4f\ud835\udc4d\ud835\udc46\ud835\udc4d\ud835\udc4d1S(-Z)+Z<s_{b}(Z)<S(-Z)+Z+1italic_S ( - italic_Z ) + italic_Z < italic_s start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_Z ) < italic_S ( - italic_Z ) + italic_Z + 1,\nsince sb\u2062(z)=sb\u2062(\u2212z)+zsubscript\ud835\udc60\ud835\udc4f\ud835\udc67subscript\ud835\udc60\ud835\udc4f\ud835\udc67\ud835\udc67s_{b}(z)=s_{b}(-z)+zitalic_s start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_z ) = italic_s start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( - italic_z ) + italic_z is a well known property of the real valued addition logarithm [1]. Thus for all integers Z\ud835\udc4dZitalic_Z, a similar property applies to the quantized addition logarithm, This implies, first of all, that the constant Sez may be taken to be \u2013SEZ, and more importantly, that the table implementing S\ud835\udc46Sitalic_S need only be indexed from 0 to SEZ (instead of from Sez to SEZ). The machine verification of theorems relating to these issues follows in a later section. They are presented above to motivate our choice of axioms. We postulate the existence of an integer constant, SEZ, and a table, ST, with integer values and integer indices, that satisfy the following: Axiom (3) ensures that SEZ is defined so that\nS\u2062(SEZ+1)=\u230asb\u2062(SEZ+1)\u230b=\u230alogb\u2061(bSEZ+1)\u230b=SEZ+1\ud835\udc46SEZ1subscript\ud835\udc60\ud835\udc4fSEZ1subscript\ud835\udc4fsuperscript\ud835\udc4fSEZ1SEZ1S(\\mathrm{SEZ}+1)=\\lfloor s_{b}(\\mathrm{SEZ}+1)\\rfloor=\\lfloor\\log_{b}(b^{%\n\\mathrm{SEZ}}+1)\\rfloor=\\mathrm{SEZ}+1italic_S ( roman_SEZ + 1 ) = \u230a italic_s start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( roman_SEZ + 1 ) \u230b = \u230a roman_log start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_b start_POSTSUPERSCRIPT roman_SEZ end_POSTSUPERSCRIPT + 1 ) \u230b = roman_SEZ + 1.\nThis is a fact that can be used in an inductive proof that for all integers\nZ>SEZ,S\u2062(Z)=Zformulae-sequence\ud835\udc4dSEZ\ud835\udc46\ud835\udc4d\ud835\udc4dZ>\\mathrm{SEZ},S(Z)=Zitalic_Z > roman_SEZ , italic_S ( italic_Z ) = italic_Z. The remaining axioms deal with the nature of the finite table, ST, used to implement addition. ST is a table of nonnegative integers, indexed by an integer between zero and the essential zero. For 0\u2264Z\u2264SEZ0\ud835\udc4dSEZ0\\leq Z\\leq\\mathrm{SEZ}0 \u2264 italic_Z \u2264 roman_SEZ, and Axiom (5) states that for 0\u2264Z\u2264SEZ0\ud835\udc4dSEZ0\\leq Z\\leq\\mathrm{SEZ}0 \u2264 italic_Z \u2264 roman_SEZ, ST\u2062(Z)=\u230alogb\u2061(bZ+1)\u230b=\u230asb\u2062(Z)\u230bST\ud835\udc4dsubscript\ud835\udc4fsuperscript\ud835\udc4f\ud835\udc4d1subscript\ud835\udc60\ud835\udc4f\ud835\udc4d\\mathrm{ST}(Z)=\\lfloor\\log_{b}(b^{Z}+1)\\rfloor=\\lfloor s_{b}(Z)\\rfloorroman_ST ( italic_Z ) = \u230a roman_log start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_b start_POSTSUPERSCRIPT italic_Z end_POSTSUPERSCRIPT + 1 ) \u230b = \u230a italic_s start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_Z ) \u230b. \n4 Mechanical Verification The Boyer-Moore theorem prover, NQTHM, is a computer program that can be used to prove theorems stated in the computational logic of Boyer and Moore. The logic is a quantifier-free, first-order logic with equality resembling Pure LISP. Both the formal logic and the theorem prover are fully described in [5]. An interactive interface to the prover, fully described in [13], was used during the work reported here. NQTHM was used to show (a) the axioms are satisfiable for a specific choice of the base b=3/2\ud835\udc4f32b=3/2italic_b = 3 / 2, (b) the axioms are satisfiable for any choice of a base b=P/Q\ud835\udc4f\ud835\udc43\ud835\udc44b=P/Qitalic_b = italic_P / italic_Q with 1<Q<P<2\u22c5Q1\ud835\udc44\ud835\udc43\u22c52\ud835\udc441<Q<P<2\\cdot Q1 < italic_Q < italic_P < 2 \u22c5 italic_Q, (c) multiplication and division implementations are correct for rational values that have exact representations as base b\ud835\udc4fbitalic_b logarithms, (d) addition implementation is correct in the sense that it works as well as can be expected for rational values with exact representations as base b\ud835\udc4fbitalic_b logarithms. (e) when the implementation of these three operations are used repeatedly in a program, the relative error increases in a way that can be described a priori. In order to explore the various issues that arise when logarithmic arithmetic is formalized, we have chosen to break the implementation into several different levels, Level 1, which is described in this paper, only deals with representing arbitrarily large positive rationals. Zero, underflow and overflow can never occur in Level 1, nor is subtraction a permissible operation. The implementations of multiplication and division in Level 1 seem trivial. Although somewhat more involved, addition can be succinctly expressed using the S\u2062(Z)\ud835\udc46\ud835\udc4dS(Z)italic_S ( italic_Z ) function described above. However, just proving that these three implementations are correct in terms of nonnegative integers requires about 6,000 lines of lemmas be derived and keyed into NQTHM to guide its reasoning. Although a library of theorems about rational numbers is available for NQTHM [56], we decided to use nonnegative integers for Level 1 of our formalization. Although this makes our axioms and theorems more intricate than they would have been if rationals had been used, this was our only option when this work began since the rationals library requires more resources than were available to us on the DEC Station R5000 that we have for running NQTHM. Although we miss the notational convenience of a predefined rational type, NQTHM is able to reason about inequalities involving nonnegative integers. Most practical applications have used the sign/logarithm number system [30], where there is a sign bit in the representation, allowing both positive and negative rationals to be represented. It is our goal to include a sign bit in a future level of our formalization. We hope that the inclusion of a sign bit will no more than double the number of lemmas, and that these lemmas will not be significantly different from the ones we have proven for Level 1. All the axioms from the previous section were manually translated into equivalent statements (in the LISP-like syntax of NQTHM) about nonnegative integers. These axioms were submitted to the theorem prover, and it verified that they are consistent by showing that the following choices for the base, b=P/Q\ud835\udc4f\ud835\udc43\ud835\udc44b=P/Qitalic_b = italic_P / italic_Q, the constant SEZ, and the table, ST, satisfy the axioms: Most of the axioms and lemmas involved in formalizing Level 1 logarithmic arithmetic are simple inequality predicates involving positive rational numbers, A positive rational must be translated into a pair of positive integers before submission to the theorem prover. The following definitions are based on the assumption that TOP, BOT, BASE_TOP, BASE_BOT are positive integers. The exponent, E\ud835\udc38Eitalic_E, may be negative as well as zero or positive, and is usually a logarithmic representation. BASE_TOP and BASE_BOT together define a base, usually b\ud835\udc4fbitalic_b. TOP and BOT together define an arbitrary finite positive rational number which is to be compared against the value bEsuperscript\ud835\udc4f\ud835\udc38b^{E}italic_b start_POSTSUPERSCRIPT italic_E end_POSTSUPERSCRIPT, One of the predicates needed for our axioms is which can be translated into equivalent nonnegative integer inequalities with the following definition: \n(DEFN\n  L-LESSP ( TOP BOT BASE-TOP BASE-BOT E )\n  (IF (NEGATIVEP E)\n      (LESSP (TIMES TOP\n                    (EXP BASE-TOP (NEGATIVE-GUTS E)))\n             (TIMES BOT\n                    (EXP BASE-BOT (NEGATIVE-GUTS E))))\n      (LESSP (TIMES TOP\n                    (EXP BASE-BOT E))\n             (TIMES BOT\n                    (EXP BASE-TOP E)))))\n where (NEGATIVE-GUTS E) is a NQTHM function that strips the sign from its negative input, E\ud835\udc38Eitalic_E, leaving |E|\ud835\udc38|E|| italic_E |. The notation L-LESSP indicates this is the less than predicate with the arbitrary rational, TOP/BOT, on the left. L-GEQ is the negation of L-LESSP. Similar definitions are used for R-LESSP and R-GEQ, which have the arbitrary rational on the right.\nFor example, here are axioms (4) and (5) in NQTHM syntax: \n  (NUMBERP (ST Z)) : S-TABLE AXIOMS\n  (IMPLIES (AND (NUMBERP Z)\n                (NOT (LESSP (SEZ) Z)))\n           (L-GEQ (PLUS (EXP (P) Z)\n                        (EXP (Q) Z))\n                  (EXP (Q) Z)\n                  (P) (Q) (ST Z)))\n  (IMPLIES (AND (NUMBERP Z)\n                (NOT (LESSP (SEZ) Z)))\n           (L-LESSP (PLUS (EXP (P) Z)\n                          (EXP (Q) Z))\n                  (EXP (Q) Z)\n                  (P) (Q) ADD1 (ST Z)))\n Note that (SEZ), (P) and (Q) are argumentless functions that return the constants SEZ, P\ud835\udc43Pitalic_P and Q\ud835\udc44Qitalic_Q respectively. \n5 Floor and Ceiling of the Logarithm When b=P/Q\ud835\udc4f\ud835\udc43\ud835\udc44b=P/Qitalic_b = italic_P / italic_Q, 0<D0\ud835\udc370<D0 < italic_D, 0<Q0\ud835\udc440<Q0 < italic_Q, and Q<P\ud835\udc44\ud835\udc43Q<Pitalic_Q < italic_P, the function is used to show that the axioms constrained above can be satisfied for any choice of P\ud835\udc43Pitalic_P and Q\ud835\udc44Qitalic_Q that satisfies axiom (1). This function can also be used to convert rationals, which are given as pairs (N,D)\ud835\udc41\ud835\udc37(N,D)( italic_N , italic_D ) of nonnegative integers, into logarithmic form.\nThe base, represented by the pair (P,Q)\ud835\udc43\ud835\udc44(P,Q)( italic_P , italic_Q ), is assumed to be a rational larger than 1. This function is defined by cases, depending whether the positive rational represented by the pair (N,D)\ud835\udc41\ud835\udc37(N,D)( italic_N , italic_D ) is less than one (which is obtained by computing (CEILING-LOG>=1 D N P Q) and negating this result) or whether the rational is greater than or equal to one (which is computed by FLOOR-LOG>=1).\nThe function CEILING-LOG>=1 is designed to correctly compute \u2308logP/Q\u2061(N/D)\u2309subscript\ud835\udc43\ud835\udc44\ud835\udc41\ud835\udc37\\lceil\\log_{P/Q}(N/D)\\rceil\u2308 roman_log start_POSTSUBSCRIPT italic_P / italic_Q end_POSTSUBSCRIPT ( italic_N / italic_D ) \u2309 when (N,D)\ud835\udc41\ud835\udc37(N,D)( italic_N , italic_D ) represents a rational at least as large as 1 and (P,Q)\ud835\udc43\ud835\udc44(P,Q)( italic_P , italic_Q ) represents a rational base larger than 1. The function searches through values of the form (P/Q)Lsuperscript\ud835\udc43\ud835\udc44\ud835\udc3f(P/Q)^{L}( italic_P / italic_Q ) start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT, for L\ud835\udc3fLitalic_L= 0,1,2,\u2026, until N/D<(P/Q)L\ud835\udc41\ud835\udc37superscript\ud835\udc43\ud835\udc44\ud835\udc3fN/D<(P/Q)^{L}italic_N / italic_D < ( italic_P / italic_Q ) start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT. When the first such value is found, the function returns the value of L. NQTHM proved that the search must end with L<N\u22c5Q\ud835\udc3f\u22c5\ud835\udc41\ud835\udc44L<N\\cdot Qitalic_L < italic_N \u22c5 italic_Q. Here is the definition as presented to the theorem prover: \n(DEFN\n   CEILING-LOG>=1-LOOP (N D P Q L I J C)\n   (IF (ZEROP C)\n       (FIX L)\n       (IF (LESSP (TIMES D I)\n                  (TIMES W J))\n           (CEILING-LOG>=1-LOOP N D P Q\n                               (ADD1 L)\n                               (TIMES I P)\n                               (TIMES J Q)\n                               (SUB1 C))\n           (FIX L))))\n(DEFN\n   CEILING-LOG>=1 (N D P Q)\n   (CEILING-LOG>=1-LOOP N D P Q 0 1 1 (TIMES N Q)))\n CEILING-LOG>=1 initializes L\ud835\udc3fLitalic_L to 0, both I\ud835\udc3cIitalic_I and J\ud835\udc3dJitalic_J to 1, and C\ud835\udc36Citalic_C to N\u22c5Q\u22c5\ud835\udc41\ud835\udc44N\\cdot Qitalic_N \u22c5 italic_Q, and then enters CEILING-LOG>=1-LOOP which accumulates the values (P/Q)Lsuperscript\ud835\udc43\ud835\udc44\ud835\udc3f(P/Q)^{L}( italic_P / italic_Q ) start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT in I/J\ud835\udc3c\ud835\udc3dI/Jitalic_I / italic_J and limits the search to L<N\u22c5Q\ud835\udc3f\u22c5\ud835\udc41\ud835\udc44L<N\\cdot Qitalic_L < italic_N \u22c5 italic_Q. The above function returns (FIX L) in both termination cases in order to ensure that the result is a nonnegative integer.\nA similar function computes FLOOR-LOG>=1. These definitions were presented to the theorem prover, and were shown to be correct. That is, when 0<D0\ud835\udc370<D0 < italic_D, 0<N0\ud835\udc410<N0 < italic_N, 0<Q0\ud835\udc440<Q0 < italic_Q, and Q<P\ud835\udc44\ud835\udc43Q<Pitalic_Q < italic_P, We want to show that our axioms can be satisfied for any choice of P\ud835\udc43Pitalic_P and Q\ud835\udc44Qitalic_Q such that 0<Q<P0\ud835\udc44\ud835\udc430<Q<P0 < italic_Q < italic_P, so that we can control the choice of precision. For example, P\ud835\udc43Pitalic_P and Q\ud835\udc44Qitalic_Q can be chosen so that the precision is the same as for single precision floating point, We want to ensure that the axioms are not merely satisfiable for a particular base, but that the axioms also hold for useful choices of b\ud835\udc4fbitalic_b. To do this, we use parameterized definitions for the constant SEZ and the table ST using FLOOR_LOG. The algorithms for computing the FLOOR_LOG are not practical to actually carry out the computations\u2014they merely demonstrate that the axioms apply to systems with practical precisions. Given parameters P\ud835\udc43Pitalic_P and Q\ud835\udc44Qitalic_Q such that 0<Q<P0\ud835\udc44\ud835\udc430<Q<P0 < italic_Q < italic_P, parameterized definitions that satisfy the axioms constrained earlier were defined for NQTHM: Here are the axioms in terms of parameterized SEQ_PQ and ST_PQ For 0\u2264Z\u2264SEZ\u2062_\u2062PQ\u2062(P1,Q1)0\ud835\udc4dSEZ_PQsubscript\ud835\udc431subscript\ud835\udc4410\\leq Z\\leq\\mathrm{SEZ\\_PQ}(P_{1},Q_{1})0 \u2264 italic_Z \u2264 roman_SEZ _ roman_PQ ( italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ), and These were translated into nonnegative integer arithmetic, and NQTHM verified that the parameterized versions of SEZ and ST actually satisfy the axioms, as they are stored in the data base of the theorem prover, given only that the parameters P1subscript\ud835\udc431P_{1}italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and Q1subscript\ud835\udc441Q_{1}italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT satisfy axiom 1<Q1<P1<2\u22c5Q11subscript\ud835\udc441subscript\ud835\udc431\u22c52subscript\ud835\udc4411<Q_{1}<P_{1}<2\\cdot Q_{1}1 < italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < 2 \u22c5 italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. For example, the axioms can be satisfied when the base is chosen to\nbe: For this choice of base, the precision is which is the precision (not counting the hidden bit) of single precision floating point systems. The number of entries in the ST table is A table of this size is too large to be practical. Furthermore, the time required to compute the complete ST table using FLOOR-LOG as defined in our formalization is astronomical because the accumulated product, I, grows to be huge (QS\u2062(Z)superscript\ud835\udc44\ud835\udc46\ud835\udc4dQ^{S(Z)}italic_Q start_POSTSUPERSCRIPT italic_S ( italic_Z ) end_POSTSUPERSCRIPT), Since Z>0\ud835\udc4d0Z>0italic_Z > 0, \u230alogb\u2061(2)<S\u2062(Z)\u230bsubscript\ud835\udc4f2\ud835\udc46\ud835\udc4d\\lfloor\\log_{b}(2)<S(Z)\\rfloor\u230a roman_log start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( 2 ) < italic_S ( italic_Z ) \u230b, and so it will take at least as long to compute S\u2062(Z)\ud835\udc46\ud835\udc4dS(Z)italic_S ( italic_Z ) as it takes to compute Q\u230alogb\u2061(2)\u230bsuperscript\ud835\udc44subscript\ud835\udc4f2Q^{\\lfloor\\log_{b}(2)\\rfloor}italic_Q start_POSTSUPERSCRIPT \u230a roman_log start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( 2 ) \u230b end_POSTSUPERSCRIPT, which in turn takes at least as long as computing the final TIMES.\nFor the choice of b\ud835\udc4fbitalic_b given above that makes F\ud835\udc39Fitalic_F = 23, \u230alogb\u2061(2)\u230bsubscript\ud835\udc4f2\\lfloor\\log_{b}(2)\\rfloor\u230a roman_log start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( 2 ) \u230b = 8,664,340, and so 8,664,340\u22c5(12,500,0008,664,340)8664\u22c534012500superscript00086643408,664,340\\cdot(12,500,000^{8,664,340})8 , 664 , 340 \u22c5 ( 12 , 500 , 000 start_POSTSUPERSCRIPT 8 , 664 , 340 end_POSTSUPERSCRIPT ) is the final TIMES. Assuming a straightforward multiplication algorithm, this requires at least\nF=\u230aloge\u2061(logb\u2061(2))\u230b=23\ud835\udc39subscript\ud835\udc52subscript\ud835\udc4f223F=\\lfloor\\log_{e}(\\log_{b}(2))\\rfloor=23italic_F = \u230a roman_log start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ( roman_log start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( 2 ) ) \u230b = 23\nadditions of k\ud835\udc58kitalic_k bit numbers, where\nk=8,664,339\u22c5\u2308log2\u2061(12,500,000)\u2309=199,279,820formulae-sequence\ud835\udc588664\u22c5339subscript212500000199279820k=8,664,339\\cdot\\lceil\\log_{2}(12,500,000)\\rceil=199,279,820italic_k = 8 , 664 , 339 \u22c5 \u2308 roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( 12 , 500 , 000 ) \u2309 = 199 , 279 , 820.\nThis means k\u22c5F=4,583,435,860\u22c5\ud835\udc58\ud835\udc394583435860k\\cdot F=4,583,435,860italic_k \u22c5 italic_F = 4 , 583 , 435 , 860 bit operations occur in this final TIMES, which is a lower bound on the number of bit operations required to compute one S\u2062(Z)\ud835\udc46\ud835\udc4dS(Z)italic_S ( italic_Z ) for any Z>0\ud835\udc4d0Z>0italic_Z > 0 with this base using FLOOR-LOG. The actual number of bit operations would be much greater. There are SEZ elements in the ST table, and so a lower bound on the number of bit operations to do the\nfinal TIMES in filling each element of the ST table is\n57,292,948,250,000,000. A similar TIMES occurs in computing PS\u2062(Z)superscript\ud835\udc43\ud835\udc46\ud835\udc4dP^{S}(Z)italic_P start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT ( italic_Z ), Assuming a machine with 1 ns. per operation, a lower bound for how long it takes to fill the ST table using FLOOR-LOG is at least two years. It would actually take much longer because of all of the other TIMES that were ignored above.\nImprovements in the implementation and efficiency of the functions that compute the floor of the logarithm are possible. For example, the techniques of [17] would allow the complete ST table to be initialized in less than two seconds. FLOOR-LOG can serve as the specification for such faster but equivalent function. \n6 Level 1 Exact Representation In our formalization, we make a distinction between the representation and the (rational) value being represented. The representation Z\ud835\udc4dZitalic_Z (which is an integer) exactly represents the positive rational value N/D\ud835\udc41\ud835\udc37N/Ditalic_N / italic_D if and only if There are two cases to consider depending on whether Z\ud835\udc4dZitalic_Z is negative or nonnegative, because (EXP X E) is only valid for a nonnegative integer, E\ud835\udc38Eitalic_E. Here is what is meant by the rational TOP/BOTTOPBOT\\mathrm{TOP}/\\mathrm{BOT}roman_TOP / roman_BOT being exactly representable as Z\ud835\udc4dZitalic_Z in base BASE\u2062_\u2062TOP/BASE\u2062_\u2062BOTBASE_TOPBASE_BOT\\mathrm{BASE\\_TOP}/\\mathrm{BASE\\_BOT}roman_BASE _ roman_TOP / roman_BASE _ roman_BOT: \n(DEFN\n  EXACT-REP ( E TOP BOT BASE-TOP BASE-BOT )\n  (IF (NEGATIVEP E)\n    (EQUAL (TIMES TOP (EXP BASE-TOP (NEGATIVE-GUTS E)))\n           (TIMES BOT (EXP BASE-BOT (NEGATIVE-GUTS E))))\n    (EQUAL (TIMES TOP (EXP BASE-BOT E))\n           (TIMES BOT (EXP BASE-TOP E)))))\n In Level 1, the base is always P/Q\ud835\udc43\ud835\udc44P/Qitalic_P / italic_Q, and the constants P\ud835\udc43Pitalic_P and Q\ud835\udc44Qitalic_Q are nonnegative integers constrained to satisfy axiom (1), and so \n7 Level 1 Multiplication and Division Here are the implementations in Level 1 of multiplication and division: \n(DEFN\n  MULT-LEVEL-1 (X Y)\n  (iPLUS X Y))\n(DEFN\n  DIV-LEVEL-1 (X Y)\n  (iDIFFERENCE X Y))\n iPLUS and iDIFFERENCE are integer versions of addition and subtraction required by NQTHM since PLUS and DIFFERENCE only work, as expected, with nonnegative integers.\nNQTHM verified that these implementations are correct. That is, if X\ud835\udc4bXitalic_X exactly represents NX/DXsubscript\ud835\udc41\ud835\udc4bsubscript\ud835\udc37\ud835\udc4bN_{X}/D_{X}italic_N start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT / italic_D start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT and Y\ud835\udc4cYitalic_Y exactly represents NY/DYsubscript\ud835\udc41\ud835\udc4csubscript\ud835\udc37\ud835\udc4cN_{Y}/D_{Y}italic_N start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT / italic_D start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT, then MULT\u2062_\u2062LEVEL\u2062_\u20621\u2062(X,Y)MULT_LEVEL_1XY\\mathrm{MULT\\_LEVEL\\_1(X,Y)}roman_MULT _ roman_LEVEL _ 1 ( roman_X , roman_Y )\nexactly represents The theorem prover verified this lemma after it was translated into its syntax: \n(IMPLIES (AND (LESSP 0 NX) (LESSP 0 NY)\n              (LESSP 0 DX) (LESSP 0 DY)\n              (EXACT-REP-LEVEL-1 X NX DX)\n              (EXACT-REP-LEVEL-1 Y NY DY})\n  (EXACT-REP-LEVEL-1 (MULT-LEVEL-1 X Y) (TIMES NX NY) (TIMES DY DX)))\n Also, if X\ud835\udc4bXitalic_X exactly represents NX/DXsubscript\ud835\udc41\ud835\udc4bsubscript\ud835\udc37\ud835\udc4bN_{X}/D_{X}italic_N start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT / italic_D start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT and Y\ud835\udc4cYitalic_Y exactly represents NY/DYsubscript\ud835\udc41\ud835\udc4csubscript\ud835\udc37\ud835\udc4cN_{Y}/D_{Y}italic_N start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT / italic_D start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT then DIV\u2062_\u2062LEVEL\u2062_\u20621\u2062(X,Y)DIV_LEVEL_1\ud835\udc4b\ud835\udc4c\\mathrm{DIV\\_LEVEL\\_1}(X,Y)roman_DIV _ roman_LEVEL _ 1 ( italic_X , italic_Y ) exactly represents This was verified in a way similar to multiplication. \n8 Definition of S Since Level 1 uses an unbounded integer, Z\ud835\udc4dZitalic_Z, to represent arbitrarily large rational values, an infinite table would be required to store the function, where b=P/Q\ud835\udc4f\ud835\udc43\ud835\udc44b=P/Qitalic_b = italic_P / italic_Q. This would obviously make implementation on a finite machine impossible. To overcome this, a finite table of values, ST, described by the axioms earlier, is used to define S as: When |Z|\ud835\udc4d|Z|| italic_Z | is larger than SEZ, the ST table is not used. For positive Z\ud835\udc4dZitalic_Z close to zero, ST is used directly. For negative Z\ud835\udc4dZitalic_Z close to zero, a simple computation using ST avoids having to store a table twice as large.\nThis definition was translated and NQTHM proved it is correct by showing that the following inequalities hold for all integers Z\ud835\udc4dZitalic_Z: In other words, the ratio of the rational bZ+1superscript\ud835\udc4f\ud835\udc4d1b^{Z}+1italic_b start_POSTSUPERSCRIPT italic_Z end_POSTSUPERSCRIPT + 1 and the rational represented by S\u2062(Z)\ud835\udc46\ud835\udc4dS(Z)italic_S ( italic_Z ) is between 1 and b\ud835\udc4fbitalic_b. This is as good a result as can be expected by any logarithmic (or floating point) number system with relative precision F. Indeed, many formal models [36, 9] of floating point use a constant equivalent to b\ud835\udc4fbitalic_b to describe such inherent relative error. Proving this with only the non-negative integer library requires separate consideration of Z<0\ud835\udc4d0Z<0italic_Z < 0 and Z\u22650\ud835\udc4d0Z\\geq 0italic_Z \u2265 0 for both inequalities. The details were quite involved, and are omitted. \n9 Level 1 Addition In Level 1, the sum of two values is approximated by one of four cases. Which case is used depends on the relative magnitude of the two values (i.e., their ratio). Since we are dealing with logarithms, the case is selected based on the difference of the representations: The following theorem shows that this implementation of addition is correct in terms of the function S\ud835\udc46Sitalic_S: \n   (EQUAL (ADD-LEVEL-1 X Y) (iPLUS Y (S (iDIFFERENCE X Y))))\n This theorem is used to assist the theorem prover in the next proof.\nThe final Level 1 theorems show that the implementation of addition is\ncorrect in the sense that the following inequalities hold: If X\ud835\udc4bXitalic_X exactly represents NX/DXsubscript\ud835\udc41\ud835\udc4bsubscript\ud835\udc37\ud835\udc4bN_{X}/D_{X}italic_N start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT / italic_D start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT and Y\ud835\udc4cYitalic_Y exactly represents NY/DYsubscript\ud835\udc41\ud835\udc4csubscript\ud835\udc37\ud835\udc4cN_{Y}/D_{Y}italic_N start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT / italic_D start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT, with DYsubscript\ud835\udc37\ud835\udc4cD_{Y}italic_D start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT and DYsubscript\ud835\udc37\ud835\udc4cD_{Y}italic_D start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT both positive, then That is, \n10 Level 1 tolerances Using the theorems given so far to prove the correctness of Level 1 programs that use repeated multiplication and division should pose no problem since such representations will remain exact if the initial inputs are exact. However, such proofs are trivial since they simply amount to saying a program that computes sums and differences of integers is correct.\nAs contrasted with multiplication and division of exact values, Level 1 addition of exact values does not produce an exact result, This inherent truncation is one major obstacle in proving \u201cfloating point\u201d programs are correct. In a program that uses Level 1 logarithmic arithmetic, the two possible sources of truncation are: (a) conversion of rational inputs to Level 1 representations, using FLOOR-LOG and (b) addition of Level 1 representations. FLOOR-LOG may\noccasionally be exact, for example when converting the number one, but in this formalization we must assume ADD-LEVEL-1 is always inexact, even when its inputs are exact. This poses a problem in proving the correctness of programs that perform more than one addition. Furthermore, the results from such program fragments may be input into other functions that do multiplication, and so the assumption of exact input(s) will no longer hold. We need a notation to account for the accumulated error that results from repeated finite precision computation.\nPast attempts to formalize this have viewed floating point values as a subset of the rationals, as they most certainly are [36, 18, 9]. On the other hand, verification of the floating point implementations [4] have concentrated on the manipulation of the representations, as we have just done in the previous sections. What is needed is a notation that merges the concepts of value and representation. This notation also must be flexible enough to describe accumulated error. This new notation, although somewhat similar to interval arithmetic [21], is specialized to the task of automated verification of logarithmic arithmetic. We define a predicate, which we refer to as a tolerance, to state, which means a particular arbitrary rational value (defined by N\ud835\udc41Nitalic_N and D\ud835\udc37Ditalic_D) and a certain machine representation (Z\ud835\udc4dZitalic_Z) in a certain base (defined by P\ud835\udc43Pitalic_P and Q\ud835\udc44Qitalic_Q) are related to one another by at most a certain kind of accumulated relative error bound (defined by TLsubscript\ud835\udc47\ud835\udc3fT_{L}italic_T start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT, and TRsubscript\ud835\udc47\ud835\udc45T_{R}italic_T start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT). In the syntax of the theorem prover, \n (DEFN\n    TOL (P Q TL TH Z N D)\n    (AND\n         (L-GEQ N D P Q (iPLUS Z TL))\n         (R-GEQ N D P Q (iPLUS Z TH))))\n In Level 1, all the tolerances use base b\ud835\udc4fbitalic_b, and so In essence, the tolerance notation generalizes the kind of inequalities commonly used in our axioms and theorems, such as axiom (5), except that the tolerance does not use a strict inequality. The reason for using R-GEQ, instead of L-LESSP is so that \n  (EXACT-REP-LEVEL-1 Z N D)\n means the same as \n    (TOL-LEVEL-1 0 0 Z N D).\n Many of the axioms and theorems can be restated using this notation, for example \n (IMPLIES\n   (AND (NUMBERP Z)\n        (NOT (LESSP (SEZ) 2)))\n   (TOL-LEVEL-1 0 1 (ST Z) (PLUS (EXP (P) Z) (EXP (Q) Z)) (EXP (Q) Z)))\n is similar to axiom (5) with \u201c\u2264\\leq\u2264\u201d used in the right inequality instead of \u201c<<<\u201d. With this notation, we have proven more general theorems about accumulated error. For example, NQTHM proved that the tolerances add when the representations of two positive rational values are multiplied using MULT-LEVEL-1: \n  (IMPLIES\n     (AND (TOL-LEVEL-1 TLX THX X NX DX)\n          (TOL-LEVEL-1 TLY THY Y NY DY))\n     (TOL-LEVEL-1 (iPLUS TLX TLY) (iPLUS THX THY)\n                  (MULT-LEVEL-1 X Y)\n                  (TIMES NX NY) (TIMES DX DY)))\n Here, TL\u2062Xsubscript\ud835\udc47\ud835\udc3f\ud835\udc4bT_{LX}italic_T start_POSTSUBSCRIPT italic_L italic_X end_POSTSUBSCRIPT and TH\u2062Xsubscript\ud835\udc47\ud835\udc3b\ud835\udc4bT_{HX}italic_T start_POSTSUBSCRIPT italic_H italic_X end_POSTSUBSCRIPT define the tolerance for X\ud835\udc4bXitalic_X, and TL\u2062Ysubscript\ud835\udc47\ud835\udc3f\ud835\udc4cT_{LY}italic_T start_POSTSUBSCRIPT italic_L italic_Y end_POSTSUBSCRIPT and TH\u2062Ysubscript\ud835\udc47\ud835\udc3b\ud835\udc4cT_{HY}italic_T start_POSTSUBSCRIPT italic_H italic_Y end_POSTSUBSCRIPT define the tolerance for Y, This generalizes the result given earlier only for exact inputs (TL\u2062X=TH\u2062X=TL\u2062Y=TH\u2062Y=0subscript\ud835\udc47\ud835\udc3f\ud835\udc4bsubscript\ud835\udc47\ud835\udc3b\ud835\udc4bsubscript\ud835\udc47\ud835\udc3f\ud835\udc4csubscript\ud835\udc47\ud835\udc3b\ud835\udc4c0T_{LX}=T_{HX}=T_{LY}=T_{HY}=0italic_T start_POSTSUBSCRIPT italic_L italic_X end_POSTSUBSCRIPT = italic_T start_POSTSUBSCRIPT italic_H italic_X end_POSTSUBSCRIPT = italic_T start_POSTSUBSCRIPT italic_L italic_Y end_POSTSUBSCRIPT = italic_T start_POSTSUBSCRIPT italic_H italic_Y end_POSTSUBSCRIPT = 0), and is very useful in proving later theorems. NOTHM also proved that computing a reciprocal causes the high and low tolerances of the representation to be both negated and interchanged in the result. By combining this fact with the multiplication theorem above, NQTHM proved what happens to the tolerance of a quotient. If it is assumed TL\u2062Xsubscript\ud835\udc47\ud835\udc3f\ud835\udc4bT_{LX}italic_T start_POSTSUBSCRIPT italic_L italic_X end_POSTSUBSCRIPT, TH\u2062Xsubscript\ud835\udc47\ud835\udc3b\ud835\udc4bT_{HX}italic_T start_POSTSUBSCRIPT italic_H italic_X end_POSTSUBSCRIPT, TL\u2062Ysubscript\ud835\udc47\ud835\udc3f\ud835\udc4cT_{LY}italic_T start_POSTSUBSCRIPT italic_L italic_Y end_POSTSUBSCRIPT and TH\u2062Ysubscript\ud835\udc47\ud835\udc3b\ud835\udc4cT_{HY}italic_T start_POSTSUBSCRIPT italic_H italic_Y end_POSTSUBSCRIPT are defined as in the multiplication tolerance theorem above, NQTHM proved the low and high tolerances of (DIV-LEVEL-1 X Y) are TL\u2062X\u2212TH\u2062Ysubscript\ud835\udc47\ud835\udc3f\ud835\udc4bsubscript\ud835\udc47\ud835\udc3b\ud835\udc4cT_{LX}-T_{HY}italic_T start_POSTSUBSCRIPT italic_L italic_X end_POSTSUBSCRIPT - italic_T start_POSTSUBSCRIPT italic_H italic_Y end_POSTSUBSCRIPT and TH\u2062X\u2212TL\u2062Ysubscript\ud835\udc47\ud835\udc3b\ud835\udc4bsubscript\ud835\udc47\ud835\udc3f\ud835\udc4cT_{HX}-T_{LY}italic_T start_POSTSUBSCRIPT italic_H italic_X end_POSTSUBSCRIPT - italic_T start_POSTSUBSCRIPT italic_L italic_Y end_POSTSUBSCRIPT, respectively. \n11 Addition tolerances Earlier, it was shown that for exact inputs, X\ud835\udc4bXitalic_X and Y\ud835\udc4cYitalic_Y, ADD\u2062_\u2062LEVEL\u2062_\u20621\u2062(X,Y)ADD_LEVEL_1\ud835\udc4b\ud835\udc4c\\mathrm{ADD\\_LEVEL\\_1}(X,Y)roman_ADD _ roman_LEVEL _ 1 ( italic_X , italic_Y ) produces a representation of bX+bYsuperscript\ud835\udc4f\ud835\udc4bsuperscript\ud835\udc4f\ud835\udc4cb^{X}+b^{Y}italic_b start_POSTSUPERSCRIPT italic_X end_POSTSUPERSCRIPT + italic_b start_POSTSUPERSCRIPT italic_Y end_POSTSUPERSCRIPT which is as good as is possible. With the tolerance notation, the relative error bounds of this result for exact X\ud835\udc4bXitalic_X and Y\ud835\udc4cYitalic_Y can be defined as TL\u2062A=0subscript\ud835\udc47\ud835\udc3f\ud835\udc340T_{LA}=0italic_T start_POSTSUBSCRIPT italic_L italic_A end_POSTSUBSCRIPT = 0 and TH\u2062A=1subscript\ud835\udc47\ud835\udc3b\ud835\udc341T_{HA}=1italic_T start_POSTSUBSCRIPT italic_H italic_A end_POSTSUBSCRIPT = 1. To be useful in complicated programs, TL\u2062Asubscript\ud835\udc47\ud835\udc3f\ud835\udc34T_{LA}italic_T start_POSTSUBSCRIPT italic_L italic_A end_POSTSUBSCRIPT and TH\u2062Asubscript\ud835\udc47\ud835\udc3b\ud835\udc34T_{HA}italic_T start_POSTSUBSCRIPT italic_H italic_A end_POSTSUBSCRIPT need to be generalized to consider the case of an inexact input.\nIf we assume X\ud835\udc4bXitalic_X represents NX/DXsubscript\ud835\udc41\ud835\udc4bsubscript\ud835\udc37\ud835\udc4bN_{X}/D_{X}italic_N start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT / italic_D start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT with tolerance defined by TL\u2062Xsubscript\ud835\udc47\ud835\udc3f\ud835\udc4bT_{LX}italic_T start_POSTSUBSCRIPT italic_L italic_X end_POSTSUBSCRIPT and TH\u2062Xsubscript\ud835\udc47\ud835\udc3b\ud835\udc4bT_{HX}italic_T start_POSTSUBSCRIPT italic_H italic_X end_POSTSUBSCRIPT and Y\ud835\udc4cYitalic_Y represents NY/DYsubscript\ud835\udc41\ud835\udc4csubscript\ud835\udc37\ud835\udc4cN_{Y}/D_{Y}italic_N start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT / italic_D start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT with tolerance defined by TL\u2062Ysubscript\ud835\udc47\ud835\udc3f\ud835\udc4cT_{LY}italic_T start_POSTSUBSCRIPT italic_L italic_Y end_POSTSUBSCRIPT and TH\u2062Ysubscript\ud835\udc47\ud835\udc3b\ud835\udc4cT_{HY}italic_T start_POSTSUBSCRIPT italic_H italic_Y end_POSTSUBSCRIPT, then the following real inequalities hold: where sbsubscript\ud835\udc60\ud835\udc4fs_{b}italic_s start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT is the real valued addition logarithm with rational base b=P/Q\ud835\udc4f\ud835\udc43\ud835\udc44b=P/Qitalic_b = italic_P / italic_Q, and the integers TL\u2062Asubscript\ud835\udc47\ud835\udc3f\ud835\udc34T_{LA}italic_T start_POSTSUBSCRIPT italic_L italic_A end_POSTSUBSCRIPT, and TH\u2062Asubscript\ud835\udc47\ud835\udc3b\ud835\udc34T_{HA}italic_T start_POSTSUBSCRIPT italic_H italic_A end_POSTSUBSCRIPT are chosen so as to make the above inequalities true: and We proved the validity of these definitions for TL\u2062Asubscript\ud835\udc47\ud835\udc3f\ud835\udc34T_{LA}italic_T start_POSTSUBSCRIPT italic_L italic_A end_POSTSUBSCRIPT and TH\u2062Asubscript\ud835\udc47\ud835\udc3b\ud835\udc34T_{HA}italic_T start_POSTSUBSCRIPT italic_H italic_A end_POSTSUBSCRIPT with NQTHM. The proof was quite involved because of the need to translate to nonnegative integers. It required several cases, depending on the sign of Y\u2212X\ud835\udc4c\ud835\udc4bY-Xitalic_Y - italic_X, on whether X+TL\u2062X\ud835\udc4bsubscript\ud835\udc47\ud835\udc3f\ud835\udc4bX+T_{LX}italic_X + italic_T start_POSTSUBSCRIPT italic_L italic_X end_POSTSUBSCRIPT and X+TH\u2062X\ud835\udc4bsubscript\ud835\udc47\ud835\udc3b\ud835\udc4bX+T_{HX}italic_X + italic_T start_POSTSUBSCRIPT italic_H italic_X end_POSTSUBSCRIPT are both positive, both negative, or of mixed signs, and also on the signs of Y+TL\u2062Y\ud835\udc4csubscript\ud835\udc47\ud835\udc3f\ud835\udc4cY+T_{LY}italic_Y + italic_T start_POSTSUBSCRIPT italic_L italic_Y end_POSTSUBSCRIPT and Y+TH\u2062Y\ud835\udc4csubscript\ud835\udc47\ud835\udc3b\ud835\udc4cY+T_{HY}italic_Y + italic_T start_POSTSUBSCRIPT italic_H italic_Y end_POSTSUBSCRIPT. X\ud835\udc4bXitalic_X and Y\ud835\udc4cYitalic_Y may not be known until run-time, but the tolerances typically are known during the proof of the correctness of the program. For those circumstances where no additional knowledge is available about X\ud835\udc4bXitalic_X and Y\ud835\udc4cYitalic_Y during the proof, a correct, but less tight, bound on LA and TH\u2062Asubscript\ud835\udc47\ud835\udc3b\ud835\udc34T_{HA}italic_T start_POSTSUBSCRIPT italic_H italic_A end_POSTSUBSCRIPT can be obtained. Although NQTHM does not prove it this way, we know for all real z\ud835\udc67zitalic_z that 0<s\u2032\u2062(z)<10superscript\ud835\udc60\u2032\ud835\udc6710<s^{\\prime}(z)<10 < italic_s start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_z ) < 1, and so it is reasonable to expect that similar bounds exist for the first difference of S\u2062(Z)\ud835\udc46\ud835\udc4dS(Z)italic_S ( italic_Z ) for any integer Z\ud835\udc4dZitalic_Z. In fact, the following theorems hold for all integers Z\ud835\udc4dZitalic_Z: and From this knowledge, we can determine a much simplier expression for the tolerance than TL\u2062Asubscript\ud835\udc47\ud835\udc3f\ud835\udc34T_{LA}italic_T start_POSTSUBSCRIPT italic_L italic_A end_POSTSUBSCRIPT and TH\u2062Asubscript\ud835\udc47\ud835\udc3b\ud835\udc34T_{HA}italic_T start_POSTSUBSCRIPT italic_H italic_A end_POSTSUBSCRIPT\nabove.\nLet \u0394=TL\u2062Y\u2212TL\u2062X\u0394subscript\ud835\udc47\ud835\udc3f\ud835\udc4csubscript\ud835\udc47\ud835\udc3f\ud835\udc4b\\Delta=T_{LY}-T_{LX}roman_\u0394 = italic_T start_POSTSUBSCRIPT italic_L italic_Y end_POSTSUBSCRIPT - italic_T start_POSTSUBSCRIPT italic_L italic_X end_POSTSUBSCRIPT and Z=Y\u2212X\ud835\udc4d\ud835\udc4c\ud835\udc4bZ=Y-Xitalic_Z = italic_Y - italic_X, and so when TL\u2062X<TL\u2062Ysubscript\ud835\udc47\ud835\udc3f\ud835\udc4bsubscript\ud835\udc47\ud835\udc3f\ud835\udc4cT_{LX}<T_{LY}italic_T start_POSTSUBSCRIPT italic_L italic_X end_POSTSUBSCRIPT < italic_T start_POSTSUBSCRIPT italic_L italic_Y end_POSTSUBSCRIPT, we know that \u0394>0\u03940\\Delta>0roman_\u0394 > 0, and so it is valid to define T\u00afL\u2062A=TL\u2062X+0<TL\u2062Asubscript\u00af\ud835\udc47\ud835\udc3f\ud835\udc34subscript\ud835\udc47\ud835\udc3f\ud835\udc4b0subscript\ud835\udc47\ud835\udc3f\ud835\udc34\\bar{T}_{LA}=T_{LX}+0<T_{LA}over\u00af start_ARG italic_T end_ARG start_POSTSUBSCRIPT italic_L italic_A end_POSTSUBSCRIPT = italic_T start_POSTSUBSCRIPT italic_L italic_X end_POSTSUBSCRIPT + 0 < italic_T start_POSTSUBSCRIPT italic_L italic_A end_POSTSUBSCRIPT in this case. Also, when TL\u2062X\u2265TL\u2062Ysubscript\ud835\udc47\ud835\udc3f\ud835\udc4bsubscript\ud835\udc47\ud835\udc3f\ud835\udc4cT_{LX}\\geq T_{LY}italic_T start_POSTSUBSCRIPT italic_L italic_X end_POSTSUBSCRIPT \u2265 italic_T start_POSTSUBSCRIPT italic_L italic_Y end_POSTSUBSCRIPT, we know \u0394\u22640\u03940\\Delta\\leq 0roman_\u0394 \u2264 0, and so it is correct to use T\u00afL\u2062A=TL\u2062X+\u0394<TL\u2062Asubscript\u00af\ud835\udc47\ud835\udc3f\ud835\udc34subscript\ud835\udc47\ud835\udc3f\ud835\udc4b\u0394subscript\ud835\udc47\ud835\udc3f\ud835\udc34\\bar{T}_{LA}=T_{LX}+\\Delta<T_{LA}over\u00af start_ARG italic_T end_ARG start_POSTSUBSCRIPT italic_L italic_A end_POSTSUBSCRIPT = italic_T start_POSTSUBSCRIPT italic_L italic_X end_POSTSUBSCRIPT + roman_\u0394 < italic_T start_POSTSUBSCRIPT italic_L italic_A end_POSTSUBSCRIPT. Therefore, when there is no other information about X\ud835\udc4bXitalic_X and Y\ud835\udc4cYitalic_Y, it is valid to use and by similar reasoning, Although these are not as tight as would be possible if something were known about Z\ud835\udc4dZitalic_Z, they are good enough to mechanically verify proofs that programs involving summation of terms, such as evaluating a polynomial, have reasonable tolerances that can be determined at proof time. Because the tolerance of the accumulated sum soon grows to be larger than the tolerance of any term, most ADD_LEVEL_1 operations simply increment TH\u2062Asubscript\ud835\udc47\ud835\udc3b\ud835\udc34T_{HA}italic_T start_POSTSUBSCRIPT italic_H italic_A end_POSTSUBSCRIPT by one. This corresponds to our intuition about what happens to accumulated relative error as a result of multiple table look ups. \n12 Taylor series example Polynomials derived from truncated series are often used to approximate functions. For example, provides an approximation to exsuperscript\ud835\udc52\ud835\udc65e^{x}italic_e start_POSTSUPERSCRIPT italic_x end_POSTSUPERSCRIPT for small positive x\ud835\udc65xitalic_x. A Level 1 program implementing this computation, \n(DEFN F (X)\n  (ADD-LEVEL-1\n    (DIV-LEVEL-1 (MULT-LEVEL-1 X (MULT-LEVEL-1 X X))\n                 (FLOOR-LOG 6 1 (P) (Q)))\n    (ADD-LEVEL-1\n       (DIV-LEVEL-1 (MULT-LEVEL-1 X X)\n                    (FLOOR-LOG 2 1 (P) (Q)))\n       (ADD-LEVEL-1 X 0))))\n was submitted to NQTHM together with several lemmas about the tolerances of each subexpression. NQTHM proved that if TL\u2062X=0subscript\ud835\udc47\ud835\udc3f\ud835\udc4b0T_{LX}=0italic_T start_POSTSUBSCRIPT italic_L italic_X end_POSTSUBSCRIPT = 0 and TH\u2062X=1subscript\ud835\udc47\ud835\udc3b\ud835\udc4b1T_{HX}=1italic_T start_POSTSUBSCRIPT italic_H italic_X end_POSTSUBSCRIPT = 1 (as would be true if x\ud835\udc65xitalic_x had been converted with FLOOR-LOG), then (F X) approximates f\u2062(x)\ud835\udc53\ud835\udc65f(x)italic_f ( italic_x ) with a tolerance of\nTF\u2062L=\u22121subscript\ud835\udc47\ud835\udc39\ud835\udc3f1T_{FL}=-1italic_T start_POSTSUBSCRIPT italic_F italic_L end_POSTSUBSCRIPT = - 1 and TF\u2062H=4subscript\ud835\udc47\ud835\udc39\ud835\udc3b4T_{FH}=4italic_T start_POSTSUBSCRIPT italic_F italic_H end_POSTSUBSCRIPT = 4. In other words, the logarithmic representation of f\u2062(z)\ud835\udc53\ud835\udc67f(z)italic_f ( italic_z ), \u230alogb\u2061(f\u2062(x))\u230bsubscript\ud835\udc4f\ud835\udc53\ud835\udc65\\lfloor\\log_{b}(f(x))\\rfloor\u230a roman_log start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_f ( italic_x ) ) \u230b, is one of the six integers F\u2062(X)\u22121\ud835\udc39\ud835\udc4b1F(X)-1italic_F ( italic_X ) - 1, F\u2062(X)\ud835\udc39\ud835\udc4bF(X)italic_F ( italic_X ), F\u2062(X)+1\ud835\udc39\ud835\udc4b1F(X)+1italic_F ( italic_X ) + 1, F\u2062(X)+2\ud835\udc39\ud835\udc4b2F(X)+2italic_F ( italic_X ) + 2, F\u2062(X)+3\ud835\udc39\ud835\udc4b3F(X)+3italic_F ( italic_X ) + 3 and F\u2062(X)+4\ud835\udc39\ud835\udc4b4F(X)+4italic_F ( italic_X ) + 4. That is, the value of F\u2062(X)\ud835\udc39\ud835\udc4bF(X)italic_F ( italic_X ), bF\u2062(X)superscript\ud835\udc4f\ud835\udc39\ud835\udc4bb^{F(X)}italic_b start_POSTSUPERSCRIPT italic_F ( italic_X ) end_POSTSUPERSCRIPT, is one of the six representable values b\u230alogb\u2061(f\u2062(x))\u230b\u22c5b\u22124\u22c5superscript\ud835\udc4fsubscript\ud835\udc4f\ud835\udc53\ud835\udc65superscript\ud835\udc4f4b^{\\lfloor\\log_{b}(f(x))\\rfloor}\\cdot b^{-4}italic_b start_POSTSUPERSCRIPT \u230a roman_log start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_f ( italic_x ) ) \u230b end_POSTSUPERSCRIPT \u22c5 italic_b start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT,\nb\u230alogb\u2061(f\u2062(x))\u230b\u22c5b\u22123\u22c5superscript\ud835\udc4fsubscript\ud835\udc4f\ud835\udc53\ud835\udc65superscript\ud835\udc4f3b^{\\lfloor\\log_{b}(f(x))\\rfloor}\\cdot b^{-3}italic_b start_POSTSUPERSCRIPT \u230a roman_log start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_f ( italic_x ) ) \u230b end_POSTSUPERSCRIPT \u22c5 italic_b start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT,\nb\u230alogb\u2061(f\u2062(x))\u230b\u22c5b\u22122\u22c5superscript\ud835\udc4fsubscript\ud835\udc4f\ud835\udc53\ud835\udc65superscript\ud835\udc4f2b^{\\lfloor\\log_{b}(f(x))\\rfloor}\\cdot b^{-2}italic_b start_POSTSUPERSCRIPT \u230a roman_log start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_f ( italic_x ) ) \u230b end_POSTSUPERSCRIPT \u22c5 italic_b start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT,\nb\u230alogb\u2061(f\u2062(x))\u230b\u22c5b\u22121\u22c5superscript\ud835\udc4fsubscript\ud835\udc4f\ud835\udc53\ud835\udc65superscript\ud835\udc4f1b^{\\lfloor\\log_{b}(f(x))\\rfloor}\\cdot b^{-1}italic_b start_POSTSUPERSCRIPT \u230a roman_log start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_f ( italic_x ) ) \u230b end_POSTSUPERSCRIPT \u22c5 italic_b start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT,\nb\u230alogb\u2061(f\u2062(x))\u230b\u22c5b^{\\lfloor\\log_{b}(f(x))\\rfloor}\\cdotitalic_b start_POSTSUPERSCRIPT \u230a roman_log start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_f ( italic_x ) ) \u230b end_POSTSUPERSCRIPT \u22c5 and\nb\u230alogb\u2061(f\u2062(x))\u230b\u22c5b1\u22c5superscript\ud835\udc4fsubscript\ud835\udc4f\ud835\udc53\ud835\udc65superscript\ud835\udc4f1b^{\\lfloor\\log_{b}(f(x))\\rfloor}\\cdot b^{1}italic_b start_POSTSUPERSCRIPT \u230a roman_log start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_f ( italic_x ) ) \u230b end_POSTSUPERSCRIPT \u22c5 italic_b start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT. To describe the rational value, f\u2062(x)\ud835\udc53\ud835\udc65f(x)italic_f ( italic_x ), being approximated here once again requires translating to statements about non-negative integers. In this case, however, the non-negative integer description is quite unwieldly. The unreduced numerator of f\u2062(x)\ud835\udc53\ud835\udc65f(x)italic_f ( italic_x ) is: \n  (PLUS (TIMES (TIMES (TIMES WX (TIMES NX NX))\n  1) (TIMES (TIMES 2 (TIMES DK DX}) (TIMES DX\n  1))) (TIMES (PLUS (TIMES (TIMES (TIMES NX\n  NX) 1) (TIMES DX 1)) (TIMES (PLUS (TIMES NX\n  1) (TIMES 1 DX}) (TIMES 2 (TIMES DK DX))))\n  (TIMES 6 (TIMES DX (TIMES DK DX))))).\n A programmer could determine that the above is\n12\u22c5DX6\u22c5f\u2062(NX/DX)\u22c512superscriptsubscript\ud835\udc37\ud835\udc4b6\ud835\udc53subscript\ud835\udc41\ud835\udc4bsubscript\ud835\udc37\ud835\udc4b12\\cdot D_{X}^{6}\\cdot f(N_{X}/D_{X})12 \u22c5 italic_D start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT \u22c5 italic_f ( italic_N start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT / italic_D start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ),\nbut such manual manipulations are tedious and error prone.\nIf we had been able to use the NQTHM rationals library for Level 1, we could have described f\u2062(x)\ud835\udc53\ud835\udc65f(x)italic_f ( italic_x ) directly, and NQTHM would have translated to such an expression. As a workaround to this problem, we wrote an unverified program to generate the lemmas involving such expressions. One lemma is generated for each subexpression. We did not bother to verify this lemma generator, since its output is intended to be input. to NQTHM. This program inserts into each lemma the hints required for NOTHM to arrive at the proper conclusions, and so the generator is somewhat more than just a rational to integer translator. It is interesting to note that addition tolerances are not associative. Had f\u2062(x)\ud835\udc53\ud835\udc65f(x)italic_f ( italic_x ) been evaluated in the reverse order, the tolerances would have been TF\u2062L=\u22121subscript\ud835\udc47\ud835\udc39\ud835\udc3f1T_{FL}=-1italic_T start_POSTSUBSCRIPT italic_F italic_L end_POSTSUBSCRIPT = - 1 and TF\u2062H=6subscript\ud835\udc47\ud835\udc39\ud835\udc3b6T_{FH}=6italic_T start_POSTSUBSCRIPT italic_F italic_H end_POSTSUBSCRIPT = 6. \n13 Level 2 The representation for Level 1 is an unbounded integer, which is somewhat analagous to a member of the infinite set, F\u2217superscript\ud835\udc39F^{*}italic_F start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT, described in the Language Compatible Arithmetic Standard (LCAS) [18]. The axioms can be made more realistic with the inclusion of the concepts of underflow and overflow, which occur when numbers are represented with a bounded exponent. The only additional axiom for Level 2 postulates two integer constants, MIN_LEVEL_2 and MAX_LEVEL_2, such that We define the predicate IN-RANGE-LEVEL-2 to indicate if a Level 1 argument is inside of the allowed range for Level 2: \n(DEFN\n   IN-RANGE-LEVEL-2 (X)\n        (AND (NOT (iLESSP X (MIN-LEVEL-2)))\n             (NOT (iLESSP (MAX-LEVEL-2) X))))\n Also, (SIGNAL-OUT-RANGE-LEVEL-2) returns MAX_LEVEL_2 + 1 which represents a result that overflowed or underflowed. With a clipping function, \n (DEFN\n   CLIP-LEVEL-2 (X Y RESULT)\n     (IF (AND (IN-RANGE-LEVEL-2 X)\n              (IN-RANGE-LEVEL-2 Y)\n              (IN-RANGE-LEVEL-2 RESULT))\n        RESULT\n        (SIGNAL-OUT-RANGE-LEVEL-2)))\n and definitions such as: \n (DEFN\n   MULT-LEVEL-2 (X Y)\n   (CLIP-LEVEL-2 X Y (MULT-LEVEL-1 X Y))))\n we can have theorems like: \n (IMPLIES\n    (IN-RANGE-LEVEL-2 (MULT-LEVEL-2 X Y))\n    (AND (IN-RANGE-LEVEL-2 X)\n         (IN-RANGE-LEVEL-2 Y)\n         (EQUAL (MULT-LEVEL-1 X Y)\n                (MULT-LEVEL-2 X Y))))\n that state Level 1 proofs validate Level 2 programs provided that the machine running the Level 2 program never detects an underflow or overflaw. Therefore, Level 2 uses a machine with a finite word size and finite tables, and so Level 2 representations have both finite range and finite precision. \n14 Conclusions We have mechanically verified the first levels of what we hope to be several levels of increasingly realistic logarithmic arithmetic implementations. Although Level 1 may seem very simplistic, the proofs required to reach this stage were non- trivial. We did not present our subtraction axioms, which requires a table,\nDT(Z) = \u2308logb\u2061|1\u2212bZ|\u2309subscript\ud835\udc4f1superscript\ud835\udc4f\ud835\udc4d\\lceil\\log_{b}|1-b^{Z}|\\rceil\u2308 roman_log start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT | 1 - italic_b start_POSTSUPERSCRIPT italic_Z end_POSTSUPERSCRIPT | \u2309, of the quantized subtraction logarithm. These axioms are similar to (2)-(5), however the DT table is indexed from 1 to SEZ+1. A complete Level 1 implementation requires verifying an implementation of the computation of the absolute value of the difference of positive rational values using DT. Although, as we have shown, it is possible to prove useful theorems about the tolerances of ADD_LEVEL_1 programs without knowing Z\ud835\udc4dZitalic_Z, this is not possible for programs that use subtraction. A proof about subtraction must have some information about Z\ud835\udc4dZitalic_Z since\n|d\u2032\u2062(z)|\u2192\u2212\u221e\u2192superscript\ud835\udc51\u2032\ud835\udc67|d^{\\prime}(z)|\\rightarrow-\\infty| italic_d start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_z ) | \u2192 - \u221e as z\ud835\udc67zitalic_z approaches zero. In other words, when Z\ud835\udc4dZitalic_Z is near zero, severe cancellation occurs. Overcoming the formalization problems presented by this inherent property of floating point subtraction [20] is beyond the scope of this paper. Levels 1 and 2 use simple table look up to approximate S\u2062(Z)\ud835\udc46\ud835\udc4dS(Z)italic_S ( italic_Z ). However, as shown earlier, it is impractical to have a complete table of the more than 227superscript2272^{27}2 start_POSTSUPERSCRIPT 27 end_POSTSUPERSCRIPT words required for single precision (F=23\ud835\udc3923F=23italic_F = 23), and so much more sophisticated algorithms than what we have considered here will be required. The addition logarithm function is often approximated by interpolation [37]. Therefore, to verily a practical, single precision logarithmic arithmetic implementation re- quires verifying fairly complicated \u201cfloating point\u201d programs which compute S\u2062(Z)\ud835\udc46\ud835\udc4dS(Z)italic_S ( italic_Z ) from a much smaller table. Initial research in this area leads us to think the divided difference method is most amenable to formalizing, but the proofs are very complicated, and we have not yet submitted any to NOTHM, We expect the tolerance notation described above to be helpful in such proofs. The functional programming paradigm of NQTHM is quite different from the typical application of logarithmic arithmetic. More realistic models of computing devices could be developed and logarithmic arithmetic correctly implemented on them. The hope is that eventually an implementation (F=23\ud835\udc3923F=23italic_F = 23)\nin a machine language that has been formally specified in NQTHM, such as MC68020 [7] or PITON [22], could be verified. Also, gate level verification [12] of logarithmic arithmetic hardware could be possible.\nWe are pleased with NQTHM\u2019s ability to deal with the problems presented here. We hope to have a better computer for running NQTHM in the future, and to use the rationals library in later levels of the formalization, which should simplify the proofs.\nWe will provide a machine readable copy of the proofs described here ta interested parties who request it. NQTHM itself is available via anonymous ftp. The conditions for its use established by its developers and the directions for obtaining it are given in [5]. References"}
